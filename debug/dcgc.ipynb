{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbe6b76b-0fd1-47df-87d7-341e7393990a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.data.data_utils import load_data, load_node_to_nearest_training\n",
    "from src.utils import set_global_seeds, arg_parse, name_model, create_nested_defaultdict, \\\n",
    "    metric_mean, metric_std, default_cal_wdecay, save_prediction\n",
    "from src.model.model import create_model\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9c0597b-726a-4e68-bb68-8430012e991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.nn import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eb379fb-36ad-4807-bf37-5f1c8222ccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da888036-ec70-47a8-ae9a-54b77bd47b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import os\n",
    "import time\n",
    "from src.model.model import create_model\n",
    "from pathlib import Path\n",
    "from src.calibrator import rbs\n",
    "from torch_geometric.utils import to_networkx\n",
    "import scipy\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9d5eca8-21ef-4165-9ddf-418fcbe16d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from src.calibloss import ECE\n",
    "\n",
    "parser = argparse.ArgumentParser(description='train.py and calibration.py share the same arguments')\n",
    "parser.add_argument('--seed', type=int, default=10, help='Random Seed')\n",
    "parser.add_argument('--dataset', type=str, default='Cora', choices=['Cora','Citeseer', 'Pubmed', \n",
    "                    'Computers', 'Photo', 'CS', 'Physics', 'CoraFull'])\n",
    "parser.add_argument('--split_type', type=str, default='5_3f_85', help='k-fold and test split')\n",
    "parser.add_argument('--model', type=str, default='GCN', choices=['GCN', 'GAT'])\n",
    "parser.add_argument('--verbose', action='store_true', default=False, help='Show training and validation loss')\n",
    "parser.add_argument('--wdecay', type=float, default=5e-4, help='Weight decay for training phase')\n",
    "parser.add_argument('--dropout_rate', type=float, default=0.5, help='Dropout rate. 1.0 denotes drop all the weights to zero')\n",
    "parser.add_argument('--calibration', type=str, default='GATS',  help='Post-hoc calibrators')\n",
    "parser.add_argument('--cal_wdecay', type=float, default=None, help='Weight decay for calibration phase')\n",
    "parser.add_argument('--cal_dropout_rate', type=float, default=0.5, help='Dropout rate for calibrators (CaGCN)')\n",
    "parser.add_argument('--folds', type=int, default=3, help='K folds cross-validation for calibration')\n",
    "parser.add_argument('--ece-bins', type=int, default=15, help='number of bins for ece')\n",
    "parser.add_argument('--ece-scheme', type=str, default='equal_width', choices=ECE.binning_schemes, help='binning scheme for ece')\n",
    "parser.add_argument('--ece-norm', type=float, default=1.0, help='norm for ece')\n",
    "parser.add_argument('--save_prediction', action='store_true', default=False)\n",
    "parser.add_argument('--config', action='store_true', default=False)\n",
    "\n",
    "gats_parser = parser.add_argument_group('optional GATS arguments')\n",
    "gats_parser.add_argument('--heads', type=int, default=2, help='Number of heads for GATS. Hyperparameter set: {1,2,4,8,16}')\n",
    "gats_parser.add_argument('--bias', type=float, default=1, help='Bias initialization for GATS')\n",
    "args = parser.parse_args(['--dataset', 'Cora', '--calibration', 'TS', '--model', 'GAT'])\n",
    "\n",
    "parser.add_argument(\"--alpha\", type=float, default=0.98)\n",
    "parser.add_argument(\"--lmbda\", type=float, default=1.0)\n",
    "parser.add_argument(\"--num_bins_rbs\", type=int, default=2)\n",
    "\n",
    "args_dict = {}\n",
    "for group in parser._action_groups:\n",
    "    if group.title == 'optional GATS arguments':\n",
    "        group_dict={a.dest:getattr(args,a.dest,None) for a in group._group_actions}\n",
    "        args_dict['gats_args'] = argparse.Namespace(**group_dict)\n",
    "    else:\n",
    "        group_dict={a.dest:getattr(args,a.dest,None) for a in group._group_actions}\n",
    "        args_dict.update(group_dict)\n",
    "args = argparse.Namespace(**args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2aafab9c-a460-4a68-9918-95ead85b3058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(alpha=None, cal_dropout_rate=0.5, cal_wdecay=None, calibration='TS', config=False, dataset='Cora', dropout_rate=0.5, ece_bins=15, ece_norm=1.0, ece_scheme='equal_width', folds=3, gats_args=Namespace(bias=1, heads=2), help=None, lmbda=None, model='GAT', num_bins_rbs=None, save_prediction=False, seed=10, split_type='5_3f_85', verbose=False, wdecay=0.0005)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "886e890e-0a00-4460-9151-c287845846b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_base_data(name: str) -> Dataset:\n",
    "    \"\"\"\n",
    "    name: str, the name of the dataset\n",
    "    \"\"\"\n",
    "    transform = T.NormalizeFeatures()\n",
    "    if name in ['Cora','Citeseer', 'Pubmed']:\n",
    "        dataset = Planetoid(root='./data/', name=name, transform=transform)\n",
    "    return dataset\n",
    "\n",
    "file_name = 'model_labelrate/model/Cora/labelrate_20/GAT_run3.pt'\n",
    "checkpoint = torch.load(file_name)\n",
    "dataset = load_base_data(args.dataset)\n",
    "data = dataset.data.to(device)\n",
    "data.train_mask = checkpoint['train_mask']\n",
    "data.val_mask = checkpoint['val_mask']\n",
    "data.test_mask = checkpoint['test_mask']\n",
    "\n",
    "model1 = create_model(dataset, args).to(device)\n",
    "model1.load_state_dict(checkpoint['model_state_dict'])\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "281e87b0-34b2-45f9-aef4-7d4806fcb1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n",
      "140\n",
      "500\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(data)\n",
    "print(data.train_mask.sum().item())\n",
    "print(data.val_mask.sum().item())\n",
    "print(data.test_mask.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e9ca6a2-233b-4adb-ab2e-f685b12ec27e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GAT(\n",
       "  (layer_list): ModuleDict(\n",
       "    (conv1): GATConv(1433, 8, heads=8)\n",
       "    (conv2): GATConv(64, 7, heads=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "47d1aadc-ec1f-49e9-b9a3-44948127ff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "class GCN_dcgc(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_edge, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "        self.edge_weight = torch.ones(num_edge).cuda()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        if edge_weight is None:\n",
    "            edge_weight = torch.ones(len(edge_index[0])).cuda()\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv1(x=x, edge_index=edge_index, edge_weight=edge_weight).relu()\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x=x, edge_index=edge_index, edge_weight=edge_weight)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "7e2c3312-6bb5-442a-acf0-216197fd3841",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dcgc = GCN_dcgc(in_channels=data.x.shape[1], hidden_channels=16, out_channels=dataset.num_classes, num_edge=data.num_edges, dropout=0.7).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78ef33e3-393d-41e2-a920-116972c4c2ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1433"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_weight = torch.ones(len(data.edge_index[0])).cuda()\n",
    "edge_weight.shape\n",
    "data.num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "a8338bea-dc3f-404b-bc84-c3a7e800be4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (layer_list): ModuleDict(\n",
      "    (conv1): GCNConv(1433, 64)\n",
      "    (conv2): GCNConv(64, 7)\n",
      "  )\n",
      ")\n",
      "GCN_dcgc(\n",
      "  (conv1): GCNConv(1433, 16)\n",
      "  (conv2): GCNConv(16, 7)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model)\n",
    "print(model_dcgc)\n",
    "from torchsummary import summary\n",
    "# summary(model_dcgc, [(2708, 1433), (2,10566)] ) \n",
    "len(model.feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d233e39a-a9e1-4beb-9066-9d497d20fa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, num_hidden, attention_head, drop_rate, num_layers):\n",
    "        super().__init__()\n",
    "        self.drop_rate = drop_rate\n",
    "        self.feature_list = [in_channels, num_hidden, num_classes]\n",
    "        for _ in range(num_layers-2):\n",
    "            self.feature_list.insert(-1, num_hidden)\n",
    "        attention_head = [1] + attention_head\n",
    "        layer_list = []\n",
    "        for i in range(len(self.feature_list)-1):\n",
    "            concat = False if i == num_layers-1 else True \n",
    "            layer_list.append([\"conv\"+str(i+1), GATConv(self.feature_list[i]* attention_head[i], self.feature_list[i+1], \n",
    "                                                        heads=attention_head[i+1], dropout=drop_rate, concat=concat)])\n",
    "        self.layer_list = torch.nn.ModuleDict(layer_list)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        for i in range(len(self.feature_list)-1):\n",
    "            x = F.dropout(x, self.drop_rate, self.training)\n",
    "            x = self.layer_list[\"conv\"+str(i+1)](x, edge_index)\n",
    "            if i < len(self.feature_list)-2:\n",
    "                x = F.elu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d2c07dd-1aeb-4599-9e8a-8cfd0a6cbf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = GAT(in_channels=dataset.num_features, num_classes = dataset.num_classes, num_hidden=8,\n",
    "                    attention_head=[8,1], drop_rate=0.6, num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e544a9fd-ff43-4bad-bd29-30e38e294fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GAT(\n",
       "  (layer_list): ModuleDict(\n",
       "    (conv1): GATConv(1433, 8, heads=8)\n",
       "    (conv2): GATConv(64, 7, heads=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "154523cf-348e-4860-8baa-e7de6b6b3da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GAT(\n",
       "  (layer_list): ModuleDict(\n",
       "    (conv1): GATConv(1433, 8, heads=8)\n",
       "    (conv2): GATConv(64, 7, heads=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3700fc2b-792b-4f2d-b9e2-de1c1a0a162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.calibloss import NodewiseECE\n",
    "\n",
    "ece_fn = NodewiseECE(data.test_mask, bins=15, scheme='equal_width', norm=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2eece356-196a-4010-9375-e8c2bf6924ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Edge_Weight(torch.nn.Module):\n",
    "    def __init__(self, model, out_channels, dropout):\n",
    "        super(Edge_Weight, self).__init__()\n",
    "        self.model = model\n",
    "        self.extractor = MLP([out_channels*2, out_channels*4, 1], dropout=dropout)\n",
    "\n",
    "        for para in self.model.parameters():\n",
    "            para.requires_grad = False\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        if edge_weight is None:\n",
    "            edge_weight = self.get_weight(x, edge_index)\n",
    "        print('edge_weight requires_grad: ', edge_weight.requires_grad)\n",
    "        logist = self.model(x, edge_index, edge_weight)\n",
    "        print('logist requires_grad: ', logist.requires_grad)\n",
    "        return logist\n",
    "    \n",
    "    def fit(self, data, train_mask, test_mask, wdecay, lr=0.01, edge_weight=None, verbose=False):\n",
    "        self.to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.extractor.parameters(),lr=lr, weight_decay=wdecay)\n",
    "        fit_calibration_dcgc(self, data, train_mask, test_mask, edge_weight=edge_weight, verbose=verbose)\n",
    "        return self\n",
    "\n",
    "    def get_weight(self, x, edge_index):\n",
    "\n",
    "        emb = self.model(x, edge_index)\n",
    "        col, row = edge_index\n",
    "        f1, f2 = emb[col], emb[row]\n",
    "        f12 = torch.cat([f1, f2], dim=-1)\n",
    "        edge_weight = self.extractor(f12)\n",
    "        return edge_weight.relu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7cf4271e-38a9-4ad5-aa83-cd4abecdd473",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit_calibration_dcgc(temp_model, data, train_mask, test_mask, edge_weight=None, patience = 100, verbose=False):\n",
    "    \"\"\"\n",
    "    Train calibrator dcgc\n",
    "    \"\"\"    \n",
    "    vlss_mn = float('Inf')\n",
    "\n",
    "    labels = data.y\n",
    "    model_dict = temp_model.state_dict()\n",
    "    parameters = {k: v for k,v in model_dict.items() if k.split(\".\")[0] != \"model\"}\n",
    "    for epoch in range(2000):\n",
    "\n",
    "        temp_model.train()\n",
    "        temp_model.optimizer.zero_grad()\n",
    "        logits = temp_model(data.x, data.edge_index, edge_weight)\n",
    "        print()\n",
    "        \n",
    "        loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
    "        print(logits.requires_grad)\n",
    "        # print(loss)\n",
    "        print('loss requires_grad: ', loss.requires_grad)\n",
    "        loss.backward()\n",
    "        temp_model.optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            temp_model.eval()\n",
    "            # logits = temp_model(data.x, data.edge_index, edge_weight)\n",
    "            \n",
    "            val_loss = F.cross_entropy(logits[test_mask], labels[test_mask])\n",
    "            if val_loss <= vlss_mn:\n",
    "                state_dict_early_model = copy.deepcopy(parameters)\n",
    "                vlss_mn = np.min((val_loss.cpu().numpy(), vlss_mn))\n",
    "                # for debug\n",
    "                preds = torch.argmax(logits, dim=1)[test_mask]\n",
    "                acc = torch.mean((preds == data.y[test_mask]).to(torch.get_default_dtype())).item()\n",
    "                ece = ece_fn(logits, data.y)\n",
    "                curr_step = 0\n",
    "            else:\n",
    "                curr_step += 1\n",
    "                if curr_step >= patience:\n",
    "                    break\n",
    "            if verbose:\n",
    "                print(f'Epoch: : {epoch+1:03d}, Accuracy: {acc:.4f}, NNL: {val_loss:.4f}, ECE: {ece:.4f}')\n",
    "    model_dict.update(state_dict_early_model)\n",
    "    \n",
    "    temp_model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62ada003-62b8-4242-803b-7db04641f468",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ew = Edge_Weight(model, dataset.num_classes, dropout=0.7).to(device)\n",
    "ew1 = Edge_Weight(model1, dataset.num_classes, dropout=0.7).to(device)\n",
    "ew2 = Edge_Weight(model2, dataset.num_classes, dropout=0.7).to(device)\n",
    "# ew.fit(data, data.val_mask, data.train_mask, wdecay=0, edge_weight=None, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c716ef3d-0e34-48ee-b3e3-b5898afcf4b1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_weight requires_grad:  True\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mew1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwdecay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 21\u001b[0m, in \u001b[0;36mEdge_Weight.fit\u001b[0;34m(self, data, train_mask, test_mask, wdecay, lr, edge_weight, verbose)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextractor\u001b[38;5;241m.\u001b[39mparameters(),lr\u001b[38;5;241m=\u001b[39mlr, weight_decay\u001b[38;5;241m=\u001b[39mwdecay)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mfit_calibration_dcgc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "Cell \u001b[0;32mIn[25], line 14\u001b[0m, in \u001b[0;36mfit_calibration_dcgc\u001b[0;34m(temp_model, data, train_mask, test_mask, edge_weight, patience, verbose)\u001b[0m\n\u001b[1;32m     12\u001b[0m temp_model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     13\u001b[0m temp_model\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 14\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mtemp_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits[train_mask], labels[train_mask])\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[24], line 14\u001b[0m, in \u001b[0;36mEdge_Weight.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m     12\u001b[0m     edge_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_weight(x, edge_index)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_weight requires_grad: \u001b[39m\u001b[38;5;124m'\u001b[39m, edge_weight\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[0;32m---> 14\u001b[0m logist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogist requires_grad: \u001b[39m\u001b[38;5;124m'\u001b[39m, logist\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logist\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "ew1.fit(data, data.val_mask, data.train_mask, wdecay=0, edge_weight=None, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f82158ad-143b-4e96-af23-9eba631c296c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_weight requires_grad:  True\n",
      "x requires_grad:  False\n",
      "logist requires_grad:  False\n",
      "\n",
      "loss requires_grad:  False\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mew2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwdecay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 22\u001b[0m, in \u001b[0;36mEdge_Weight.fit\u001b[0;34m(self, data, train_mask, test_mask, wdecay, lr, edge_weight, verbose)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextractor\u001b[38;5;241m.\u001b[39mparameters(),lr\u001b[38;5;241m=\u001b[39mlr, weight_decay\u001b[38;5;241m=\u001b[39mwdecay)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mfit_calibration_dcgc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "Cell \u001b[0;32mIn[22], line 24\u001b[0m, in \u001b[0;36mfit_calibration_dcgc\u001b[0;34m(temp_model, data, train_mask, test_mask, edge_weight, patience, verbose)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# print(logits.requires_grad)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# print(loss)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss requires_grad: \u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[0;32m---> 24\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m temp_model\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "ew2.fit(data, data.val_mask, data.train_mask, wdecay=0, edge_weight=None, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6475d648-83fa-4f6a-837f-fed820475c59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TS(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.temperature = nn.Parameter(torch.ones(1) * 1.5)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        logits = self.model(x, edge_index, edge_weight)\n",
    "        temperature = self.temperature_scale(logits)\n",
    "        return logits / temperature\n",
    "\n",
    "    def temperature_scale(self, logits):\n",
    "        \"\"\"\n",
    "        Expand temperature to match the size of logits\n",
    "        \"\"\"\n",
    "        temperature = self.temperature.unsqueeze(1).expand(logits.size(0), logits.size(1))\n",
    "        return temperature\n",
    "\n",
    "    def fit(self, data, train_mask, test_mask, wdecay, edge_weight=None, verbose=False):\n",
    "        self.to(device)\n",
    "        def eval(logits):\n",
    "            temperature = self.temperature_scale(logits)\n",
    "            calibrated = logits / temperature\n",
    "            return calibrated\n",
    "\n",
    "        self.train_param = [self.temperature]\n",
    "        self.optimizer = torch.optim.Adam(self.train_param, lr=0.01, weight_decay=wdecay)\n",
    "        fit_calibration(self, eval, data, train_mask, test_mask, edge_weight=edge_weight, verbose=verbose)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b681aa8-b6e3-4484-aed6-577d34619ca4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit_calibration(temp_model, eval, data, train_mask, test_mask, edge_weight=None, patience = 100, verbose=False):\n",
    "    \"\"\"\n",
    "    Train calibrator\n",
    "    \"\"\"    \n",
    "    vlss_mn = float('Inf')\n",
    "    with torch.no_grad():\n",
    "        logits = temp_model.model(data.x, data.edge_index, edge_weight)\n",
    "        labels = data.y\n",
    "        edge_index = data.edge_index\n",
    "        model_dict = temp_model.state_dict()\n",
    "        parameters = {k: v for k,v in model_dict.items() if k.split(\".\")[0] != \"model\"}\n",
    "    for epoch in range(2000):\n",
    "        temp_model.optimizer.zero_grad()\n",
    "        temp_model.train()\n",
    "        # Post-hoc calibration set the classifier to the evaluation mode\n",
    "        temp_model.model.eval()\n",
    "        assert not temp_model.model.training\n",
    "        calibrated = eval(logits)\n",
    "        loss = F.cross_entropy(calibrated[train_mask], labels[train_mask])\n",
    "        # dist_reg = intra_distance_loss(calibrated[train_mask], labels[train_mask])\n",
    "        # margin_reg = 0.\n",
    "        # loss = loss + margin_reg * dist_reg\n",
    "        loss.backward()\n",
    "        temp_model.optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            temp_model.eval()\n",
    "            calibrated = eval(logits)\n",
    "            val_loss = F.cross_entropy(calibrated[test_mask], labels[test_mask])\n",
    "            # dist_reg = intra_distance_loss(calibrated[train_mask], labels[train_mask])\n",
    "            # val_loss = val_loss + margin_reg * dist_reg\n",
    "            if val_loss <= vlss_mn:\n",
    "                state_dict_early_model = copy.deepcopy(parameters)\n",
    "                vlss_mn = np.min((val_loss.cpu().numpy(), vlss_mn))\n",
    "                # for debug\n",
    "                preds = torch.argmax(logits, dim=1)[test_mask]\n",
    "                acc = torch.mean((preds == data.y[test_mask]).to(torch.get_default_dtype())).item()\n",
    "                ece = ece_fn(logits, data.y)\n",
    "                curr_step = 0\n",
    "            else:\n",
    "                curr_step += 1\n",
    "                if curr_step >= patience:\n",
    "                    break\n",
    "            if verbose:\n",
    "                print(f'Epoch: : {epoch+1:03d}, Accuracy: {acc:.4f}, NNL: {val_loss:.4f}, ECE:{ece:.4f}')\n",
    "    model_dict.update(state_dict_early_model)\n",
    "    temp_model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "154e5b6d-6e49-4f60-bb3d-b029168cc274",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mew\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwdecay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 19\u001b[0m, in \u001b[0;36mEdge_Weight.fit\u001b[0;34m(self, data, train_mask, test_mask, wdecay, lr, edge_weight, verbose)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextractor\u001b[38;5;241m.\u001b[39mparameters(),lr\u001b[38;5;241m=\u001b[39mlr, weight_decay\u001b[38;5;241m=\u001b[39mwdecay)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mfit_calibration_dcgc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "Cell \u001b[0;32mIn[18], line 20\u001b[0m, in \u001b[0;36mfit_calibration_dcgc\u001b[0;34m(temp_model, data, train_mask, test_mask, edge_weight, patience, verbose)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Post-hoc calibration set the classifier to the evaluation mode\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# temp_model.model.eval()\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# assert not temp_model.model.training\u001b[39;00m\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits[train_mask], labels[train_mask])\n\u001b[0;32m---> 20\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m temp_model\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba8b2a3-775d-4fb5-879e-a7bf0904e653",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "968677cd-a702-4251-89b6-aece89dd6c4d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m ew \u001b[38;5;241m=\u001b[39m Edge_Weight(model1, dataset\u001b[38;5;241m.\u001b[39mnum_classes, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# training \u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mew\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwdecay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     11\u001b[0m     ew\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[0;32mIn[17], line 19\u001b[0m, in \u001b[0;36mEdge_Weight.fit\u001b[0;34m(self, data, train_mask, test_mask, wdecay, lr, edge_weight, verbose)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextractor\u001b[38;5;241m.\u001b[39mparameters(),lr\u001b[38;5;241m=\u001b[39mlr, weight_decay\u001b[38;5;241m=\u001b[39mwdecay)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mfit_calibration_dcgc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "Cell \u001b[0;32mIn[18], line 20\u001b[0m, in \u001b[0;36mfit_calibration_dcgc\u001b[0;34m(temp_model, data, train_mask, test_mask, edge_weight, patience, verbose)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Post-hoc calibration set the classifier to the evaluation mode\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# temp_model.model.eval()\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# assert not temp_model.model.training\u001b[39;00m\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits[train_mask], labels[train_mask])\n\u001b[0;32m---> 20\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m temp_model\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "dcgc_beta=10\n",
    "dcgc_alpha = 0.5\n",
    "for run in range(10):\n",
    "    model1 = create_model(dataset, args).to(device)\n",
    "    model1.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    ew = Edge_Weight(model1, dataset.num_classes, dropout=0.7).to(device)\n",
    "    # training \n",
    "    ew.fit(data, data.val_mask, data.train_mask, wdecay=0, edge_weight=None, verbose=False)\n",
    "    with torch.no_grad():\n",
    "        ew.eval()\n",
    "        logits = ew(data.x, data.edge_index)\n",
    "        prob = F.softmax(logits, dim=1)\n",
    "        edge_weight = ew.get_weight(data.x, data.edge_index)\n",
    "    ece1 = ece_fn(logits, data.y)\n",
    "    print(f'run {run} ECE1 : ', ece1.item())\n",
    "    \n",
    "    pred = torch.exp(dcgc_beta * prob)\n",
    "    pred /= torch.sum(pred, dim=1, keepdim=True)\n",
    "\n",
    "    col, row = data.edge_index\n",
    "    coefficient = torch.norm(pred[col] - pred[row], dim=1)\n",
    "    coefficient = 1 / (coefficient + dcgc_alpha)\n",
    "    \n",
    "    edge_weight = edge_weight.reshape(-1)\n",
    "    edge_weight = edge_weight * coefficient\n",
    "    edge_weight = edge_weight.reshape([data.num_edges, 1])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        ew.eval()\n",
    "        logits = ew(data.x, data.edge_index, edge_weight)\n",
    "        prob = F.softmax(logits, dim=1)\n",
    "    ece2 = ece_fn(logits, data.y)\n",
    "    print(f'run {run} DCGC ECE : ', ece2.item())\n",
    "    \n",
    "    ts = TS(model1)\n",
    "    ts.fit(data, data.val_mask, data.train_mask, wdecay=0, edge_weight=None)\n",
    "    with torch.no_grad():\n",
    "        ts.eval()\n",
    "        logits = ts(data.x, data.edge_index)\n",
    "    ece3 = ece_fn(logits, data.y)\n",
    "    print(f'run {run} TS ECE : ', ece3.item())\n",
    "    \n",
    "    \n",
    "    ts = TS(model1)\n",
    "    ts.fit(data, data.val_mask, data.train_mask, wdecay=0, edge_weight=edge_weight)\n",
    "    with torch.no_grad():\n",
    "        ts.eval()\n",
    "        logits = ts(data.x, data.edge_index, edge_weight)\n",
    "    ece4 = ece_fn(logits, data.y)\n",
    "    print(f'run {run} TS+DCGC ECE : ', ece4.item())\n",
    "    print('----------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a93f51-dd2d-45af-b7f7-52e2aff07607",
   "metadata": {},
   "outputs": [],
   "source": [
    "ew = Edge_Weight(model, dataset.num_classes, dropout=0.7).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "024b1169-019a-456b-904f-a4a345eacaf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edge_Weight(\n",
       "  (model): GCN(\n",
       "    (layer_list): ModuleDict(\n",
       "      (conv1): GCNConv(1433, 64)\n",
       "      (conv2): GCNConv(64, 7)\n",
       "    )\n",
       "  )\n",
       "  (extractor): MLP(14, 28, 1)\n",
       ")"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "7758ad16-9efe-4e49-87d5-16eb20d63e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layer_list.conv1.bias requires_grad:  False\n",
      "model.layer_list.conv1.lin.weight requires_grad:  False\n",
      "model.layer_list.conv2.bias requires_grad:  False\n",
      "model.layer_list.conv2.lin.weight requires_grad:  False\n",
      "extractor.lins.0.weight requires_grad:  True\n",
      "extractor.lins.0.bias requires_grad:  True\n",
      "extractor.lins.1.weight requires_grad:  True\n",
      "extractor.lins.1.bias requires_grad:  True\n",
      "extractor.norms.0.module.weight requires_grad:  True\n",
      "extractor.norms.0.module.bias requires_grad:  True\n"
     ]
    }
   ],
   "source": [
    "for name, param in ew.named_parameters():\n",
    "    # if param.requires_grad:\n",
    "    #     print(name)\n",
    "    print(name, 'requires_grad: ', param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "f2812ba9-59af-437b-8c10-fe68e8280ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature requires_grad:  True\n",
      "model.layer_list.conv1.att_src requires_grad:  True\n",
      "model.layer_list.conv1.att_dst requires_grad:  True\n",
      "model.layer_list.conv1.bias requires_grad:  True\n",
      "model.layer_list.conv1.lin_src.weight requires_grad:  True\n",
      "model.layer_list.conv2.att_src requires_grad:  True\n",
      "model.layer_list.conv2.att_dst requires_grad:  True\n",
      "model.layer_list.conv2.bias requires_grad:  True\n",
      "model.layer_list.conv2.lin_src.weight requires_grad:  True\n"
     ]
    }
   ],
   "source": [
    "for name, param in ts.named_parameters():\n",
    "    # if param.requires_grad:\n",
    "    #     print(name)\n",
    "    print(name, 'requires_grad: ', param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5040b41c-8b08-424e-9a55-1a0a553a944b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "6b040f26-36c2-4bf1-8950-067a6b857e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer1 = torch.optim.Adam(ew.extractor.parameters(),lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "optimizer2 = torch.optim.Adam(filter(lambda p: p.requires_grad, ew.parameters()),lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "optimizer3 = torch.optim.Adam(filter(lambda p: p.requires_grad, ts.parameters()),lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5de519d-ef38-433b-8aea-59fb6cb06071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "631b2dd2-ea44-4e92-9b8a-f420ccf017ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 14])\n",
      "torch.Size([28, 14])\n",
      "Parameter containing:\n",
      "tensor([[[ 0.3796, -0.1712,  0.2955, -0.4129,  0.3349, -0.3706,  0.1070,\n",
      "           0.2653],\n",
      "         [-0.3712, -0.1141,  0.2433, -0.3011,  0.0007, -0.0482, -0.2178,\n",
      "           0.4061],\n",
      "         [-0.1434,  0.0167,  0.4106,  0.2659,  0.4136,  0.1064, -0.1923,\n",
      "          -0.1996],\n",
      "         [-0.4281, -0.0906, -0.1002, -0.4382,  0.3606, -0.1047,  0.1988,\n",
      "          -0.0653],\n",
      "         [ 0.0889,  0.3991, -0.1549,  0.2003, -0.0655,  0.4254, -0.2080,\n",
      "           0.1142],\n",
      "         [ 0.0125,  0.5047,  0.0303,  0.2931,  0.2347,  0.1075, -0.3888,\n",
      "          -0.1143],\n",
      "         [-0.3364, -0.0560,  0.0879, -0.1971, -0.2688,  0.2203, -0.0550,\n",
      "           0.3173],\n",
      "         [-0.4845, -0.5075,  0.2733,  0.1145,  0.0184,  0.3727,  0.3794,\n",
      "          -0.0495]]], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param_group in optimizer1.param_groups:\n",
    "    print(param_group['params'][0].shape) \n",
    "    \n",
    "for param_group in optimizer2.param_groups:\n",
    "    print(param_group['params'][0].shape) \n",
    "    \n",
    "for param_group in optimizer3.param_groups:\n",
    "    print(param_group['params'][2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "6ab3b84b-522c-4d0b-ab3a-2acfc7059470",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = ew.state_dict()\n",
    "parameters = {k: v for k,v in model_dict.items() if k.split(\".\")[0] != \"model\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "1cce72c6-37f2-49a8-85bd-e7af55e832fe",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_items([('model.layer_list.conv1.bias', tensor([0.1471, 0.0852, 0.1140, 0.1511, 0.1553, 0.1403, 0.1438, 0.1173, 0.1504,\n",
       "        0.1091, 0.1358, 0.0904, 0.1467, 0.1499, 0.0903, 0.0788, 0.1143, 0.1191,\n",
       "        0.1619, 0.1390, 0.1049, 0.1211, 0.1010, 0.1368, 0.0684, 0.0936, 0.1228,\n",
       "        0.1543, 0.1843, 0.0738, 0.0934, 0.0971, 0.1567, 0.1015, 0.1315, 0.0935,\n",
       "        0.1566, 0.0909, 0.0914, 0.1183, 0.0942, 0.0845, 0.1034, 0.1373, 0.0609,\n",
       "        0.1030, 0.1177, 0.0831, 0.1021, 0.1106, 0.0708, 0.1199, 0.1163, 0.1396,\n",
       "        0.1266, 0.0947, 0.1399, 0.0789, 0.1100, 0.1114, 0.0961, 0.1010, 0.0808,\n",
       "        0.1446], device='cuda:0')), ('model.layer_list.conv1.lin.weight', tensor([[-0.0105, -0.0289,  0.0527,  ..., -0.0041, -0.0372, -0.0432],\n",
       "        [-0.0093,  0.0293, -0.0208,  ..., -0.0130, -0.0551,  0.0081],\n",
       "        [-0.0080,  0.0388, -0.0297,  ..., -0.0002,  0.0090, -0.0137],\n",
       "        ...,\n",
       "        [-0.0652,  0.0062,  0.0349,  ..., -0.0067, -0.0839,  0.0179],\n",
       "        [-0.0358,  0.0062,  0.0201,  ..., -0.0121, -0.1302,  0.0233],\n",
       "        [ 0.0048, -0.0368,  0.0167,  ..., -0.0019, -0.0651,  0.0099]],\n",
       "       device='cuda:0')), ('model.layer_list.conv2.bias', tensor([-0.1201, -0.1391,  0.1540,  0.1983,  0.0972, -0.1515,  0.0186],\n",
       "       device='cuda:0')), ('model.layer_list.conv2.lin.weight', tensor([[-0.0966,  0.1217, -0.2509,  0.4092, -0.2612, -0.1145,  0.1135,  0.3378,\n",
       "          0.3868,  0.3215,  0.5080,  0.2685,  0.1029,  0.2549, -0.1287, -0.4521,\n",
       "         -0.4128,  0.0046,  0.3408, -0.1233, -0.2668,  0.4364,  0.4392, -0.5029,\n",
       "         -0.4802,  0.2818,  0.1730,  0.2684, -0.1646, -0.4795, -0.3587, -0.2560,\n",
       "         -0.3378,  0.4847,  0.4270,  0.3550, -0.3267, -0.2020, -0.3174, -0.3877,\n",
       "          0.3893, -0.4437, -0.2148, -0.4806, -0.0280, -0.4140, -0.3360, -0.5091,\n",
       "         -0.2344,  0.4493,  0.4073,  0.0644,  0.3087, -0.3944, -0.1885,  0.3634,\n",
       "         -0.4336, -0.4594, -0.0787,  0.4941, -0.4894, -0.0297,  0.3404,  0.3161],\n",
       "        [ 0.2771, -0.1572,  0.0386, -0.4038,  0.2364,  0.4184,  0.1518, -0.4996,\n",
       "          0.2233,  0.1499, -0.1399, -0.3135,  0.2563,  0.3874,  0.4885, -0.1184,\n",
       "          0.4466, -0.1115,  0.2378, -0.5328, -0.4760, -0.3701,  0.2091, -0.2361,\n",
       "          0.2115, -0.5102,  0.3876, -0.2081,  0.2707, -0.3138,  0.2930,  0.1732,\n",
       "         -0.5204, -0.0715,  0.2923, -0.4240, -0.1489,  0.4687, -0.3390,  0.1789,\n",
       "         -0.3342,  0.3567,  0.1791,  0.2249,  0.4090, -0.1654, -0.4371,  0.0989,\n",
       "         -0.3588, -0.4532, -0.3664,  0.4636,  0.0348,  0.0939, -0.4471,  0.1272,\n",
       "         -0.3846, -0.4136,  0.1637, -0.0813, -0.3887,  0.3943,  0.2744, -0.3670],\n",
       "        [-0.1684,  0.4504,  0.4116, -0.4965,  0.3225,  0.2366, -0.4883,  0.4848,\n",
       "         -0.2464, -0.4575,  0.3428,  0.3365,  0.2631, -0.3683, -0.4951, -0.3145,\n",
       "         -0.3901,  0.4015, -0.4564, -0.0418, -0.3206, -0.2766, -0.2185, -0.0008,\n",
       "          0.3309,  0.2471,  0.3564, -0.1587, -0.4805,  0.2159,  0.2213, -0.4897,\n",
       "          0.3555,  0.4126,  0.1227, -0.2524, -0.1985, -0.4361, -0.3893,  0.2498,\n",
       "         -0.3331,  0.4661,  0.2880, -0.4201,  0.4356, -0.3717,  0.2467,  0.3670,\n",
       "          0.2509, -0.3297,  0.2864,  0.0747, -0.4384, -0.4388, -0.2725,  0.0137,\n",
       "          0.1006,  0.4565, -0.4630, -0.2196,  0.0946,  0.4316,  0.3936, -0.4509],\n",
       "        [ 0.3744, -0.1597,  0.2536, -0.4227, -0.1833, -0.4124,  0.2090, -0.4668,\n",
       "          0.2686,  0.3216, -0.1547,  0.3410, -0.5258,  0.1498, -0.4044, -0.3839,\n",
       "         -0.3916,  0.2674,  0.3149,  0.0716,  0.1754,  0.3688,  0.3810,  0.2112,\n",
       "         -0.2544,  0.3475, -0.2589, -0.4581,  0.1846,  0.2804, -0.5128, -0.4697,\n",
       "         -0.0178,  0.3846, -0.4104,  0.4010, -0.2312,  0.4090,  0.2930, -0.3200,\n",
       "         -0.2775,  0.3590, -0.1882,  0.4382, -0.1216,  0.3686, -0.2883, -0.1253,\n",
       "          0.2577,  0.2552, -0.4543, -0.3380,  0.2379,  0.4370,  0.4215, -0.4611,\n",
       "         -0.1474, -0.1634,  0.3275, -0.2903,  0.2987, -0.3304, -0.4336, -0.2807],\n",
       "        [-0.2216,  0.4616, -0.2622, -0.2000,  0.4957,  0.4057,  0.1851, -0.2876,\n",
       "         -0.3204, -0.0786,  0.2035, -0.2284, -0.4264,  0.3301, -0.3884,  0.1819,\n",
       "          0.2737,  0.3705,  0.3873, -0.2265,  0.2168, -0.3083,  0.2170, -0.4275,\n",
       "         -0.3285,  0.3786, -0.3769,  0.2091,  0.5173, -0.4427, -0.3871, -0.3416,\n",
       "          0.3866,  0.2017, -0.3539, -0.3414,  0.3811,  0.1305, -0.3607,  0.3682,\n",
       "         -0.2549, -0.1680,  0.3404, -0.2056, -0.2933, -0.2366, -0.3846, -0.3789,\n",
       "         -0.4429,  0.4399, -0.3008, -0.4882, -0.4400, -0.2125,  0.4517, -0.4151,\n",
       "          0.1895, -0.1881,  0.3819, -0.2416, -0.2834,  0.4554,  0.4328,  0.1837],\n",
       "        [ 0.4154, -0.0909, -0.1519,  0.3753, -0.0533, -0.4403, -0.3335, -0.0542,\n",
       "         -0.2781, -0.2421,  0.3886,  0.3048,  0.1212, -0.4396, -0.0182,  0.3873,\n",
       "          0.0603, -0.2477, -0.1456,  0.2023, -0.2782, -0.3802, -0.0661,  0.1071,\n",
       "          0.3739,  0.1363,  0.0762, -0.2024,  0.2115,  0.2411,  0.3920,  0.3274,\n",
       "         -0.2453, -0.1911, -0.3352, -0.3838,  0.4259, -0.4379,  0.0085,  0.3046,\n",
       "         -0.3971, -0.0420, -0.3604,  0.4583, -0.2301,  0.3304,  0.5053, -0.4648,\n",
       "          0.2470, -0.3763,  0.4836, -0.4107, -0.1915,  0.4147,  0.1173,  0.2117,\n",
       "          0.3946, -0.2107,  0.2981, -0.4008, -0.0224, -0.3660, -0.1728,  0.1443],\n",
       "        [-0.4673, -0.2969,  0.1255,  0.2617, -0.2443, -0.0256,  0.2805,  0.2657,\n",
       "         -0.4936,  0.3199, -0.4253, -0.2807,  0.0375, -0.3015,  0.3113, -0.2688,\n",
       "          0.2896,  0.1392, -0.4653,  0.3982,  0.2676, -0.4627, -0.2816, -0.4554,\n",
       "         -0.2276, -0.4659,  0.3773,  0.3153,  0.1765,  0.2056, -0.2682, -0.3808,\n",
       "         -0.2945, -0.0811,  0.1667,  0.1726, -0.1916, -0.2828,  0.1800, -0.2245,\n",
       "          0.3661, -0.2768,  0.4334, -0.3870, -0.3844,  0.3792, -0.2877,  0.1785,\n",
       "          0.0191, -0.3933,  0.2556,  0.2933,  0.2189, -0.1764, -0.4271,  0.1812,\n",
       "          0.2995,  0.4629, -0.3348,  0.3914,  0.3896, -0.3345, -0.3601, -0.3926]],\n",
       "       device='cuda:0')), ('extractor.lins.0.weight', tensor([[ 0.1337,  0.1620, -0.2581, -0.1105, -0.2221, -0.2474,  0.0641, -0.1344,\n",
       "          0.1057,  0.0074,  0.0109, -0.0435,  0.1393, -0.2382],\n",
       "        [ 0.2116,  0.0565, -0.2333, -0.2119, -0.2113,  0.0923, -0.2551,  0.2014,\n",
       "         -0.1888,  0.0917, -0.1823, -0.2223,  0.0856, -0.1191],\n",
       "        [-0.1981, -0.0439, -0.1955,  0.2443,  0.2541, -0.2596, -0.0643,  0.0102,\n",
       "         -0.1709, -0.2005, -0.1953,  0.0375,  0.0622,  0.0377],\n",
       "        [-0.2663, -0.1189, -0.0696, -0.2214,  0.0675,  0.1261, -0.2112,  0.2127,\n",
       "         -0.2101,  0.0401, -0.2613,  0.2323, -0.1987,  0.1018],\n",
       "        [ 0.2505, -0.1149,  0.1537, -0.1680, -0.1875, -0.0164,  0.0421, -0.0627,\n",
       "         -0.0206,  0.1091,  0.0536,  0.2562,  0.1857, -0.2610],\n",
       "        [ 0.2221,  0.2378, -0.1127,  0.2133,  0.2387, -0.1909,  0.1848,  0.2265,\n",
       "          0.2396,  0.2259, -0.2047,  0.1182,  0.1542,  0.1022],\n",
       "        [ 0.0723, -0.0086, -0.1636,  0.2442, -0.1462,  0.1999, -0.0330,  0.2535,\n",
       "          0.1055,  0.2488, -0.1285, -0.1173,  0.1179, -0.0175],\n",
       "        [ 0.0192, -0.1251,  0.1876,  0.1343,  0.2530,  0.2533, -0.2397,  0.0761,\n",
       "         -0.1676,  0.2401,  0.0696,  0.0625, -0.0859, -0.1315],\n",
       "        [-0.0830, -0.1702, -0.0614, -0.1092, -0.1960,  0.0407,  0.2373,  0.0040,\n",
       "         -0.1839, -0.0771,  0.1207,  0.2414,  0.1637,  0.1228],\n",
       "        [-0.1572,  0.0177,  0.1257, -0.1038,  0.0297,  0.1949, -0.1052,  0.1791,\n",
       "          0.1347, -0.1797, -0.0066,  0.0740,  0.2125,  0.1388],\n",
       "        [ 0.2048, -0.0116, -0.1303, -0.0631,  0.0073, -0.0052,  0.1740,  0.0808,\n",
       "          0.0200, -0.2663,  0.1207,  0.1975,  0.1448,  0.1838],\n",
       "        [-0.1914,  0.0642,  0.1323, -0.0190,  0.2247,  0.1744,  0.0801, -0.0628,\n",
       "         -0.0499,  0.0194, -0.1198, -0.2505,  0.0389, -0.2443],\n",
       "        [ 0.1775, -0.2427, -0.2163, -0.2208, -0.1297, -0.2190, -0.2101,  0.0608,\n",
       "          0.2294,  0.2340,  0.0773, -0.2004, -0.1803, -0.0911],\n",
       "        [-0.1004, -0.1538,  0.0527, -0.2648,  0.1877, -0.0264,  0.2066, -0.0263,\n",
       "          0.1182, -0.0979, -0.1141, -0.1721,  0.2565,  0.0412],\n",
       "        [-0.2227,  0.1998, -0.2606, -0.2581, -0.1626, -0.1209, -0.1471, -0.1504,\n",
       "          0.1005,  0.0639, -0.2379,  0.0330,  0.0452,  0.1034],\n",
       "        [ 0.1287,  0.1364,  0.0957,  0.0234, -0.1149, -0.1425, -0.1533,  0.2178,\n",
       "          0.0641,  0.1287, -0.1310, -0.1815, -0.0702, -0.0622],\n",
       "        [ 0.1391, -0.1480, -0.2448, -0.2065,  0.0563,  0.1555, -0.1497,  0.2470,\n",
       "         -0.0186, -0.2565, -0.0935,  0.0164,  0.0152,  0.2274],\n",
       "        [ 0.1038,  0.0210,  0.0926,  0.0825, -0.0561,  0.2417,  0.1780, -0.2483,\n",
       "          0.2286,  0.1138,  0.0224,  0.2266,  0.0555,  0.2551],\n",
       "        [-0.2277,  0.2375,  0.2048, -0.0724,  0.2386, -0.1155,  0.2482,  0.1239,\n",
       "          0.0836,  0.0638,  0.1935,  0.1569,  0.1565,  0.0267],\n",
       "        [-0.1255,  0.0669, -0.1718, -0.1387, -0.1247,  0.2615,  0.0104,  0.2193,\n",
       "          0.1081, -0.0399, -0.0662,  0.1535, -0.2064, -0.0773],\n",
       "        [ 0.2536, -0.1186, -0.1505, -0.1868, -0.1260,  0.0306,  0.2219, -0.2440,\n",
       "         -0.2261,  0.2617, -0.1127, -0.1589, -0.0078,  0.2580],\n",
       "        [ 0.2212,  0.1778, -0.1046,  0.1203, -0.0477,  0.1082,  0.2415, -0.1753,\n",
       "          0.1345,  0.1868, -0.2596,  0.1871, -0.2570,  0.0956],\n",
       "        [ 0.1786,  0.2055, -0.2017,  0.1998, -0.2092, -0.2320,  0.2657,  0.2200,\n",
       "         -0.1606, -0.1501, -0.1357,  0.1909, -0.1696,  0.0938],\n",
       "        [ 0.0708, -0.0413, -0.0316,  0.1542, -0.0816, -0.0054,  0.2128,  0.1631,\n",
       "         -0.1187,  0.1992,  0.0839,  0.0828,  0.0458,  0.1759],\n",
       "        [-0.0841,  0.1718,  0.0430, -0.0999, -0.1529, -0.0523,  0.0970, -0.0052,\n",
       "         -0.1102,  0.0113, -0.0758, -0.0006, -0.2597,  0.2041],\n",
       "        [-0.0141, -0.1506, -0.0235, -0.0908, -0.0586, -0.2389,  0.2174, -0.1060,\n",
       "          0.2635, -0.1074,  0.1826,  0.0354,  0.2079,  0.1814],\n",
       "        [-0.2488,  0.1940, -0.0843, -0.2332,  0.1410,  0.1162,  0.2269, -0.0341,\n",
       "          0.0878, -0.1506, -0.1961, -0.1974,  0.0363, -0.1746],\n",
       "        [ 0.1058,  0.1788,  0.1566, -0.2280,  0.0137,  0.1795, -0.0765,  0.0003,\n",
       "          0.0999,  0.1051, -0.0968, -0.0080, -0.1188, -0.0768]],\n",
       "       device='cuda:0')), ('extractor.lins.0.bias', tensor([-0.0730, -0.0217,  0.0078, -0.1053,  0.1050, -0.0039,  0.2546, -0.2645,\n",
       "        -0.0950, -0.1565, -0.1969, -0.0824,  0.2193, -0.1972, -0.0072,  0.2202,\n",
       "         0.2322,  0.0755, -0.1313, -0.2574,  0.0381, -0.1875, -0.0521,  0.1476,\n",
       "         0.1151,  0.1482, -0.2120,  0.1938], device='cuda:0')), ('extractor.lins.1.weight', tensor([[-0.0677, -0.0657, -0.0424, -0.0608, -0.0603, -0.0748,  0.1601, -0.1817,\n",
       "          0.0582,  0.0897,  0.1029, -0.0949,  0.1691,  0.1881,  0.0300, -0.0543,\n",
       "         -0.1864, -0.0765,  0.0561,  0.1376,  0.0481,  0.1285,  0.1039, -0.0122,\n",
       "          0.0845, -0.0269,  0.0992, -0.0734]], device='cuda:0')), ('extractor.lins.1.bias', tensor([0.1831], device='cuda:0')), ('extractor.norms.0.module.weight', tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')), ('extractor.norms.0.module.bias', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.], device='cuda:0')), ('extractor.norms.0.module.running_mean', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.], device='cuda:0')), ('extractor.norms.0.module.running_var', tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')), ('extractor.norms.0.module.num_batches_tracked', tensor(0, device='cuda:0'))])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "78a71b91-a789-4808-b76b-d3369bb572bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['extractor.lins.0.weight', 'extractor.lins.0.bias', 'extractor.lins.1.weight', 'extractor.lins.1.bias', 'extractor.norms.0.module.weight', 'extractor.norms.0.module.bias', 'extractor.norms.0.module.running_mean', 'extractor.norms.0.module.running_var', 'extractor.norms.0.module.num_batches_tracked'])\n",
      "------------------------------------------\n",
      "odict_keys(['model.layer_list.conv1.bias', 'model.layer_list.conv1.lin.weight', 'model.layer_list.conv2.bias', 'model.layer_list.conv2.lin.weight', 'extractor.lins.0.weight', 'extractor.lins.0.bias', 'extractor.lins.1.weight', 'extractor.lins.1.bias', 'extractor.norms.0.module.weight', 'extractor.norms.0.module.bias', 'extractor.norms.0.module.running_mean', 'extractor.norms.0.module.running_var', 'extractor.norms.0.module.num_batches_tracked'])\n"
     ]
    }
   ],
   "source": [
    "parameters = {k: v for k,v in model_dict.items() if k.split(\".\")[0] != \"model\"}\n",
    "print(parameters.keys())\n",
    "print('------------------------------------------')\n",
    "print(model_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "1afdf4ea-a6a3-4307-b631-23155a5994ad",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[156], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# fit_calibration_dcgc(ew, data, data.train_mask, data.test_mask, edge_weight=None, verbose=True)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mew\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwdecay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[144], line 19\u001b[0m, in \u001b[0;36mEdge_Weight.fit\u001b[0;34m(self, data, train_mask, test_mask, wdecay, lr, edge_weight, verbose)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextractor\u001b[38;5;241m.\u001b[39mparameters(),lr\u001b[38;5;241m=\u001b[39mlr, weight_decay\u001b[38;5;241m=\u001b[39mwdecay)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mfit_calibration_dcgc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "Cell \u001b[0;32mIn[147], line 20\u001b[0m, in \u001b[0;36mfit_calibration_dcgc\u001b[0;34m(temp_model, data, train_mask, test_mask, edge_weight, patience, verbose)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Post-hoc calibration set the classifier to the evaluation mode\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# temp_model.model.eval()\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# assert not temp_model.model.training\u001b[39;00m\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits[train_mask], labels[train_mask])\n\u001b[0;32m---> 20\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m temp_model\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "\n",
    "# fit_calibration_dcgc(ew, data, data.train_mask, data.test_mask, edge_weight=None, verbose=True)\n",
    "ew.fit(data, data.val_mask, data.train_mask, wdecay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf07bac-783b-4480-bb29-00874585b18f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "0e8cf4fc-4fa3-441e-8e9b-0fffbb5bf9c9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: : 001, Accuracy: 0.9143, NNL: 0.4729, ECE:0.0751\n",
      "Epoch: : 002, Accuracy: 0.9143, NNL: 0.4704, ECE:0.0751\n",
      "Epoch: : 003, Accuracy: 0.9143, NNL: 0.4679, ECE:0.0751\n",
      "Epoch: : 004, Accuracy: 0.9143, NNL: 0.4654, ECE:0.0751\n",
      "Epoch: : 005, Accuracy: 0.9143, NNL: 0.4629, ECE:0.0751\n",
      "Epoch: : 006, Accuracy: 0.9143, NNL: 0.4604, ECE:0.0751\n",
      "Epoch: : 007, Accuracy: 0.9143, NNL: 0.4579, ECE:0.0751\n",
      "Epoch: : 008, Accuracy: 0.9143, NNL: 0.4554, ECE:0.0751\n",
      "Epoch: : 009, Accuracy: 0.9143, NNL: 0.4529, ECE:0.0751\n",
      "Epoch: : 010, Accuracy: 0.9143, NNL: 0.4504, ECE:0.0751\n",
      "Epoch: : 011, Accuracy: 0.9143, NNL: 0.4479, ECE:0.0751\n",
      "Epoch: : 012, Accuracy: 0.9143, NNL: 0.4454, ECE:0.0751\n",
      "Epoch: : 013, Accuracy: 0.9143, NNL: 0.4430, ECE:0.0751\n",
      "Epoch: : 014, Accuracy: 0.9143, NNL: 0.4405, ECE:0.0751\n",
      "Epoch: : 015, Accuracy: 0.9143, NNL: 0.4381, ECE:0.0751\n",
      "Epoch: : 016, Accuracy: 0.9143, NNL: 0.4357, ECE:0.0751\n",
      "Epoch: : 017, Accuracy: 0.9143, NNL: 0.4333, ECE:0.0751\n",
      "Epoch: : 018, Accuracy: 0.9143, NNL: 0.4309, ECE:0.0751\n",
      "Epoch: : 019, Accuracy: 0.9143, NNL: 0.4285, ECE:0.0751\n",
      "Epoch: : 020, Accuracy: 0.9143, NNL: 0.4262, ECE:0.0751\n",
      "Epoch: : 021, Accuracy: 0.9143, NNL: 0.4239, ECE:0.0751\n",
      "Epoch: : 022, Accuracy: 0.9143, NNL: 0.4216, ECE:0.0751\n",
      "Epoch: : 023, Accuracy: 0.9143, NNL: 0.4193, ECE:0.0751\n",
      "Epoch: : 024, Accuracy: 0.9143, NNL: 0.4170, ECE:0.0751\n",
      "Epoch: : 025, Accuracy: 0.9143, NNL: 0.4148, ECE:0.0751\n",
      "Epoch: : 026, Accuracy: 0.9143, NNL: 0.4126, ECE:0.0751\n",
      "Epoch: : 027, Accuracy: 0.9143, NNL: 0.4105, ECE:0.0751\n",
      "Epoch: : 028, Accuracy: 0.9143, NNL: 0.4083, ECE:0.0751\n",
      "Epoch: : 029, Accuracy: 0.9143, NNL: 0.4062, ECE:0.0751\n",
      "Epoch: : 030, Accuracy: 0.9143, NNL: 0.4042, ECE:0.0751\n",
      "Epoch: : 031, Accuracy: 0.9143, NNL: 0.4022, ECE:0.0751\n",
      "Epoch: : 032, Accuracy: 0.9143, NNL: 0.4002, ECE:0.0751\n",
      "Epoch: : 033, Accuracy: 0.9143, NNL: 0.3983, ECE:0.0751\n",
      "Epoch: : 034, Accuracy: 0.9143, NNL: 0.3964, ECE:0.0751\n",
      "Epoch: : 035, Accuracy: 0.9143, NNL: 0.3946, ECE:0.0751\n",
      "Epoch: : 036, Accuracy: 0.9143, NNL: 0.3928, ECE:0.0751\n",
      "Epoch: : 037, Accuracy: 0.9143, NNL: 0.3910, ECE:0.0751\n",
      "Epoch: : 038, Accuracy: 0.9143, NNL: 0.3894, ECE:0.0751\n",
      "Epoch: : 039, Accuracy: 0.9143, NNL: 0.3878, ECE:0.0751\n",
      "Epoch: : 040, Accuracy: 0.9143, NNL: 0.3862, ECE:0.0751\n",
      "Epoch: : 041, Accuracy: 0.9143, NNL: 0.3847, ECE:0.0751\n",
      "Epoch: : 042, Accuracy: 0.9143, NNL: 0.3832, ECE:0.0751\n",
      "Epoch: : 043, Accuracy: 0.9143, NNL: 0.3819, ECE:0.0751\n",
      "Epoch: : 044, Accuracy: 0.9143, NNL: 0.3806, ECE:0.0751\n",
      "Epoch: : 045, Accuracy: 0.9143, NNL: 0.3793, ECE:0.0751\n",
      "Epoch: : 046, Accuracy: 0.9143, NNL: 0.3781, ECE:0.0751\n",
      "Epoch: : 047, Accuracy: 0.9143, NNL: 0.3770, ECE:0.0751\n",
      "Epoch: : 048, Accuracy: 0.9143, NNL: 0.3760, ECE:0.0751\n",
      "Epoch: : 049, Accuracy: 0.9143, NNL: 0.3750, ECE:0.0751\n",
      "Epoch: : 050, Accuracy: 0.9143, NNL: 0.3741, ECE:0.0751\n",
      "Epoch: : 051, Accuracy: 0.9143, NNL: 0.3732, ECE:0.0751\n",
      "Epoch: : 052, Accuracy: 0.9143, NNL: 0.3724, ECE:0.0751\n",
      "Epoch: : 053, Accuracy: 0.9143, NNL: 0.3717, ECE:0.0751\n",
      "Epoch: : 054, Accuracy: 0.9143, NNL: 0.3711, ECE:0.0751\n",
      "Epoch: : 055, Accuracy: 0.9143, NNL: 0.3705, ECE:0.0751\n",
      "Epoch: : 056, Accuracy: 0.9143, NNL: 0.3700, ECE:0.0751\n",
      "Epoch: : 057, Accuracy: 0.9143, NNL: 0.3695, ECE:0.0751\n",
      "Epoch: : 058, Accuracy: 0.9143, NNL: 0.3691, ECE:0.0751\n",
      "Epoch: : 059, Accuracy: 0.9143, NNL: 0.3687, ECE:0.0751\n",
      "Epoch: : 060, Accuracy: 0.9143, NNL: 0.3685, ECE:0.0751\n",
      "Epoch: : 061, Accuracy: 0.9143, NNL: 0.3682, ECE:0.0751\n",
      "Epoch: : 062, Accuracy: 0.9143, NNL: 0.3680, ECE:0.0751\n",
      "Epoch: : 063, Accuracy: 0.9143, NNL: 0.3679, ECE:0.0751\n",
      "Epoch: : 064, Accuracy: 0.9143, NNL: 0.3678, ECE:0.0751\n",
      "Epoch: : 065, Accuracy: 0.9143, NNL: 0.3677, ECE:0.0751\n",
      "Epoch: : 066, Accuracy: 0.9143, NNL: 0.3677, ECE:0.0751\n",
      "Epoch: : 067, Accuracy: 0.9143, NNL: 0.3677, ECE:0.0751\n",
      "Epoch: : 068, Accuracy: 0.9143, NNL: 0.3677, ECE:0.0751\n",
      "Epoch: : 069, Accuracy: 0.9143, NNL: 0.3678, ECE:0.0751\n",
      "Epoch: : 070, Accuracy: 0.9143, NNL: 0.3679, ECE:0.0751\n",
      "Epoch: : 071, Accuracy: 0.9143, NNL: 0.3680, ECE:0.0751\n",
      "Epoch: : 072, Accuracy: 0.9143, NNL: 0.3682, ECE:0.0751\n",
      "Epoch: : 073, Accuracy: 0.9143, NNL: 0.3683, ECE:0.0751\n",
      "Epoch: : 074, Accuracy: 0.9143, NNL: 0.3685, ECE:0.0751\n",
      "Epoch: : 075, Accuracy: 0.9143, NNL: 0.3687, ECE:0.0751\n",
      "Epoch: : 076, Accuracy: 0.9143, NNL: 0.3689, ECE:0.0751\n",
      "Epoch: : 077, Accuracy: 0.9143, NNL: 0.3691, ECE:0.0751\n",
      "Epoch: : 078, Accuracy: 0.9143, NNL: 0.3693, ECE:0.0751\n",
      "Epoch: : 079, Accuracy: 0.9143, NNL: 0.3695, ECE:0.0751\n",
      "Epoch: : 080, Accuracy: 0.9143, NNL: 0.3697, ECE:0.0751\n",
      "Epoch: : 081, Accuracy: 0.9143, NNL: 0.3699, ECE:0.0751\n",
      "Epoch: : 082, Accuracy: 0.9143, NNL: 0.3701, ECE:0.0751\n",
      "Epoch: : 083, Accuracy: 0.9143, NNL: 0.3703, ECE:0.0751\n",
      "Epoch: : 084, Accuracy: 0.9143, NNL: 0.3705, ECE:0.0751\n",
      "Epoch: : 085, Accuracy: 0.9143, NNL: 0.3706, ECE:0.0751\n",
      "Epoch: : 086, Accuracy: 0.9143, NNL: 0.3708, ECE:0.0751\n",
      "Epoch: : 087, Accuracy: 0.9143, NNL: 0.3710, ECE:0.0751\n",
      "Epoch: : 088, Accuracy: 0.9143, NNL: 0.3711, ECE:0.0751\n",
      "Epoch: : 089, Accuracy: 0.9143, NNL: 0.3713, ECE:0.0751\n",
      "Epoch: : 090, Accuracy: 0.9143, NNL: 0.3714, ECE:0.0751\n",
      "Epoch: : 091, Accuracy: 0.9143, NNL: 0.3715, ECE:0.0751\n",
      "Epoch: : 092, Accuracy: 0.9143, NNL: 0.3716, ECE:0.0751\n",
      "Epoch: : 093, Accuracy: 0.9143, NNL: 0.3717, ECE:0.0751\n",
      "Epoch: : 094, Accuracy: 0.9143, NNL: 0.3718, ECE:0.0751\n",
      "Epoch: : 095, Accuracy: 0.9143, NNL: 0.3719, ECE:0.0751\n",
      "Epoch: : 096, Accuracy: 0.9143, NNL: 0.3720, ECE:0.0751\n",
      "Epoch: : 097, Accuracy: 0.9143, NNL: 0.3720, ECE:0.0751\n",
      "Epoch: : 098, Accuracy: 0.9143, NNL: 0.3721, ECE:0.0751\n",
      "Epoch: : 099, Accuracy: 0.9143, NNL: 0.3721, ECE:0.0751\n",
      "Epoch: : 100, Accuracy: 0.9143, NNL: 0.3722, ECE:0.0751\n",
      "Epoch: : 101, Accuracy: 0.9143, NNL: 0.3722, ECE:0.0751\n",
      "Epoch: : 102, Accuracy: 0.9143, NNL: 0.3722, ECE:0.0751\n",
      "Epoch: : 103, Accuracy: 0.9143, NNL: 0.3722, ECE:0.0751\n",
      "Epoch: : 104, Accuracy: 0.9143, NNL: 0.3722, ECE:0.0751\n",
      "Epoch: : 105, Accuracy: 0.9143, NNL: 0.3722, ECE:0.0751\n",
      "Epoch: : 106, Accuracy: 0.9143, NNL: 0.3722, ECE:0.0751\n",
      "Epoch: : 107, Accuracy: 0.9143, NNL: 0.3722, ECE:0.0751\n",
      "Epoch: : 108, Accuracy: 0.9143, NNL: 0.3722, ECE:0.0751\n",
      "Epoch: : 109, Accuracy: 0.9143, NNL: 0.3721, ECE:0.0751\n",
      "Epoch: : 110, Accuracy: 0.9143, NNL: 0.3721, ECE:0.0751\n",
      "Epoch: : 111, Accuracy: 0.9143, NNL: 0.3721, ECE:0.0751\n",
      "Epoch: : 112, Accuracy: 0.9143, NNL: 0.3721, ECE:0.0751\n",
      "Epoch: : 113, Accuracy: 0.9143, NNL: 0.3720, ECE:0.0751\n",
      "Epoch: : 114, Accuracy: 0.9143, NNL: 0.3720, ECE:0.0751\n",
      "Epoch: : 115, Accuracy: 0.9143, NNL: 0.3720, ECE:0.0751\n",
      "Epoch: : 116, Accuracy: 0.9143, NNL: 0.3719, ECE:0.0751\n",
      "Epoch: : 117, Accuracy: 0.9143, NNL: 0.3719, ECE:0.0751\n",
      "Epoch: : 118, Accuracy: 0.9143, NNL: 0.3719, ECE:0.0751\n",
      "Epoch: : 119, Accuracy: 0.9143, NNL: 0.3718, ECE:0.0751\n",
      "Epoch: : 120, Accuracy: 0.9143, NNL: 0.3718, ECE:0.0751\n",
      "Epoch: : 121, Accuracy: 0.9143, NNL: 0.3718, ECE:0.0751\n",
      "Epoch: : 122, Accuracy: 0.9143, NNL: 0.3717, ECE:0.0751\n",
      "Epoch: : 123, Accuracy: 0.9143, NNL: 0.3717, ECE:0.0751\n",
      "Epoch: : 124, Accuracy: 0.9143, NNL: 0.3717, ECE:0.0751\n",
      "Epoch: : 125, Accuracy: 0.9143, NNL: 0.3717, ECE:0.0751\n",
      "Epoch: : 126, Accuracy: 0.9143, NNL: 0.3716, ECE:0.0751\n",
      "Epoch: : 127, Accuracy: 0.9143, NNL: 0.3716, ECE:0.0751\n",
      "Epoch: : 128, Accuracy: 0.9143, NNL: 0.3716, ECE:0.0751\n",
      "Epoch: : 129, Accuracy: 0.9143, NNL: 0.3716, ECE:0.0751\n",
      "Epoch: : 130, Accuracy: 0.9143, NNL: 0.3716, ECE:0.0751\n",
      "Epoch: : 131, Accuracy: 0.9143, NNL: 0.3715, ECE:0.0751\n",
      "Epoch: : 132, Accuracy: 0.9143, NNL: 0.3715, ECE:0.0751\n",
      "Epoch: : 133, Accuracy: 0.9143, NNL: 0.3715, ECE:0.0751\n",
      "Epoch: : 134, Accuracy: 0.9143, NNL: 0.3715, ECE:0.0751\n",
      "Epoch: : 135, Accuracy: 0.9143, NNL: 0.3715, ECE:0.0751\n",
      "Epoch: : 136, Accuracy: 0.9143, NNL: 0.3715, ECE:0.0751\n",
      "Epoch: : 137, Accuracy: 0.9143, NNL: 0.3715, ECE:0.0751\n",
      "Epoch: : 138, Accuracy: 0.9143, NNL: 0.3715, ECE:0.0751\n",
      "Epoch: : 139, Accuracy: 0.9143, NNL: 0.3715, ECE:0.0751\n",
      "Epoch: : 140, Accuracy: 0.9143, NNL: 0.3715, ECE:0.0751\n",
      "Epoch: : 141, Accuracy: 0.9143, NNL: 0.3715, ECE:0.0751\n",
      "Epoch: : 142, Accuracy: 0.9143, NNL: 0.3715, ECE:0.0751\n",
      "Epoch: : 143, Accuracy: 0.9143, NNL: 0.3715, ECE:0.0751\n",
      "Epoch: : 144, Accuracy: 0.9143, NNL: 0.3715, ECE:0.0751\n",
      "Epoch: : 145, Accuracy: 0.9143, NNL: 0.3715, ECE:0.0751\n",
      "Epoch: : 146, Accuracy: 0.9143, NNL: 0.3716, ECE:0.0751\n",
      "Epoch: : 147, Accuracy: 0.9143, NNL: 0.3716, ECE:0.0751\n",
      "Epoch: : 148, Accuracy: 0.9143, NNL: 0.3716, ECE:0.0751\n",
      "Epoch: : 149, Accuracy: 0.9143, NNL: 0.3716, ECE:0.0751\n",
      "Epoch: : 150, Accuracy: 0.9143, NNL: 0.3716, ECE:0.0751\n",
      "Epoch: : 151, Accuracy: 0.9143, NNL: 0.3716, ECE:0.0751\n",
      "Epoch: : 152, Accuracy: 0.9143, NNL: 0.3716, ECE:0.0751\n",
      "Epoch: : 153, Accuracy: 0.9143, NNL: 0.3716, ECE:0.0751\n",
      "Epoch: : 154, Accuracy: 0.9143, NNL: 0.3716, ECE:0.0751\n",
      "Epoch: : 155, Accuracy: 0.9143, NNL: 0.3716, ECE:0.0751\n",
      "Epoch: : 156, Accuracy: 0.9143, NNL: 0.3716, ECE:0.0751\n",
      "Epoch: : 157, Accuracy: 0.9143, NNL: 0.3716, ECE:0.0751\n",
      "Epoch: : 158, Accuracy: 0.9143, NNL: 0.3716, ECE:0.0751\n",
      "Epoch: : 159, Accuracy: 0.9143, NNL: 0.3716, ECE:0.0751\n",
      "Epoch: : 160, Accuracy: 0.9143, NNL: 0.3716, ECE:0.0751\n",
      "Epoch: : 161, Accuracy: 0.9143, NNL: 0.3716, ECE:0.0751\n",
      "Epoch: : 162, Accuracy: 0.9143, NNL: 0.3716, ECE:0.0751\n",
      "Epoch: : 163, Accuracy: 0.9143, NNL: 0.3716, ECE:0.0751\n",
      "Epoch: : 164, Accuracy: 0.9143, NNL: 0.3716, ECE:0.0751\n",
      "Epoch: : 165, Accuracy: 0.9143, NNL: 0.3716, ECE:0.0751\n",
      "run 1 TS tempreture:  1.067091464996338\n",
      "Epoch: : 001, Accuracy: 0.8786, NNL: 0.5514, ECE:0.0611\n",
      "Epoch: : 002, Accuracy: 0.8786, NNL: 0.5490, ECE:0.0611\n",
      "Epoch: : 003, Accuracy: 0.8786, NNL: 0.5466, ECE:0.0611\n",
      "Epoch: : 004, Accuracy: 0.8786, NNL: 0.5442, ECE:0.0611\n",
      "Epoch: : 005, Accuracy: 0.8786, NNL: 0.5418, ECE:0.0611\n",
      "Epoch: : 006, Accuracy: 0.8786, NNL: 0.5394, ECE:0.0611\n",
      "Epoch: : 007, Accuracy: 0.8786, NNL: 0.5370, ECE:0.0611\n",
      "Epoch: : 008, Accuracy: 0.8786, NNL: 0.5347, ECE:0.0611\n",
      "Epoch: : 009, Accuracy: 0.8786, NNL: 0.5323, ECE:0.0611\n",
      "Epoch: : 010, Accuracy: 0.8786, NNL: 0.5299, ECE:0.0611\n",
      "Epoch: : 011, Accuracy: 0.8786, NNL: 0.5276, ECE:0.0611\n",
      "Epoch: : 012, Accuracy: 0.8786, NNL: 0.5253, ECE:0.0611\n",
      "Epoch: : 013, Accuracy: 0.8786, NNL: 0.5229, ECE:0.0611\n",
      "Epoch: : 014, Accuracy: 0.8786, NNL: 0.5206, ECE:0.0611\n",
      "Epoch: : 015, Accuracy: 0.8786, NNL: 0.5183, ECE:0.0611\n",
      "Epoch: : 016, Accuracy: 0.8786, NNL: 0.5160, ECE:0.0611\n",
      "Epoch: : 017, Accuracy: 0.8786, NNL: 0.5137, ECE:0.0611\n",
      "Epoch: : 018, Accuracy: 0.8786, NNL: 0.5115, ECE:0.0611\n",
      "Epoch: : 019, Accuracy: 0.8786, NNL: 0.5092, ECE:0.0611\n",
      "Epoch: : 020, Accuracy: 0.8786, NNL: 0.5070, ECE:0.0611\n",
      "Epoch: : 021, Accuracy: 0.8786, NNL: 0.5048, ECE:0.0611\n",
      "Epoch: : 022, Accuracy: 0.8786, NNL: 0.5026, ECE:0.0611\n",
      "Epoch: : 023, Accuracy: 0.8786, NNL: 0.5004, ECE:0.0611\n",
      "Epoch: : 024, Accuracy: 0.8786, NNL: 0.4982, ECE:0.0611\n",
      "Epoch: : 025, Accuracy: 0.8786, NNL: 0.4961, ECE:0.0611\n",
      "Epoch: : 026, Accuracy: 0.8786, NNL: 0.4940, ECE:0.0611\n",
      "Epoch: : 027, Accuracy: 0.8786, NNL: 0.4919, ECE:0.0611\n",
      "Epoch: : 028, Accuracy: 0.8786, NNL: 0.4899, ECE:0.0611\n",
      "Epoch: : 029, Accuracy: 0.8786, NNL: 0.4878, ECE:0.0611\n",
      "Epoch: : 030, Accuracy: 0.8786, NNL: 0.4858, ECE:0.0611\n",
      "Epoch: : 031, Accuracy: 0.8786, NNL: 0.4838, ECE:0.0611\n",
      "Epoch: : 032, Accuracy: 0.8786, NNL: 0.4819, ECE:0.0611\n",
      "Epoch: : 033, Accuracy: 0.8786, NNL: 0.4800, ECE:0.0611\n",
      "Epoch: : 034, Accuracy: 0.8786, NNL: 0.4781, ECE:0.0611\n",
      "Epoch: : 035, Accuracy: 0.8786, NNL: 0.4763, ECE:0.0611\n",
      "Epoch: : 036, Accuracy: 0.8786, NNL: 0.4744, ECE:0.0611\n",
      "Epoch: : 037, Accuracy: 0.8786, NNL: 0.4727, ECE:0.0611\n",
      "Epoch: : 038, Accuracy: 0.8786, NNL: 0.4709, ECE:0.0611\n",
      "Epoch: : 039, Accuracy: 0.8786, NNL: 0.4692, ECE:0.0611\n",
      "Epoch: : 040, Accuracy: 0.8786, NNL: 0.4676, ECE:0.0611\n",
      "Epoch: : 041, Accuracy: 0.8786, NNL: 0.4660, ECE:0.0611\n",
      "Epoch: : 042, Accuracy: 0.8786, NNL: 0.4644, ECE:0.0611\n",
      "Epoch: : 043, Accuracy: 0.8786, NNL: 0.4629, ECE:0.0611\n",
      "Epoch: : 044, Accuracy: 0.8786, NNL: 0.4614, ECE:0.0611\n",
      "Epoch: : 045, Accuracy: 0.8786, NNL: 0.4599, ECE:0.0611\n",
      "Epoch: : 046, Accuracy: 0.8786, NNL: 0.4585, ECE:0.0611\n",
      "Epoch: : 047, Accuracy: 0.8786, NNL: 0.4572, ECE:0.0611\n",
      "Epoch: : 048, Accuracy: 0.8786, NNL: 0.4559, ECE:0.0611\n",
      "Epoch: : 049, Accuracy: 0.8786, NNL: 0.4546, ECE:0.0611\n",
      "Epoch: : 050, Accuracy: 0.8786, NNL: 0.4534, ECE:0.0611\n",
      "Epoch: : 051, Accuracy: 0.8786, NNL: 0.4522, ECE:0.0611\n",
      "Epoch: : 052, Accuracy: 0.8786, NNL: 0.4511, ECE:0.0611\n",
      "Epoch: : 053, Accuracy: 0.8786, NNL: 0.4501, ECE:0.0611\n",
      "Epoch: : 054, Accuracy: 0.8786, NNL: 0.4491, ECE:0.0611\n",
      "Epoch: : 055, Accuracy: 0.8786, NNL: 0.4481, ECE:0.0611\n",
      "Epoch: : 056, Accuracy: 0.8786, NNL: 0.4472, ECE:0.0611\n",
      "Epoch: : 057, Accuracy: 0.8786, NNL: 0.4463, ECE:0.0611\n",
      "Epoch: : 058, Accuracy: 0.8786, NNL: 0.4455, ECE:0.0611\n",
      "Epoch: : 059, Accuracy: 0.8786, NNL: 0.4448, ECE:0.0611\n",
      "Epoch: : 060, Accuracy: 0.8786, NNL: 0.4440, ECE:0.0611\n",
      "Epoch: : 061, Accuracy: 0.8786, NNL: 0.4434, ECE:0.0611\n",
      "Epoch: : 062, Accuracy: 0.8786, NNL: 0.4428, ECE:0.0611\n",
      "Epoch: : 063, Accuracy: 0.8786, NNL: 0.4422, ECE:0.0611\n",
      "Epoch: : 064, Accuracy: 0.8786, NNL: 0.4416, ECE:0.0611\n",
      "Epoch: : 065, Accuracy: 0.8786, NNL: 0.4412, ECE:0.0611\n",
      "Epoch: : 066, Accuracy: 0.8786, NNL: 0.4407, ECE:0.0611\n",
      "Epoch: : 067, Accuracy: 0.8786, NNL: 0.4403, ECE:0.0611\n",
      "Epoch: : 068, Accuracy: 0.8786, NNL: 0.4400, ECE:0.0611\n",
      "Epoch: : 069, Accuracy: 0.8786, NNL: 0.4396, ECE:0.0611\n",
      "Epoch: : 070, Accuracy: 0.8786, NNL: 0.4394, ECE:0.0611\n",
      "Epoch: : 071, Accuracy: 0.8786, NNL: 0.4391, ECE:0.0611\n",
      "Epoch: : 072, Accuracy: 0.8786, NNL: 0.4389, ECE:0.0611\n",
      "Epoch: : 073, Accuracy: 0.8786, NNL: 0.4387, ECE:0.0611\n",
      "Epoch: : 074, Accuracy: 0.8786, NNL: 0.4386, ECE:0.0611\n",
      "Epoch: : 075, Accuracy: 0.8786, NNL: 0.4384, ECE:0.0611\n",
      "Epoch: : 076, Accuracy: 0.8786, NNL: 0.4384, ECE:0.0611\n",
      "Epoch: : 077, Accuracy: 0.8786, NNL: 0.4383, ECE:0.0611\n",
      "Epoch: : 078, Accuracy: 0.8786, NNL: 0.4382, ECE:0.0611\n",
      "Epoch: : 079, Accuracy: 0.8786, NNL: 0.4382, ECE:0.0611\n",
      "Epoch: : 080, Accuracy: 0.8786, NNL: 0.4382, ECE:0.0611\n",
      "Epoch: : 081, Accuracy: 0.8786, NNL: 0.4382, ECE:0.0611\n",
      "Epoch: : 082, Accuracy: 0.8786, NNL: 0.4383, ECE:0.0611\n",
      "Epoch: : 083, Accuracy: 0.8786, NNL: 0.4383, ECE:0.0611\n",
      "Epoch: : 084, Accuracy: 0.8786, NNL: 0.4384, ECE:0.0611\n",
      "Epoch: : 085, Accuracy: 0.8786, NNL: 0.4385, ECE:0.0611\n",
      "Epoch: : 086, Accuracy: 0.8786, NNL: 0.4385, ECE:0.0611\n",
      "Epoch: : 087, Accuracy: 0.8786, NNL: 0.4386, ECE:0.0611\n",
      "Epoch: : 088, Accuracy: 0.8786, NNL: 0.4387, ECE:0.0611\n",
      "Epoch: : 089, Accuracy: 0.8786, NNL: 0.4388, ECE:0.0611\n",
      "Epoch: : 090, Accuracy: 0.8786, NNL: 0.4389, ECE:0.0611\n",
      "Epoch: : 091, Accuracy: 0.8786, NNL: 0.4390, ECE:0.0611\n",
      "Epoch: : 092, Accuracy: 0.8786, NNL: 0.4392, ECE:0.0611\n",
      "Epoch: : 093, Accuracy: 0.8786, NNL: 0.4393, ECE:0.0611\n",
      "Epoch: : 094, Accuracy: 0.8786, NNL: 0.4394, ECE:0.0611\n",
      "Epoch: : 095, Accuracy: 0.8786, NNL: 0.4395, ECE:0.0611\n",
      "Epoch: : 096, Accuracy: 0.8786, NNL: 0.4396, ECE:0.0611\n",
      "Epoch: : 097, Accuracy: 0.8786, NNL: 0.4397, ECE:0.0611\n",
      "Epoch: : 098, Accuracy: 0.8786, NNL: 0.4398, ECE:0.0611\n",
      "Epoch: : 099, Accuracy: 0.8786, NNL: 0.4399, ECE:0.0611\n",
      "Epoch: : 100, Accuracy: 0.8786, NNL: 0.4400, ECE:0.0611\n",
      "Epoch: : 101, Accuracy: 0.8786, NNL: 0.4401, ECE:0.0611\n",
      "Epoch: : 102, Accuracy: 0.8786, NNL: 0.4402, ECE:0.0611\n",
      "Epoch: : 103, Accuracy: 0.8786, NNL: 0.4403, ECE:0.0611\n",
      "Epoch: : 104, Accuracy: 0.8786, NNL: 0.4404, ECE:0.0611\n",
      "Epoch: : 105, Accuracy: 0.8786, NNL: 0.4405, ECE:0.0611\n",
      "Epoch: : 106, Accuracy: 0.8786, NNL: 0.4405, ECE:0.0611\n",
      "Epoch: : 107, Accuracy: 0.8786, NNL: 0.4406, ECE:0.0611\n",
      "Epoch: : 108, Accuracy: 0.8786, NNL: 0.4407, ECE:0.0611\n",
      "Epoch: : 109, Accuracy: 0.8786, NNL: 0.4407, ECE:0.0611\n",
      "Epoch: : 110, Accuracy: 0.8786, NNL: 0.4408, ECE:0.0611\n",
      "Epoch: : 111, Accuracy: 0.8786, NNL: 0.4408, ECE:0.0611\n",
      "Epoch: : 112, Accuracy: 0.8786, NNL: 0.4408, ECE:0.0611\n",
      "Epoch: : 113, Accuracy: 0.8786, NNL: 0.4409, ECE:0.0611\n",
      "Epoch: : 114, Accuracy: 0.8786, NNL: 0.4409, ECE:0.0611\n",
      "Epoch: : 115, Accuracy: 0.8786, NNL: 0.4409, ECE:0.0611\n",
      "Epoch: : 116, Accuracy: 0.8786, NNL: 0.4409, ECE:0.0611\n",
      "Epoch: : 117, Accuracy: 0.8786, NNL: 0.4409, ECE:0.0611\n",
      "Epoch: : 118, Accuracy: 0.8786, NNL: 0.4410, ECE:0.0611\n",
      "Epoch: : 119, Accuracy: 0.8786, NNL: 0.4410, ECE:0.0611\n",
      "Epoch: : 120, Accuracy: 0.8786, NNL: 0.4410, ECE:0.0611\n",
      "Epoch: : 121, Accuracy: 0.8786, NNL: 0.4410, ECE:0.0611\n",
      "Epoch: : 122, Accuracy: 0.8786, NNL: 0.4410, ECE:0.0611\n",
      "Epoch: : 123, Accuracy: 0.8786, NNL: 0.4409, ECE:0.0611\n",
      "Epoch: : 124, Accuracy: 0.8786, NNL: 0.4409, ECE:0.0611\n",
      "Epoch: : 125, Accuracy: 0.8786, NNL: 0.4409, ECE:0.0611\n",
      "Epoch: : 126, Accuracy: 0.8786, NNL: 0.4409, ECE:0.0611\n",
      "Epoch: : 127, Accuracy: 0.8786, NNL: 0.4409, ECE:0.0611\n",
      "Epoch: : 128, Accuracy: 0.8786, NNL: 0.4409, ECE:0.0611\n",
      "Epoch: : 129, Accuracy: 0.8786, NNL: 0.4409, ECE:0.0611\n",
      "Epoch: : 130, Accuracy: 0.8786, NNL: 0.4409, ECE:0.0611\n",
      "Epoch: : 131, Accuracy: 0.8786, NNL: 0.4408, ECE:0.0611\n",
      "Epoch: : 132, Accuracy: 0.8786, NNL: 0.4408, ECE:0.0611\n",
      "Epoch: : 133, Accuracy: 0.8786, NNL: 0.4408, ECE:0.0611\n",
      "Epoch: : 134, Accuracy: 0.8786, NNL: 0.4408, ECE:0.0611\n",
      "Epoch: : 135, Accuracy: 0.8786, NNL: 0.4408, ECE:0.0611\n",
      "Epoch: : 136, Accuracy: 0.8786, NNL: 0.4408, ECE:0.0611\n",
      "Epoch: : 137, Accuracy: 0.8786, NNL: 0.4407, ECE:0.0611\n",
      "Epoch: : 138, Accuracy: 0.8786, NNL: 0.4407, ECE:0.0611\n",
      "Epoch: : 139, Accuracy: 0.8786, NNL: 0.4407, ECE:0.0611\n",
      "Epoch: : 140, Accuracy: 0.8786, NNL: 0.4407, ECE:0.0611\n",
      "Epoch: : 141, Accuracy: 0.8786, NNL: 0.4407, ECE:0.0611\n",
      "Epoch: : 142, Accuracy: 0.8786, NNL: 0.4407, ECE:0.0611\n",
      "Epoch: : 143, Accuracy: 0.8786, NNL: 0.4407, ECE:0.0611\n",
      "Epoch: : 144, Accuracy: 0.8786, NNL: 0.4407, ECE:0.0611\n",
      "Epoch: : 145, Accuracy: 0.8786, NNL: 0.4406, ECE:0.0611\n",
      "Epoch: : 146, Accuracy: 0.8786, NNL: 0.4406, ECE:0.0611\n",
      "Epoch: : 147, Accuracy: 0.8786, NNL: 0.4406, ECE:0.0611\n",
      "Epoch: : 148, Accuracy: 0.8786, NNL: 0.4406, ECE:0.0611\n",
      "Epoch: : 149, Accuracy: 0.8786, NNL: 0.4406, ECE:0.0611\n",
      "Epoch: : 150, Accuracy: 0.8786, NNL: 0.4406, ECE:0.0611\n",
      "Epoch: : 151, Accuracy: 0.8786, NNL: 0.4406, ECE:0.0611\n",
      "Epoch: : 152, Accuracy: 0.8786, NNL: 0.4406, ECE:0.0611\n",
      "Epoch: : 153, Accuracy: 0.8786, NNL: 0.4406, ECE:0.0611\n",
      "Epoch: : 154, Accuracy: 0.8786, NNL: 0.4406, ECE:0.0611\n",
      "Epoch: : 155, Accuracy: 0.8786, NNL: 0.4406, ECE:0.0611\n",
      "Epoch: : 156, Accuracy: 0.8786, NNL: 0.4406, ECE:0.0611\n",
      "Epoch: : 157, Accuracy: 0.8786, NNL: 0.4406, ECE:0.0611\n",
      "Epoch: : 158, Accuracy: 0.8786, NNL: 0.4406, ECE:0.0611\n",
      "Epoch: : 159, Accuracy: 0.8786, NNL: 0.4406, ECE:0.0611\n",
      "Epoch: : 160, Accuracy: 0.8786, NNL: 0.4406, ECE:0.0611\n",
      "Epoch: : 161, Accuracy: 0.8786, NNL: 0.4406, ECE:0.0611\n",
      "Epoch: : 162, Accuracy: 0.8786, NNL: 0.4406, ECE:0.0611\n",
      "Epoch: : 163, Accuracy: 0.8786, NNL: 0.4406, ECE:0.0611\n",
      "Epoch: : 164, Accuracy: 0.8786, NNL: 0.4406, ECE:0.0611\n",
      "Epoch: : 165, Accuracy: 0.8786, NNL: 0.4406, ECE:0.0611\n",
      "Epoch: : 166, Accuracy: 0.8786, NNL: 0.4406, ECE:0.0611\n",
      "Epoch: : 167, Accuracy: 0.8786, NNL: 0.4406, ECE:0.0611\n",
      "Epoch: : 168, Accuracy: 0.8786, NNL: 0.4406, ECE:0.0611\n",
      "Epoch: : 169, Accuracy: 0.8786, NNL: 0.4406, ECE:0.0611\n",
      "Epoch: : 170, Accuracy: 0.8786, NNL: 0.4406, ECE:0.0611\n",
      "Epoch: : 171, Accuracy: 0.8786, NNL: 0.4406, ECE:0.0611\n",
      "Epoch: : 172, Accuracy: 0.8786, NNL: 0.4406, ECE:0.0611\n",
      "Epoch: : 173, Accuracy: 0.8786, NNL: 0.4406, ECE:0.0611\n",
      "Epoch: : 174, Accuracy: 0.8786, NNL: 0.4406, ECE:0.0611\n",
      "Epoch: : 175, Accuracy: 0.8786, NNL: 0.4406, ECE:0.0611\n",
      "Epoch: : 176, Accuracy: 0.8786, NNL: 0.4406, ECE:0.0611\n",
      "Epoch: : 177, Accuracy: 0.8786, NNL: 0.4406, ECE:0.0611\n",
      "Epoch: : 178, Accuracy: 0.8786, NNL: 0.4406, ECE:0.0611\n",
      "Epoch: : 179, Accuracy: 0.8786, NNL: 0.4406, ECE:0.0611\n",
      "run 2 TS tempreture:  0.9783794283866882\n",
      "Epoch: : 001, Accuracy: 0.8786, NNL: 0.5214, ECE:0.0789\n",
      "Epoch: : 002, Accuracy: 0.8786, NNL: 0.5192, ECE:0.0789\n",
      "Epoch: : 003, Accuracy: 0.8786, NNL: 0.5171, ECE:0.0789\n",
      "Epoch: : 004, Accuracy: 0.8786, NNL: 0.5150, ECE:0.0789\n",
      "Epoch: : 005, Accuracy: 0.8786, NNL: 0.5129, ECE:0.0789\n",
      "Epoch: : 006, Accuracy: 0.8786, NNL: 0.5109, ECE:0.0789\n",
      "Epoch: : 007, Accuracy: 0.8786, NNL: 0.5088, ECE:0.0789\n",
      "Epoch: : 008, Accuracy: 0.8786, NNL: 0.5067, ECE:0.0789\n",
      "Epoch: : 009, Accuracy: 0.8786, NNL: 0.5046, ECE:0.0789\n",
      "Epoch: : 010, Accuracy: 0.8786, NNL: 0.5026, ECE:0.0789\n",
      "Epoch: : 011, Accuracy: 0.8786, NNL: 0.5006, ECE:0.0789\n",
      "Epoch: : 012, Accuracy: 0.8786, NNL: 0.4985, ECE:0.0789\n",
      "Epoch: : 013, Accuracy: 0.8786, NNL: 0.4965, ECE:0.0789\n",
      "Epoch: : 014, Accuracy: 0.8786, NNL: 0.4945, ECE:0.0789\n",
      "Epoch: : 015, Accuracy: 0.8786, NNL: 0.4925, ECE:0.0789\n",
      "Epoch: : 016, Accuracy: 0.8786, NNL: 0.4906, ECE:0.0789\n",
      "Epoch: : 017, Accuracy: 0.8786, NNL: 0.4887, ECE:0.0789\n",
      "Epoch: : 018, Accuracy: 0.8786, NNL: 0.4868, ECE:0.0789\n",
      "Epoch: : 019, Accuracy: 0.8786, NNL: 0.4849, ECE:0.0789\n",
      "Epoch: : 020, Accuracy: 0.8786, NNL: 0.4830, ECE:0.0789\n",
      "Epoch: : 021, Accuracy: 0.8786, NNL: 0.4812, ECE:0.0789\n",
      "Epoch: : 022, Accuracy: 0.8786, NNL: 0.4794, ECE:0.0789\n",
      "Epoch: : 023, Accuracy: 0.8786, NNL: 0.4776, ECE:0.0789\n",
      "Epoch: : 024, Accuracy: 0.8786, NNL: 0.4759, ECE:0.0789\n",
      "Epoch: : 025, Accuracy: 0.8786, NNL: 0.4742, ECE:0.0789\n",
      "Epoch: : 026, Accuracy: 0.8786, NNL: 0.4726, ECE:0.0789\n",
      "Epoch: : 027, Accuracy: 0.8786, NNL: 0.4710, ECE:0.0789\n",
      "Epoch: : 028, Accuracy: 0.8786, NNL: 0.4695, ECE:0.0789\n",
      "Epoch: : 029, Accuracy: 0.8786, NNL: 0.4680, ECE:0.0789\n",
      "Epoch: : 030, Accuracy: 0.8786, NNL: 0.4665, ECE:0.0789\n",
      "Epoch: : 031, Accuracy: 0.8786, NNL: 0.4651, ECE:0.0789\n",
      "Epoch: : 032, Accuracy: 0.8786, NNL: 0.4638, ECE:0.0789\n",
      "Epoch: : 033, Accuracy: 0.8786, NNL: 0.4625, ECE:0.0789\n",
      "Epoch: : 034, Accuracy: 0.8786, NNL: 0.4613, ECE:0.0789\n",
      "Epoch: : 035, Accuracy: 0.8786, NNL: 0.4602, ECE:0.0789\n",
      "Epoch: : 036, Accuracy: 0.8786, NNL: 0.4591, ECE:0.0789\n",
      "Epoch: : 037, Accuracy: 0.8786, NNL: 0.4580, ECE:0.0789\n",
      "Epoch: : 038, Accuracy: 0.8786, NNL: 0.4571, ECE:0.0789\n",
      "Epoch: : 039, Accuracy: 0.8786, NNL: 0.4562, ECE:0.0789\n",
      "Epoch: : 040, Accuracy: 0.8786, NNL: 0.4554, ECE:0.0789\n",
      "Epoch: : 041, Accuracy: 0.8786, NNL: 0.4546, ECE:0.0789\n",
      "Epoch: : 042, Accuracy: 0.8786, NNL: 0.4539, ECE:0.0789\n",
      "Epoch: : 043, Accuracy: 0.8786, NNL: 0.4533, ECE:0.0789\n",
      "Epoch: : 044, Accuracy: 0.8786, NNL: 0.4527, ECE:0.0789\n",
      "Epoch: : 045, Accuracy: 0.8786, NNL: 0.4523, ECE:0.0789\n",
      "Epoch: : 046, Accuracy: 0.8786, NNL: 0.4518, ECE:0.0789\n",
      "Epoch: : 047, Accuracy: 0.8786, NNL: 0.4515, ECE:0.0789\n",
      "Epoch: : 048, Accuracy: 0.8786, NNL: 0.4512, ECE:0.0789\n",
      "Epoch: : 049, Accuracy: 0.8786, NNL: 0.4509, ECE:0.0789\n",
      "Epoch: : 050, Accuracy: 0.8786, NNL: 0.4507, ECE:0.0789\n",
      "Epoch: : 051, Accuracy: 0.8786, NNL: 0.4506, ECE:0.0789\n",
      "Epoch: : 052, Accuracy: 0.8786, NNL: 0.4505, ECE:0.0789\n",
      "Epoch: : 053, Accuracy: 0.8786, NNL: 0.4505, ECE:0.0789\n",
      "Epoch: : 054, Accuracy: 0.8786, NNL: 0.4505, ECE:0.0789\n",
      "Epoch: : 055, Accuracy: 0.8786, NNL: 0.4505, ECE:0.0789\n",
      "Epoch: : 056, Accuracy: 0.8786, NNL: 0.4506, ECE:0.0789\n",
      "Epoch: : 057, Accuracy: 0.8786, NNL: 0.4507, ECE:0.0789\n",
      "Epoch: : 058, Accuracy: 0.8786, NNL: 0.4508, ECE:0.0789\n",
      "Epoch: : 059, Accuracy: 0.8786, NNL: 0.4510, ECE:0.0789\n",
      "Epoch: : 060, Accuracy: 0.8786, NNL: 0.4511, ECE:0.0789\n",
      "Epoch: : 061, Accuracy: 0.8786, NNL: 0.4513, ECE:0.0789\n",
      "Epoch: : 062, Accuracy: 0.8786, NNL: 0.4515, ECE:0.0789\n",
      "Epoch: : 063, Accuracy: 0.8786, NNL: 0.4518, ECE:0.0789\n",
      "Epoch: : 064, Accuracy: 0.8786, NNL: 0.4520, ECE:0.0789\n",
      "Epoch: : 065, Accuracy: 0.8786, NNL: 0.4522, ECE:0.0789\n",
      "Epoch: : 066, Accuracy: 0.8786, NNL: 0.4525, ECE:0.0789\n",
      "Epoch: : 067, Accuracy: 0.8786, NNL: 0.4527, ECE:0.0789\n",
      "Epoch: : 068, Accuracy: 0.8786, NNL: 0.4529, ECE:0.0789\n",
      "Epoch: : 069, Accuracy: 0.8786, NNL: 0.4531, ECE:0.0789\n",
      "Epoch: : 070, Accuracy: 0.8786, NNL: 0.4534, ECE:0.0789\n",
      "Epoch: : 071, Accuracy: 0.8786, NNL: 0.4536, ECE:0.0789\n",
      "Epoch: : 072, Accuracy: 0.8786, NNL: 0.4538, ECE:0.0789\n",
      "Epoch: : 073, Accuracy: 0.8786, NNL: 0.4540, ECE:0.0789\n",
      "Epoch: : 074, Accuracy: 0.8786, NNL: 0.4541, ECE:0.0789\n",
      "Epoch: : 075, Accuracy: 0.8786, NNL: 0.4543, ECE:0.0789\n",
      "Epoch: : 076, Accuracy: 0.8786, NNL: 0.4545, ECE:0.0789\n",
      "Epoch: : 077, Accuracy: 0.8786, NNL: 0.4546, ECE:0.0789\n",
      "Epoch: : 078, Accuracy: 0.8786, NNL: 0.4547, ECE:0.0789\n",
      "Epoch: : 079, Accuracy: 0.8786, NNL: 0.4548, ECE:0.0789\n",
      "Epoch: : 080, Accuracy: 0.8786, NNL: 0.4549, ECE:0.0789\n",
      "Epoch: : 081, Accuracy: 0.8786, NNL: 0.4550, ECE:0.0789\n",
      "Epoch: : 082, Accuracy: 0.8786, NNL: 0.4551, ECE:0.0789\n",
      "Epoch: : 083, Accuracy: 0.8786, NNL: 0.4551, ECE:0.0789\n",
      "Epoch: : 084, Accuracy: 0.8786, NNL: 0.4552, ECE:0.0789\n",
      "Epoch: : 085, Accuracy: 0.8786, NNL: 0.4552, ECE:0.0789\n",
      "Epoch: : 086, Accuracy: 0.8786, NNL: 0.4552, ECE:0.0789\n",
      "Epoch: : 087, Accuracy: 0.8786, NNL: 0.4552, ECE:0.0789\n",
      "Epoch: : 088, Accuracy: 0.8786, NNL: 0.4553, ECE:0.0789\n",
      "Epoch: : 089, Accuracy: 0.8786, NNL: 0.4552, ECE:0.0789\n",
      "Epoch: : 090, Accuracy: 0.8786, NNL: 0.4552, ECE:0.0789\n",
      "Epoch: : 091, Accuracy: 0.8786, NNL: 0.4552, ECE:0.0789\n",
      "Epoch: : 092, Accuracy: 0.8786, NNL: 0.4552, ECE:0.0789\n",
      "Epoch: : 093, Accuracy: 0.8786, NNL: 0.4551, ECE:0.0789\n",
      "Epoch: : 094, Accuracy: 0.8786, NNL: 0.4551, ECE:0.0789\n",
      "Epoch: : 095, Accuracy: 0.8786, NNL: 0.4551, ECE:0.0789\n",
      "Epoch: : 096, Accuracy: 0.8786, NNL: 0.4550, ECE:0.0789\n",
      "Epoch: : 097, Accuracy: 0.8786, NNL: 0.4550, ECE:0.0789\n",
      "Epoch: : 098, Accuracy: 0.8786, NNL: 0.4549, ECE:0.0789\n",
      "Epoch: : 099, Accuracy: 0.8786, NNL: 0.4549, ECE:0.0789\n",
      "Epoch: : 100, Accuracy: 0.8786, NNL: 0.4548, ECE:0.0789\n",
      "Epoch: : 101, Accuracy: 0.8786, NNL: 0.4548, ECE:0.0789\n",
      "Epoch: : 102, Accuracy: 0.8786, NNL: 0.4547, ECE:0.0789\n",
      "Epoch: : 103, Accuracy: 0.8786, NNL: 0.4547, ECE:0.0789\n",
      "Epoch: : 104, Accuracy: 0.8786, NNL: 0.4546, ECE:0.0789\n",
      "Epoch: : 105, Accuracy: 0.8786, NNL: 0.4546, ECE:0.0789\n",
      "Epoch: : 106, Accuracy: 0.8786, NNL: 0.4546, ECE:0.0789\n",
      "Epoch: : 107, Accuracy: 0.8786, NNL: 0.4545, ECE:0.0789\n",
      "Epoch: : 108, Accuracy: 0.8786, NNL: 0.4545, ECE:0.0789\n",
      "Epoch: : 109, Accuracy: 0.8786, NNL: 0.4545, ECE:0.0789\n",
      "Epoch: : 110, Accuracy: 0.8786, NNL: 0.4545, ECE:0.0789\n",
      "Epoch: : 111, Accuracy: 0.8786, NNL: 0.4544, ECE:0.0789\n",
      "Epoch: : 112, Accuracy: 0.8786, NNL: 0.4544, ECE:0.0789\n",
      "Epoch: : 113, Accuracy: 0.8786, NNL: 0.4544, ECE:0.0789\n",
      "Epoch: : 114, Accuracy: 0.8786, NNL: 0.4544, ECE:0.0789\n",
      "Epoch: : 115, Accuracy: 0.8786, NNL: 0.4544, ECE:0.0789\n",
      "Epoch: : 116, Accuracy: 0.8786, NNL: 0.4544, ECE:0.0789\n",
      "Epoch: : 117, Accuracy: 0.8786, NNL: 0.4544, ECE:0.0789\n",
      "Epoch: : 118, Accuracy: 0.8786, NNL: 0.4544, ECE:0.0789\n",
      "Epoch: : 119, Accuracy: 0.8786, NNL: 0.4544, ECE:0.0789\n",
      "Epoch: : 120, Accuracy: 0.8786, NNL: 0.4544, ECE:0.0789\n",
      "Epoch: : 121, Accuracy: 0.8786, NNL: 0.4544, ECE:0.0789\n",
      "Epoch: : 122, Accuracy: 0.8786, NNL: 0.4544, ECE:0.0789\n",
      "Epoch: : 123, Accuracy: 0.8786, NNL: 0.4544, ECE:0.0789\n",
      "Epoch: : 124, Accuracy: 0.8786, NNL: 0.4544, ECE:0.0789\n",
      "Epoch: : 125, Accuracy: 0.8786, NNL: 0.4544, ECE:0.0789\n",
      "Epoch: : 126, Accuracy: 0.8786, NNL: 0.4544, ECE:0.0789\n",
      "Epoch: : 127, Accuracy: 0.8786, NNL: 0.4544, ECE:0.0789\n",
      "Epoch: : 128, Accuracy: 0.8786, NNL: 0.4544, ECE:0.0789\n",
      "Epoch: : 129, Accuracy: 0.8786, NNL: 0.4544, ECE:0.0789\n",
      "Epoch: : 130, Accuracy: 0.8786, NNL: 0.4545, ECE:0.0789\n",
      "Epoch: : 131, Accuracy: 0.8786, NNL: 0.4545, ECE:0.0789\n",
      "Epoch: : 132, Accuracy: 0.8786, NNL: 0.4545, ECE:0.0789\n",
      "Epoch: : 133, Accuracy: 0.8786, NNL: 0.4545, ECE:0.0789\n",
      "Epoch: : 134, Accuracy: 0.8786, NNL: 0.4545, ECE:0.0789\n",
      "Epoch: : 135, Accuracy: 0.8786, NNL: 0.4545, ECE:0.0789\n",
      "Epoch: : 136, Accuracy: 0.8786, NNL: 0.4545, ECE:0.0789\n",
      "Epoch: : 137, Accuracy: 0.8786, NNL: 0.4545, ECE:0.0789\n",
      "Epoch: : 138, Accuracy: 0.8786, NNL: 0.4545, ECE:0.0789\n",
      "Epoch: : 139, Accuracy: 0.8786, NNL: 0.4545, ECE:0.0789\n",
      "Epoch: : 140, Accuracy: 0.8786, NNL: 0.4545, ECE:0.0789\n",
      "Epoch: : 141, Accuracy: 0.8786, NNL: 0.4545, ECE:0.0789\n",
      "Epoch: : 142, Accuracy: 0.8786, NNL: 0.4545, ECE:0.0789\n",
      "Epoch: : 143, Accuracy: 0.8786, NNL: 0.4545, ECE:0.0789\n",
      "Epoch: : 144, Accuracy: 0.8786, NNL: 0.4546, ECE:0.0789\n",
      "Epoch: : 145, Accuracy: 0.8786, NNL: 0.4546, ECE:0.0789\n",
      "Epoch: : 146, Accuracy: 0.8786, NNL: 0.4546, ECE:0.0789\n",
      "Epoch: : 147, Accuracy: 0.8786, NNL: 0.4546, ECE:0.0789\n",
      "Epoch: : 148, Accuracy: 0.8786, NNL: 0.4546, ECE:0.0789\n",
      "Epoch: : 149, Accuracy: 0.8786, NNL: 0.4546, ECE:0.0789\n",
      "Epoch: : 150, Accuracy: 0.8786, NNL: 0.4545, ECE:0.0789\n",
      "Epoch: : 151, Accuracy: 0.8786, NNL: 0.4545, ECE:0.0789\n",
      "Epoch: : 152, Accuracy: 0.8786, NNL: 0.4545, ECE:0.0789\n",
      "Epoch: : 153, Accuracy: 0.8786, NNL: 0.4545, ECE:0.0789\n",
      "run 3 TS tempreture:  1.1468168497085571\n",
      "Epoch: : 001, Accuracy: 0.8571, NNL: 0.5828, ECE:0.0708\n",
      "Epoch: : 002, Accuracy: 0.8571, NNL: 0.5805, ECE:0.0708\n",
      "Epoch: : 003, Accuracy: 0.8571, NNL: 0.5783, ECE:0.0708\n",
      "Epoch: : 004, Accuracy: 0.8571, NNL: 0.5760, ECE:0.0708\n",
      "Epoch: : 005, Accuracy: 0.8571, NNL: 0.5737, ECE:0.0708\n",
      "Epoch: : 006, Accuracy: 0.8571, NNL: 0.5715, ECE:0.0708\n",
      "Epoch: : 007, Accuracy: 0.8571, NNL: 0.5693, ECE:0.0708\n",
      "Epoch: : 008, Accuracy: 0.8571, NNL: 0.5670, ECE:0.0708\n",
      "Epoch: : 009, Accuracy: 0.8571, NNL: 0.5648, ECE:0.0708\n",
      "Epoch: : 010, Accuracy: 0.8571, NNL: 0.5626, ECE:0.0708\n",
      "Epoch: : 011, Accuracy: 0.8571, NNL: 0.5603, ECE:0.0708\n",
      "Epoch: : 012, Accuracy: 0.8571, NNL: 0.5581, ECE:0.0708\n",
      "Epoch: : 013, Accuracy: 0.8571, NNL: 0.5559, ECE:0.0708\n",
      "Epoch: : 014, Accuracy: 0.8571, NNL: 0.5537, ECE:0.0708\n",
      "Epoch: : 015, Accuracy: 0.8571, NNL: 0.5516, ECE:0.0708\n",
      "Epoch: : 016, Accuracy: 0.8571, NNL: 0.5494, ECE:0.0708\n",
      "Epoch: : 017, Accuracy: 0.8571, NNL: 0.5472, ECE:0.0708\n",
      "Epoch: : 018, Accuracy: 0.8571, NNL: 0.5451, ECE:0.0708\n",
      "Epoch: : 019, Accuracy: 0.8571, NNL: 0.5430, ECE:0.0708\n",
      "Epoch: : 020, Accuracy: 0.8571, NNL: 0.5409, ECE:0.0708\n",
      "Epoch: : 021, Accuracy: 0.8571, NNL: 0.5388, ECE:0.0708\n",
      "Epoch: : 022, Accuracy: 0.8571, NNL: 0.5367, ECE:0.0708\n",
      "Epoch: : 023, Accuracy: 0.8571, NNL: 0.5346, ECE:0.0708\n",
      "Epoch: : 024, Accuracy: 0.8571, NNL: 0.5326, ECE:0.0708\n",
      "Epoch: : 025, Accuracy: 0.8571, NNL: 0.5306, ECE:0.0708\n",
      "Epoch: : 026, Accuracy: 0.8571, NNL: 0.5286, ECE:0.0708\n",
      "Epoch: : 027, Accuracy: 0.8571, NNL: 0.5266, ECE:0.0708\n",
      "Epoch: : 028, Accuracy: 0.8571, NNL: 0.5247, ECE:0.0708\n",
      "Epoch: : 029, Accuracy: 0.8571, NNL: 0.5228, ECE:0.0708\n",
      "Epoch: : 030, Accuracy: 0.8571, NNL: 0.5209, ECE:0.0708\n",
      "Epoch: : 031, Accuracy: 0.8571, NNL: 0.5190, ECE:0.0708\n",
      "Epoch: : 032, Accuracy: 0.8571, NNL: 0.5172, ECE:0.0708\n",
      "Epoch: : 033, Accuracy: 0.8571, NNL: 0.5154, ECE:0.0708\n",
      "Epoch: : 034, Accuracy: 0.8571, NNL: 0.5137, ECE:0.0708\n",
      "Epoch: : 035, Accuracy: 0.8571, NNL: 0.5119, ECE:0.0708\n",
      "Epoch: : 036, Accuracy: 0.8571, NNL: 0.5102, ECE:0.0708\n",
      "Epoch: : 037, Accuracy: 0.8571, NNL: 0.5086, ECE:0.0708\n",
      "Epoch: : 038, Accuracy: 0.8571, NNL: 0.5070, ECE:0.0708\n",
      "Epoch: : 039, Accuracy: 0.8571, NNL: 0.5054, ECE:0.0708\n",
      "Epoch: : 040, Accuracy: 0.8571, NNL: 0.5039, ECE:0.0708\n",
      "Epoch: : 041, Accuracy: 0.8571, NNL: 0.5024, ECE:0.0708\n",
      "Epoch: : 042, Accuracy: 0.8571, NNL: 0.5009, ECE:0.0708\n",
      "Epoch: : 043, Accuracy: 0.8571, NNL: 0.4995, ECE:0.0708\n",
      "Epoch: : 044, Accuracy: 0.8571, NNL: 0.4982, ECE:0.0708\n",
      "Epoch: : 045, Accuracy: 0.8571, NNL: 0.4969, ECE:0.0708\n",
      "Epoch: : 046, Accuracy: 0.8571, NNL: 0.4956, ECE:0.0708\n",
      "Epoch: : 047, Accuracy: 0.8571, NNL: 0.4944, ECE:0.0708\n",
      "Epoch: : 048, Accuracy: 0.8571, NNL: 0.4932, ECE:0.0708\n",
      "Epoch: : 049, Accuracy: 0.8571, NNL: 0.4921, ECE:0.0708\n",
      "Epoch: : 050, Accuracy: 0.8571, NNL: 0.4910, ECE:0.0708\n",
      "Epoch: : 051, Accuracy: 0.8571, NNL: 0.4900, ECE:0.0708\n",
      "Epoch: : 052, Accuracy: 0.8571, NNL: 0.4890, ECE:0.0708\n",
      "Epoch: : 053, Accuracy: 0.8571, NNL: 0.4881, ECE:0.0708\n",
      "Epoch: : 054, Accuracy: 0.8571, NNL: 0.4873, ECE:0.0708\n",
      "Epoch: : 055, Accuracy: 0.8571, NNL: 0.4864, ECE:0.0708\n",
      "Epoch: : 056, Accuracy: 0.8571, NNL: 0.4857, ECE:0.0708\n",
      "Epoch: : 057, Accuracy: 0.8571, NNL: 0.4850, ECE:0.0708\n",
      "Epoch: : 058, Accuracy: 0.8571, NNL: 0.4843, ECE:0.0708\n",
      "Epoch: : 059, Accuracy: 0.8571, NNL: 0.4837, ECE:0.0708\n",
      "Epoch: : 060, Accuracy: 0.8571, NNL: 0.4831, ECE:0.0708\n",
      "Epoch: : 061, Accuracy: 0.8571, NNL: 0.4826, ECE:0.0708\n",
      "Epoch: : 062, Accuracy: 0.8571, NNL: 0.4821, ECE:0.0708\n",
      "Epoch: : 063, Accuracy: 0.8571, NNL: 0.4817, ECE:0.0708\n",
      "Epoch: : 064, Accuracy: 0.8571, NNL: 0.4813, ECE:0.0708\n",
      "Epoch: : 065, Accuracy: 0.8571, NNL: 0.4810, ECE:0.0708\n",
      "Epoch: : 066, Accuracy: 0.8571, NNL: 0.4806, ECE:0.0708\n",
      "Epoch: : 067, Accuracy: 0.8571, NNL: 0.4804, ECE:0.0708\n",
      "Epoch: : 068, Accuracy: 0.8571, NNL: 0.4802, ECE:0.0708\n",
      "Epoch: : 069, Accuracy: 0.8571, NNL: 0.4800, ECE:0.0708\n",
      "Epoch: : 070, Accuracy: 0.8571, NNL: 0.4798, ECE:0.0708\n",
      "Epoch: : 071, Accuracy: 0.8571, NNL: 0.4797, ECE:0.0708\n",
      "Epoch: : 072, Accuracy: 0.8571, NNL: 0.4796, ECE:0.0708\n",
      "Epoch: : 073, Accuracy: 0.8571, NNL: 0.4795, ECE:0.0708\n",
      "Epoch: : 074, Accuracy: 0.8571, NNL: 0.4795, ECE:0.0708\n",
      "Epoch: : 075, Accuracy: 0.8571, NNL: 0.4795, ECE:0.0708\n",
      "Epoch: : 076, Accuracy: 0.8571, NNL: 0.4795, ECE:0.0708\n",
      "Epoch: : 077, Accuracy: 0.8571, NNL: 0.4795, ECE:0.0708\n",
      "Epoch: : 078, Accuracy: 0.8571, NNL: 0.4795, ECE:0.0708\n",
      "Epoch: : 079, Accuracy: 0.8571, NNL: 0.4796, ECE:0.0708\n",
      "Epoch: : 080, Accuracy: 0.8571, NNL: 0.4797, ECE:0.0708\n",
      "Epoch: : 081, Accuracy: 0.8571, NNL: 0.4797, ECE:0.0708\n",
      "Epoch: : 082, Accuracy: 0.8571, NNL: 0.4798, ECE:0.0708\n",
      "Epoch: : 083, Accuracy: 0.8571, NNL: 0.4799, ECE:0.0708\n",
      "Epoch: : 084, Accuracy: 0.8571, NNL: 0.4801, ECE:0.0708\n",
      "Epoch: : 085, Accuracy: 0.8571, NNL: 0.4802, ECE:0.0708\n",
      "Epoch: : 086, Accuracy: 0.8571, NNL: 0.4803, ECE:0.0708\n",
      "Epoch: : 087, Accuracy: 0.8571, NNL: 0.4804, ECE:0.0708\n",
      "Epoch: : 088, Accuracy: 0.8571, NNL: 0.4806, ECE:0.0708\n",
      "Epoch: : 089, Accuracy: 0.8571, NNL: 0.4807, ECE:0.0708\n",
      "Epoch: : 090, Accuracy: 0.8571, NNL: 0.4808, ECE:0.0708\n",
      "Epoch: : 091, Accuracy: 0.8571, NNL: 0.4809, ECE:0.0708\n",
      "Epoch: : 092, Accuracy: 0.8571, NNL: 0.4811, ECE:0.0708\n",
      "Epoch: : 093, Accuracy: 0.8571, NNL: 0.4812, ECE:0.0708\n",
      "Epoch: : 094, Accuracy: 0.8571, NNL: 0.4813, ECE:0.0708\n",
      "Epoch: : 095, Accuracy: 0.8571, NNL: 0.4814, ECE:0.0708\n",
      "Epoch: : 096, Accuracy: 0.8571, NNL: 0.4815, ECE:0.0708\n",
      "Epoch: : 097, Accuracy: 0.8571, NNL: 0.4816, ECE:0.0708\n",
      "Epoch: : 098, Accuracy: 0.8571, NNL: 0.4817, ECE:0.0708\n",
      "Epoch: : 099, Accuracy: 0.8571, NNL: 0.4818, ECE:0.0708\n",
      "Epoch: : 100, Accuracy: 0.8571, NNL: 0.4819, ECE:0.0708\n",
      "Epoch: : 101, Accuracy: 0.8571, NNL: 0.4820, ECE:0.0708\n",
      "Epoch: : 102, Accuracy: 0.8571, NNL: 0.4821, ECE:0.0708\n",
      "Epoch: : 103, Accuracy: 0.8571, NNL: 0.4821, ECE:0.0708\n",
      "Epoch: : 104, Accuracy: 0.8571, NNL: 0.4822, ECE:0.0708\n",
      "Epoch: : 105, Accuracy: 0.8571, NNL: 0.4822, ECE:0.0708\n",
      "Epoch: : 106, Accuracy: 0.8571, NNL: 0.4823, ECE:0.0708\n",
      "Epoch: : 107, Accuracy: 0.8571, NNL: 0.4823, ECE:0.0708\n",
      "Epoch: : 108, Accuracy: 0.8571, NNL: 0.4824, ECE:0.0708\n",
      "Epoch: : 109, Accuracy: 0.8571, NNL: 0.4824, ECE:0.0708\n",
      "Epoch: : 110, Accuracy: 0.8571, NNL: 0.4824, ECE:0.0708\n",
      "Epoch: : 111, Accuracy: 0.8571, NNL: 0.4824, ECE:0.0708\n",
      "Epoch: : 112, Accuracy: 0.8571, NNL: 0.4825, ECE:0.0708\n",
      "Epoch: : 113, Accuracy: 0.8571, NNL: 0.4825, ECE:0.0708\n",
      "Epoch: : 114, Accuracy: 0.8571, NNL: 0.4825, ECE:0.0708\n",
      "Epoch: : 115, Accuracy: 0.8571, NNL: 0.4825, ECE:0.0708\n",
      "Epoch: : 116, Accuracy: 0.8571, NNL: 0.4825, ECE:0.0708\n",
      "Epoch: : 117, Accuracy: 0.8571, NNL: 0.4825, ECE:0.0708\n",
      "Epoch: : 118, Accuracy: 0.8571, NNL: 0.4825, ECE:0.0708\n",
      "Epoch: : 119, Accuracy: 0.8571, NNL: 0.4825, ECE:0.0708\n",
      "Epoch: : 120, Accuracy: 0.8571, NNL: 0.4824, ECE:0.0708\n",
      "Epoch: : 121, Accuracy: 0.8571, NNL: 0.4824, ECE:0.0708\n",
      "Epoch: : 122, Accuracy: 0.8571, NNL: 0.4824, ECE:0.0708\n",
      "Epoch: : 123, Accuracy: 0.8571, NNL: 0.4824, ECE:0.0708\n",
      "Epoch: : 124, Accuracy: 0.8571, NNL: 0.4824, ECE:0.0708\n",
      "Epoch: : 125, Accuracy: 0.8571, NNL: 0.4824, ECE:0.0708\n",
      "Epoch: : 126, Accuracy: 0.8571, NNL: 0.4823, ECE:0.0708\n",
      "Epoch: : 127, Accuracy: 0.8571, NNL: 0.4823, ECE:0.0708\n",
      "Epoch: : 128, Accuracy: 0.8571, NNL: 0.4823, ECE:0.0708\n",
      "Epoch: : 129, Accuracy: 0.8571, NNL: 0.4823, ECE:0.0708\n",
      "Epoch: : 130, Accuracy: 0.8571, NNL: 0.4823, ECE:0.0708\n",
      "Epoch: : 131, Accuracy: 0.8571, NNL: 0.4822, ECE:0.0708\n",
      "Epoch: : 132, Accuracy: 0.8571, NNL: 0.4822, ECE:0.0708\n",
      "Epoch: : 133, Accuracy: 0.8571, NNL: 0.4822, ECE:0.0708\n",
      "Epoch: : 134, Accuracy: 0.8571, NNL: 0.4822, ECE:0.0708\n",
      "Epoch: : 135, Accuracy: 0.8571, NNL: 0.4822, ECE:0.0708\n",
      "Epoch: : 136, Accuracy: 0.8571, NNL: 0.4822, ECE:0.0708\n",
      "Epoch: : 137, Accuracy: 0.8571, NNL: 0.4821, ECE:0.0708\n",
      "Epoch: : 138, Accuracy: 0.8571, NNL: 0.4821, ECE:0.0708\n",
      "Epoch: : 139, Accuracy: 0.8571, NNL: 0.4821, ECE:0.0708\n",
      "Epoch: : 140, Accuracy: 0.8571, NNL: 0.4821, ECE:0.0708\n",
      "Epoch: : 141, Accuracy: 0.8571, NNL: 0.4821, ECE:0.0708\n",
      "Epoch: : 142, Accuracy: 0.8571, NNL: 0.4821, ECE:0.0708\n",
      "Epoch: : 143, Accuracy: 0.8571, NNL: 0.4821, ECE:0.0708\n",
      "Epoch: : 144, Accuracy: 0.8571, NNL: 0.4821, ECE:0.0708\n",
      "Epoch: : 145, Accuracy: 0.8571, NNL: 0.4821, ECE:0.0708\n",
      "Epoch: : 146, Accuracy: 0.8571, NNL: 0.4821, ECE:0.0708\n",
      "Epoch: : 147, Accuracy: 0.8571, NNL: 0.4821, ECE:0.0708\n",
      "Epoch: : 148, Accuracy: 0.8571, NNL: 0.4821, ECE:0.0708\n",
      "Epoch: : 149, Accuracy: 0.8571, NNL: 0.4821, ECE:0.0708\n",
      "Epoch: : 150, Accuracy: 0.8571, NNL: 0.4821, ECE:0.0708\n",
      "Epoch: : 151, Accuracy: 0.8571, NNL: 0.4821, ECE:0.0708\n",
      "Epoch: : 152, Accuracy: 0.8571, NNL: 0.4821, ECE:0.0708\n",
      "Epoch: : 153, Accuracy: 0.8571, NNL: 0.4821, ECE:0.0708\n",
      "Epoch: : 154, Accuracy: 0.8571, NNL: 0.4821, ECE:0.0708\n",
      "Epoch: : 155, Accuracy: 0.8571, NNL: 0.4821, ECE:0.0708\n",
      "Epoch: : 156, Accuracy: 0.8571, NNL: 0.4821, ECE:0.0708\n",
      "Epoch: : 157, Accuracy: 0.8571, NNL: 0.4821, ECE:0.0708\n",
      "Epoch: : 158, Accuracy: 0.8571, NNL: 0.4821, ECE:0.0708\n",
      "Epoch: : 159, Accuracy: 0.8571, NNL: 0.4821, ECE:0.0708\n",
      "Epoch: : 160, Accuracy: 0.8571, NNL: 0.4821, ECE:0.0708\n",
      "Epoch: : 161, Accuracy: 0.8571, NNL: 0.4821, ECE:0.0708\n",
      "Epoch: : 162, Accuracy: 0.8571, NNL: 0.4821, ECE:0.0708\n",
      "Epoch: : 163, Accuracy: 0.8571, NNL: 0.4821, ECE:0.0708\n",
      "Epoch: : 164, Accuracy: 0.8571, NNL: 0.4821, ECE:0.0708\n",
      "Epoch: : 165, Accuracy: 0.8571, NNL: 0.4821, ECE:0.0708\n",
      "Epoch: : 166, Accuracy: 0.8571, NNL: 0.4821, ECE:0.0708\n",
      "Epoch: : 167, Accuracy: 0.8571, NNL: 0.4821, ECE:0.0708\n",
      "Epoch: : 168, Accuracy: 0.8571, NNL: 0.4821, ECE:0.0708\n",
      "Epoch: : 169, Accuracy: 0.8571, NNL: 0.4821, ECE:0.0708\n",
      "Epoch: : 170, Accuracy: 0.8571, NNL: 0.4821, ECE:0.0708\n",
      "Epoch: : 171, Accuracy: 0.8571, NNL: 0.4821, ECE:0.0708\n",
      "Epoch: : 172, Accuracy: 0.8571, NNL: 0.4821, ECE:0.0708\n",
      "Epoch: : 173, Accuracy: 0.8571, NNL: 0.4821, ECE:0.0708\n",
      "Epoch: : 174, Accuracy: 0.8571, NNL: 0.4821, ECE:0.0708\n",
      "run 4 TS tempreture:  1.0084925889968872\n",
      "Epoch: : 001, Accuracy: 0.9071, NNL: 0.4277, ECE:0.0671\n",
      "Epoch: : 002, Accuracy: 0.9071, NNL: 0.4253, ECE:0.0671\n",
      "Epoch: : 003, Accuracy: 0.9071, NNL: 0.4229, ECE:0.0671\n",
      "Epoch: : 004, Accuracy: 0.9071, NNL: 0.4205, ECE:0.0671\n",
      "Epoch: : 005, Accuracy: 0.9071, NNL: 0.4182, ECE:0.0671\n",
      "Epoch: : 006, Accuracy: 0.9071, NNL: 0.4158, ECE:0.0671\n",
      "Epoch: : 007, Accuracy: 0.9071, NNL: 0.4135, ECE:0.0671\n",
      "Epoch: : 008, Accuracy: 0.9071, NNL: 0.4111, ECE:0.0671\n",
      "Epoch: : 009, Accuracy: 0.9071, NNL: 0.4088, ECE:0.0671\n",
      "Epoch: : 010, Accuracy: 0.9071, NNL: 0.4064, ECE:0.0671\n",
      "Epoch: : 011, Accuracy: 0.9071, NNL: 0.4041, ECE:0.0671\n",
      "Epoch: : 012, Accuracy: 0.9071, NNL: 0.4018, ECE:0.0671\n",
      "Epoch: : 013, Accuracy: 0.9071, NNL: 0.3995, ECE:0.0671\n",
      "Epoch: : 014, Accuracy: 0.9071, NNL: 0.3972, ECE:0.0671\n",
      "Epoch: : 015, Accuracy: 0.9071, NNL: 0.3949, ECE:0.0671\n",
      "Epoch: : 016, Accuracy: 0.9071, NNL: 0.3927, ECE:0.0671\n",
      "Epoch: : 017, Accuracy: 0.9071, NNL: 0.3904, ECE:0.0671\n",
      "Epoch: : 018, Accuracy: 0.9071, NNL: 0.3882, ECE:0.0671\n",
      "Epoch: : 019, Accuracy: 0.9071, NNL: 0.3860, ECE:0.0671\n",
      "Epoch: : 020, Accuracy: 0.9071, NNL: 0.3838, ECE:0.0671\n",
      "Epoch: : 021, Accuracy: 0.9071, NNL: 0.3816, ECE:0.0671\n",
      "Epoch: : 022, Accuracy: 0.9071, NNL: 0.3795, ECE:0.0671\n",
      "Epoch: : 023, Accuracy: 0.9071, NNL: 0.3773, ECE:0.0671\n",
      "Epoch: : 024, Accuracy: 0.9071, NNL: 0.3752, ECE:0.0671\n",
      "Epoch: : 025, Accuracy: 0.9071, NNL: 0.3732, ECE:0.0671\n",
      "Epoch: : 026, Accuracy: 0.9071, NNL: 0.3711, ECE:0.0671\n",
      "Epoch: : 027, Accuracy: 0.9071, NNL: 0.3691, ECE:0.0671\n",
      "Epoch: : 028, Accuracy: 0.9071, NNL: 0.3671, ECE:0.0671\n",
      "Epoch: : 029, Accuracy: 0.9071, NNL: 0.3652, ECE:0.0671\n",
      "Epoch: : 030, Accuracy: 0.9071, NNL: 0.3632, ECE:0.0671\n",
      "Epoch: : 031, Accuracy: 0.9071, NNL: 0.3614, ECE:0.0671\n",
      "Epoch: : 032, Accuracy: 0.9071, NNL: 0.3595, ECE:0.0671\n",
      "Epoch: : 033, Accuracy: 0.9071, NNL: 0.3577, ECE:0.0671\n",
      "Epoch: : 034, Accuracy: 0.9071, NNL: 0.3559, ECE:0.0671\n",
      "Epoch: : 035, Accuracy: 0.9071, NNL: 0.3542, ECE:0.0671\n",
      "Epoch: : 036, Accuracy: 0.9071, NNL: 0.3525, ECE:0.0671\n",
      "Epoch: : 037, Accuracy: 0.9071, NNL: 0.3509, ECE:0.0671\n",
      "Epoch: : 038, Accuracy: 0.9071, NNL: 0.3493, ECE:0.0671\n",
      "Epoch: : 039, Accuracy: 0.9071, NNL: 0.3478, ECE:0.0671\n",
      "Epoch: : 040, Accuracy: 0.9071, NNL: 0.3463, ECE:0.0671\n",
      "Epoch: : 041, Accuracy: 0.9071, NNL: 0.3449, ECE:0.0671\n",
      "Epoch: : 042, Accuracy: 0.9071, NNL: 0.3435, ECE:0.0671\n",
      "Epoch: : 043, Accuracy: 0.9071, NNL: 0.3422, ECE:0.0671\n",
      "Epoch: : 044, Accuracy: 0.9071, NNL: 0.3409, ECE:0.0671\n",
      "Epoch: : 045, Accuracy: 0.9071, NNL: 0.3397, ECE:0.0671\n",
      "Epoch: : 046, Accuracy: 0.9071, NNL: 0.3385, ECE:0.0671\n",
      "Epoch: : 047, Accuracy: 0.9071, NNL: 0.3374, ECE:0.0671\n",
      "Epoch: : 048, Accuracy: 0.9071, NNL: 0.3364, ECE:0.0671\n",
      "Epoch: : 049, Accuracy: 0.9071, NNL: 0.3354, ECE:0.0671\n",
      "Epoch: : 050, Accuracy: 0.9071, NNL: 0.3345, ECE:0.0671\n",
      "Epoch: : 051, Accuracy: 0.9071, NNL: 0.3336, ECE:0.0671\n",
      "Epoch: : 052, Accuracy: 0.9071, NNL: 0.3328, ECE:0.0671\n",
      "Epoch: : 053, Accuracy: 0.9071, NNL: 0.3321, ECE:0.0671\n",
      "Epoch: : 054, Accuracy: 0.9071, NNL: 0.3314, ECE:0.0671\n",
      "Epoch: : 055, Accuracy: 0.9071, NNL: 0.3307, ECE:0.0671\n",
      "Epoch: : 056, Accuracy: 0.9071, NNL: 0.3302, ECE:0.0671\n",
      "Epoch: : 057, Accuracy: 0.9071, NNL: 0.3296, ECE:0.0671\n",
      "Epoch: : 058, Accuracy: 0.9071, NNL: 0.3292, ECE:0.0671\n",
      "Epoch: : 059, Accuracy: 0.9071, NNL: 0.3288, ECE:0.0671\n",
      "Epoch: : 060, Accuracy: 0.9071, NNL: 0.3284, ECE:0.0671\n",
      "Epoch: : 061, Accuracy: 0.9071, NNL: 0.3281, ECE:0.0671\n",
      "Epoch: : 062, Accuracy: 0.9071, NNL: 0.3278, ECE:0.0671\n",
      "Epoch: : 063, Accuracy: 0.9071, NNL: 0.3276, ECE:0.0671\n",
      "Epoch: : 064, Accuracy: 0.9071, NNL: 0.3274, ECE:0.0671\n",
      "Epoch: : 065, Accuracy: 0.9071, NNL: 0.3273, ECE:0.0671\n",
      "Epoch: : 066, Accuracy: 0.9071, NNL: 0.3272, ECE:0.0671\n",
      "Epoch: : 067, Accuracy: 0.9071, NNL: 0.3271, ECE:0.0671\n",
      "Epoch: : 068, Accuracy: 0.9071, NNL: 0.3271, ECE:0.0671\n",
      "Epoch: : 069, Accuracy: 0.9071, NNL: 0.3271, ECE:0.0671\n",
      "Epoch: : 070, Accuracy: 0.9071, NNL: 0.3271, ECE:0.0671\n",
      "Epoch: : 071, Accuracy: 0.9071, NNL: 0.3271, ECE:0.0671\n",
      "Epoch: : 072, Accuracy: 0.9071, NNL: 0.3272, ECE:0.0671\n",
      "Epoch: : 073, Accuracy: 0.9071, NNL: 0.3273, ECE:0.0671\n",
      "Epoch: : 074, Accuracy: 0.9071, NNL: 0.3274, ECE:0.0671\n",
      "Epoch: : 075, Accuracy: 0.9071, NNL: 0.3275, ECE:0.0671\n",
      "Epoch: : 076, Accuracy: 0.9071, NNL: 0.3276, ECE:0.0671\n",
      "Epoch: : 077, Accuracy: 0.9071, NNL: 0.3278, ECE:0.0671\n",
      "Epoch: : 078, Accuracy: 0.9071, NNL: 0.3279, ECE:0.0671\n",
      "Epoch: : 079, Accuracy: 0.9071, NNL: 0.3281, ECE:0.0671\n",
      "Epoch: : 080, Accuracy: 0.9071, NNL: 0.3283, ECE:0.0671\n",
      "Epoch: : 081, Accuracy: 0.9071, NNL: 0.3284, ECE:0.0671\n",
      "Epoch: : 082, Accuracy: 0.9071, NNL: 0.3286, ECE:0.0671\n",
      "Epoch: : 083, Accuracy: 0.9071, NNL: 0.3288, ECE:0.0671\n",
      "Epoch: : 084, Accuracy: 0.9071, NNL: 0.3289, ECE:0.0671\n",
      "Epoch: : 085, Accuracy: 0.9071, NNL: 0.3291, ECE:0.0671\n",
      "Epoch: : 086, Accuracy: 0.9071, NNL: 0.3293, ECE:0.0671\n",
      "Epoch: : 087, Accuracy: 0.9071, NNL: 0.3294, ECE:0.0671\n",
      "Epoch: : 088, Accuracy: 0.9071, NNL: 0.3296, ECE:0.0671\n",
      "Epoch: : 089, Accuracy: 0.9071, NNL: 0.3297, ECE:0.0671\n",
      "Epoch: : 090, Accuracy: 0.9071, NNL: 0.3298, ECE:0.0671\n",
      "Epoch: : 091, Accuracy: 0.9071, NNL: 0.3300, ECE:0.0671\n",
      "Epoch: : 092, Accuracy: 0.9071, NNL: 0.3301, ECE:0.0671\n",
      "Epoch: : 093, Accuracy: 0.9071, NNL: 0.3302, ECE:0.0671\n",
      "Epoch: : 094, Accuracy: 0.9071, NNL: 0.3303, ECE:0.0671\n",
      "Epoch: : 095, Accuracy: 0.9071, NNL: 0.3304, ECE:0.0671\n",
      "Epoch: : 096, Accuracy: 0.9071, NNL: 0.3305, ECE:0.0671\n",
      "Epoch: : 097, Accuracy: 0.9071, NNL: 0.3306, ECE:0.0671\n",
      "Epoch: : 098, Accuracy: 0.9071, NNL: 0.3306, ECE:0.0671\n",
      "Epoch: : 099, Accuracy: 0.9071, NNL: 0.3307, ECE:0.0671\n",
      "Epoch: : 100, Accuracy: 0.9071, NNL: 0.3307, ECE:0.0671\n",
      "Epoch: : 101, Accuracy: 0.9071, NNL: 0.3308, ECE:0.0671\n",
      "Epoch: : 102, Accuracy: 0.9071, NNL: 0.3308, ECE:0.0671\n",
      "Epoch: : 103, Accuracy: 0.9071, NNL: 0.3308, ECE:0.0671\n",
      "Epoch: : 104, Accuracy: 0.9071, NNL: 0.3309, ECE:0.0671\n",
      "Epoch: : 105, Accuracy: 0.9071, NNL: 0.3309, ECE:0.0671\n",
      "Epoch: : 106, Accuracy: 0.9071, NNL: 0.3309, ECE:0.0671\n",
      "Epoch: : 107, Accuracy: 0.9071, NNL: 0.3309, ECE:0.0671\n",
      "Epoch: : 108, Accuracy: 0.9071, NNL: 0.3309, ECE:0.0671\n",
      "Epoch: : 109, Accuracy: 0.9071, NNL: 0.3309, ECE:0.0671\n",
      "Epoch: : 110, Accuracy: 0.9071, NNL: 0.3309, ECE:0.0671\n",
      "Epoch: : 111, Accuracy: 0.9071, NNL: 0.3309, ECE:0.0671\n",
      "Epoch: : 112, Accuracy: 0.9071, NNL: 0.3308, ECE:0.0671\n",
      "Epoch: : 113, Accuracy: 0.9071, NNL: 0.3308, ECE:0.0671\n",
      "Epoch: : 114, Accuracy: 0.9071, NNL: 0.3308, ECE:0.0671\n",
      "Epoch: : 115, Accuracy: 0.9071, NNL: 0.3308, ECE:0.0671\n",
      "Epoch: : 116, Accuracy: 0.9071, NNL: 0.3307, ECE:0.0671\n",
      "Epoch: : 117, Accuracy: 0.9071, NNL: 0.3307, ECE:0.0671\n",
      "Epoch: : 118, Accuracy: 0.9071, NNL: 0.3307, ECE:0.0671\n",
      "Epoch: : 119, Accuracy: 0.9071, NNL: 0.3307, ECE:0.0671\n",
      "Epoch: : 120, Accuracy: 0.9071, NNL: 0.3306, ECE:0.0671\n",
      "Epoch: : 121, Accuracy: 0.9071, NNL: 0.3306, ECE:0.0671\n",
      "Epoch: : 122, Accuracy: 0.9071, NNL: 0.3306, ECE:0.0671\n",
      "Epoch: : 123, Accuracy: 0.9071, NNL: 0.3306, ECE:0.0671\n",
      "Epoch: : 124, Accuracy: 0.9071, NNL: 0.3305, ECE:0.0671\n",
      "Epoch: : 125, Accuracy: 0.9071, NNL: 0.3305, ECE:0.0671\n",
      "Epoch: : 126, Accuracy: 0.9071, NNL: 0.3305, ECE:0.0671\n",
      "Epoch: : 127, Accuracy: 0.9071, NNL: 0.3305, ECE:0.0671\n",
      "Epoch: : 128, Accuracy: 0.9071, NNL: 0.3304, ECE:0.0671\n",
      "Epoch: : 129, Accuracy: 0.9071, NNL: 0.3304, ECE:0.0671\n",
      "Epoch: : 130, Accuracy: 0.9071, NNL: 0.3304, ECE:0.0671\n",
      "Epoch: : 131, Accuracy: 0.9071, NNL: 0.3304, ECE:0.0671\n",
      "Epoch: : 132, Accuracy: 0.9071, NNL: 0.3304, ECE:0.0671\n",
      "Epoch: : 133, Accuracy: 0.9071, NNL: 0.3304, ECE:0.0671\n",
      "Epoch: : 134, Accuracy: 0.9071, NNL: 0.3304, ECE:0.0671\n",
      "Epoch: : 135, Accuracy: 0.9071, NNL: 0.3304, ECE:0.0671\n",
      "Epoch: : 136, Accuracy: 0.9071, NNL: 0.3303, ECE:0.0671\n",
      "Epoch: : 137, Accuracy: 0.9071, NNL: 0.3303, ECE:0.0671\n",
      "Epoch: : 138, Accuracy: 0.9071, NNL: 0.3303, ECE:0.0671\n",
      "Epoch: : 139, Accuracy: 0.9071, NNL: 0.3303, ECE:0.0671\n",
      "Epoch: : 140, Accuracy: 0.9071, NNL: 0.3303, ECE:0.0671\n",
      "Epoch: : 141, Accuracy: 0.9071, NNL: 0.3303, ECE:0.0671\n",
      "Epoch: : 142, Accuracy: 0.9071, NNL: 0.3303, ECE:0.0671\n",
      "Epoch: : 143, Accuracy: 0.9071, NNL: 0.3303, ECE:0.0671\n",
      "Epoch: : 144, Accuracy: 0.9071, NNL: 0.3303, ECE:0.0671\n",
      "Epoch: : 145, Accuracy: 0.9071, NNL: 0.3303, ECE:0.0671\n",
      "Epoch: : 146, Accuracy: 0.9071, NNL: 0.3303, ECE:0.0671\n",
      "Epoch: : 147, Accuracy: 0.9071, NNL: 0.3303, ECE:0.0671\n",
      "Epoch: : 148, Accuracy: 0.9071, NNL: 0.3303, ECE:0.0671\n",
      "Epoch: : 149, Accuracy: 0.9071, NNL: 0.3303, ECE:0.0671\n",
      "Epoch: : 150, Accuracy: 0.9071, NNL: 0.3304, ECE:0.0671\n",
      "Epoch: : 151, Accuracy: 0.9071, NNL: 0.3304, ECE:0.0671\n",
      "Epoch: : 152, Accuracy: 0.9071, NNL: 0.3304, ECE:0.0671\n",
      "Epoch: : 153, Accuracy: 0.9071, NNL: 0.3304, ECE:0.0671\n",
      "Epoch: : 154, Accuracy: 0.9071, NNL: 0.3304, ECE:0.0671\n",
      "Epoch: : 155, Accuracy: 0.9071, NNL: 0.3304, ECE:0.0671\n",
      "Epoch: : 156, Accuracy: 0.9071, NNL: 0.3304, ECE:0.0671\n",
      "Epoch: : 157, Accuracy: 0.9071, NNL: 0.3304, ECE:0.0671\n",
      "Epoch: : 158, Accuracy: 0.9071, NNL: 0.3304, ECE:0.0671\n",
      "Epoch: : 159, Accuracy: 0.9071, NNL: 0.3304, ECE:0.0671\n",
      "Epoch: : 160, Accuracy: 0.9071, NNL: 0.3304, ECE:0.0671\n",
      "Epoch: : 161, Accuracy: 0.9071, NNL: 0.3304, ECE:0.0671\n",
      "Epoch: : 162, Accuracy: 0.9071, NNL: 0.3304, ECE:0.0671\n",
      "Epoch: : 163, Accuracy: 0.9071, NNL: 0.3304, ECE:0.0671\n",
      "Epoch: : 164, Accuracy: 0.9071, NNL: 0.3304, ECE:0.0671\n",
      "Epoch: : 165, Accuracy: 0.9071, NNL: 0.3304, ECE:0.0671\n",
      "Epoch: : 166, Accuracy: 0.9071, NNL: 0.3304, ECE:0.0671\n",
      "Epoch: : 167, Accuracy: 0.9071, NNL: 0.3304, ECE:0.0671\n",
      "Epoch: : 168, Accuracy: 0.9071, NNL: 0.3304, ECE:0.0671\n",
      "run 5 TS tempreture:  1.0526481866836548\n",
      "Epoch: : 001, Accuracy: 0.8357, NNL: 0.5835, ECE:0.0658\n",
      "Epoch: : 002, Accuracy: 0.8357, NNL: 0.5812, ECE:0.0658\n",
      "Epoch: : 003, Accuracy: 0.8357, NNL: 0.5789, ECE:0.0658\n",
      "Epoch: : 004, Accuracy: 0.8357, NNL: 0.5766, ECE:0.0658\n",
      "Epoch: : 005, Accuracy: 0.8357, NNL: 0.5743, ECE:0.0658\n",
      "Epoch: : 006, Accuracy: 0.8357, NNL: 0.5720, ECE:0.0658\n",
      "Epoch: : 007, Accuracy: 0.8357, NNL: 0.5697, ECE:0.0658\n",
      "Epoch: : 008, Accuracy: 0.8357, NNL: 0.5674, ECE:0.0658\n",
      "Epoch: : 009, Accuracy: 0.8357, NNL: 0.5651, ECE:0.0658\n",
      "Epoch: : 010, Accuracy: 0.8357, NNL: 0.5628, ECE:0.0658\n",
      "Epoch: : 011, Accuracy: 0.8357, NNL: 0.5606, ECE:0.0658\n",
      "Epoch: : 012, Accuracy: 0.8357, NNL: 0.5583, ECE:0.0658\n",
      "Epoch: : 013, Accuracy: 0.8357, NNL: 0.5561, ECE:0.0658\n",
      "Epoch: : 014, Accuracy: 0.8357, NNL: 0.5538, ECE:0.0658\n",
      "Epoch: : 015, Accuracy: 0.8357, NNL: 0.5516, ECE:0.0658\n",
      "Epoch: : 016, Accuracy: 0.8357, NNL: 0.5494, ECE:0.0658\n",
      "Epoch: : 017, Accuracy: 0.8357, NNL: 0.5472, ECE:0.0658\n",
      "Epoch: : 018, Accuracy: 0.8357, NNL: 0.5450, ECE:0.0658\n",
      "Epoch: : 019, Accuracy: 0.8357, NNL: 0.5428, ECE:0.0658\n",
      "Epoch: : 020, Accuracy: 0.8357, NNL: 0.5406, ECE:0.0658\n",
      "Epoch: : 021, Accuracy: 0.8357, NNL: 0.5384, ECE:0.0658\n",
      "Epoch: : 022, Accuracy: 0.8357, NNL: 0.5363, ECE:0.0658\n",
      "Epoch: : 023, Accuracy: 0.8357, NNL: 0.5341, ECE:0.0658\n",
      "Epoch: : 024, Accuracy: 0.8357, NNL: 0.5320, ECE:0.0658\n",
      "Epoch: : 025, Accuracy: 0.8357, NNL: 0.5299, ECE:0.0658\n",
      "Epoch: : 026, Accuracy: 0.8357, NNL: 0.5278, ECE:0.0658\n",
      "Epoch: : 027, Accuracy: 0.8357, NNL: 0.5258, ECE:0.0658\n",
      "Epoch: : 028, Accuracy: 0.8357, NNL: 0.5237, ECE:0.0658\n",
      "Epoch: : 029, Accuracy: 0.8357, NNL: 0.5217, ECE:0.0658\n",
      "Epoch: : 030, Accuracy: 0.8357, NNL: 0.5197, ECE:0.0658\n",
      "Epoch: : 031, Accuracy: 0.8357, NNL: 0.5178, ECE:0.0658\n",
      "Epoch: : 032, Accuracy: 0.8357, NNL: 0.5158, ECE:0.0658\n",
      "Epoch: : 033, Accuracy: 0.8357, NNL: 0.5139, ECE:0.0658\n",
      "Epoch: : 034, Accuracy: 0.8357, NNL: 0.5120, ECE:0.0658\n",
      "Epoch: : 035, Accuracy: 0.8357, NNL: 0.5101, ECE:0.0658\n",
      "Epoch: : 036, Accuracy: 0.8357, NNL: 0.5083, ECE:0.0658\n",
      "Epoch: : 037, Accuracy: 0.8357, NNL: 0.5065, ECE:0.0658\n",
      "Epoch: : 038, Accuracy: 0.8357, NNL: 0.5047, ECE:0.0658\n",
      "Epoch: : 039, Accuracy: 0.8357, NNL: 0.5030, ECE:0.0658\n",
      "Epoch: : 040, Accuracy: 0.8357, NNL: 0.5013, ECE:0.0658\n",
      "Epoch: : 041, Accuracy: 0.8357, NNL: 0.4996, ECE:0.0658\n",
      "Epoch: : 042, Accuracy: 0.8357, NNL: 0.4980, ECE:0.0658\n",
      "Epoch: : 043, Accuracy: 0.8357, NNL: 0.4964, ECE:0.0658\n",
      "Epoch: : 044, Accuracy: 0.8357, NNL: 0.4948, ECE:0.0658\n",
      "Epoch: : 045, Accuracy: 0.8357, NNL: 0.4933, ECE:0.0658\n",
      "Epoch: : 046, Accuracy: 0.8357, NNL: 0.4918, ECE:0.0658\n",
      "Epoch: : 047, Accuracy: 0.8357, NNL: 0.4904, ECE:0.0658\n",
      "Epoch: : 048, Accuracy: 0.8357, NNL: 0.4890, ECE:0.0658\n",
      "Epoch: : 049, Accuracy: 0.8357, NNL: 0.4877, ECE:0.0658\n",
      "Epoch: : 050, Accuracy: 0.8357, NNL: 0.4864, ECE:0.0658\n",
      "Epoch: : 051, Accuracy: 0.8357, NNL: 0.4851, ECE:0.0658\n",
      "Epoch: : 052, Accuracy: 0.8357, NNL: 0.4839, ECE:0.0658\n",
      "Epoch: : 053, Accuracy: 0.8357, NNL: 0.4827, ECE:0.0658\n",
      "Epoch: : 054, Accuracy: 0.8357, NNL: 0.4816, ECE:0.0658\n",
      "Epoch: : 055, Accuracy: 0.8357, NNL: 0.4806, ECE:0.0658\n",
      "Epoch: : 056, Accuracy: 0.8357, NNL: 0.4796, ECE:0.0658\n",
      "Epoch: : 057, Accuracy: 0.8357, NNL: 0.4786, ECE:0.0658\n",
      "Epoch: : 058, Accuracy: 0.8357, NNL: 0.4777, ECE:0.0658\n",
      "Epoch: : 059, Accuracy: 0.8357, NNL: 0.4768, ECE:0.0658\n",
      "Epoch: : 060, Accuracy: 0.8357, NNL: 0.4760, ECE:0.0658\n",
      "Epoch: : 061, Accuracy: 0.8357, NNL: 0.4752, ECE:0.0658\n",
      "Epoch: : 062, Accuracy: 0.8357, NNL: 0.4745, ECE:0.0658\n",
      "Epoch: : 063, Accuracy: 0.8357, NNL: 0.4738, ECE:0.0658\n",
      "Epoch: : 064, Accuracy: 0.8357, NNL: 0.4732, ECE:0.0658\n",
      "Epoch: : 065, Accuracy: 0.8357, NNL: 0.4726, ECE:0.0658\n",
      "Epoch: : 066, Accuracy: 0.8357, NNL: 0.4721, ECE:0.0658\n",
      "Epoch: : 067, Accuracy: 0.8357, NNL: 0.4716, ECE:0.0658\n",
      "Epoch: : 068, Accuracy: 0.8357, NNL: 0.4712, ECE:0.0658\n",
      "Epoch: : 069, Accuracy: 0.8357, NNL: 0.4708, ECE:0.0658\n",
      "Epoch: : 070, Accuracy: 0.8357, NNL: 0.4704, ECE:0.0658\n",
      "Epoch: : 071, Accuracy: 0.8357, NNL: 0.4701, ECE:0.0658\n",
      "Epoch: : 072, Accuracy: 0.8357, NNL: 0.4699, ECE:0.0658\n",
      "Epoch: : 073, Accuracy: 0.8357, NNL: 0.4696, ECE:0.0658\n",
      "Epoch: : 074, Accuracy: 0.8357, NNL: 0.4694, ECE:0.0658\n",
      "Epoch: : 075, Accuracy: 0.8357, NNL: 0.4692, ECE:0.0658\n",
      "Epoch: : 076, Accuracy: 0.8357, NNL: 0.4691, ECE:0.0658\n",
      "Epoch: : 077, Accuracy: 0.8357, NNL: 0.4690, ECE:0.0658\n",
      "Epoch: : 078, Accuracy: 0.8357, NNL: 0.4689, ECE:0.0658\n",
      "Epoch: : 079, Accuracy: 0.8357, NNL: 0.4689, ECE:0.0658\n",
      "Epoch: : 080, Accuracy: 0.8357, NNL: 0.4688, ECE:0.0658\n",
      "Epoch: : 081, Accuracy: 0.8357, NNL: 0.4688, ECE:0.0658\n",
      "Epoch: : 082, Accuracy: 0.8357, NNL: 0.4688, ECE:0.0658\n",
      "Epoch: : 083, Accuracy: 0.8357, NNL: 0.4689, ECE:0.0658\n",
      "Epoch: : 084, Accuracy: 0.8357, NNL: 0.4689, ECE:0.0658\n",
      "Epoch: : 085, Accuracy: 0.8357, NNL: 0.4690, ECE:0.0658\n",
      "Epoch: : 086, Accuracy: 0.8357, NNL: 0.4690, ECE:0.0658\n",
      "Epoch: : 087, Accuracy: 0.8357, NNL: 0.4691, ECE:0.0658\n",
      "Epoch: : 088, Accuracy: 0.8357, NNL: 0.4692, ECE:0.0658\n",
      "Epoch: : 089, Accuracy: 0.8357, NNL: 0.4693, ECE:0.0658\n",
      "Epoch: : 090, Accuracy: 0.8357, NNL: 0.4694, ECE:0.0658\n",
      "Epoch: : 091, Accuracy: 0.8357, NNL: 0.4695, ECE:0.0658\n",
      "Epoch: : 092, Accuracy: 0.8357, NNL: 0.4697, ECE:0.0658\n",
      "Epoch: : 093, Accuracy: 0.8357, NNL: 0.4698, ECE:0.0658\n",
      "Epoch: : 094, Accuracy: 0.8357, NNL: 0.4699, ECE:0.0658\n",
      "Epoch: : 095, Accuracy: 0.8357, NNL: 0.4700, ECE:0.0658\n",
      "Epoch: : 096, Accuracy: 0.8357, NNL: 0.4702, ECE:0.0658\n",
      "Epoch: : 097, Accuracy: 0.8357, NNL: 0.4703, ECE:0.0658\n",
      "Epoch: : 098, Accuracy: 0.8357, NNL: 0.4704, ECE:0.0658\n",
      "Epoch: : 099, Accuracy: 0.8357, NNL: 0.4705, ECE:0.0658\n",
      "Epoch: : 100, Accuracy: 0.8357, NNL: 0.4706, ECE:0.0658\n",
      "Epoch: : 101, Accuracy: 0.8357, NNL: 0.4707, ECE:0.0658\n",
      "Epoch: : 102, Accuracy: 0.8357, NNL: 0.4708, ECE:0.0658\n",
      "Epoch: : 103, Accuracy: 0.8357, NNL: 0.4709, ECE:0.0658\n",
      "Epoch: : 104, Accuracy: 0.8357, NNL: 0.4710, ECE:0.0658\n",
      "Epoch: : 105, Accuracy: 0.8357, NNL: 0.4711, ECE:0.0658\n",
      "Epoch: : 106, Accuracy: 0.8357, NNL: 0.4712, ECE:0.0658\n",
      "Epoch: : 107, Accuracy: 0.8357, NNL: 0.4713, ECE:0.0658\n",
      "Epoch: : 108, Accuracy: 0.8357, NNL: 0.4713, ECE:0.0658\n",
      "Epoch: : 109, Accuracy: 0.8357, NNL: 0.4714, ECE:0.0658\n",
      "Epoch: : 110, Accuracy: 0.8357, NNL: 0.4715, ECE:0.0658\n",
      "Epoch: : 111, Accuracy: 0.8357, NNL: 0.4715, ECE:0.0658\n",
      "Epoch: : 112, Accuracy: 0.8357, NNL: 0.4715, ECE:0.0658\n",
      "Epoch: : 113, Accuracy: 0.8357, NNL: 0.4716, ECE:0.0658\n",
      "Epoch: : 114, Accuracy: 0.8357, NNL: 0.4716, ECE:0.0658\n",
      "Epoch: : 115, Accuracy: 0.8357, NNL: 0.4716, ECE:0.0658\n",
      "Epoch: : 116, Accuracy: 0.8357, NNL: 0.4717, ECE:0.0658\n",
      "Epoch: : 117, Accuracy: 0.8357, NNL: 0.4717, ECE:0.0658\n",
      "Epoch: : 118, Accuracy: 0.8357, NNL: 0.4717, ECE:0.0658\n",
      "Epoch: : 119, Accuracy: 0.8357, NNL: 0.4717, ECE:0.0658\n",
      "Epoch: : 120, Accuracy: 0.8357, NNL: 0.4717, ECE:0.0658\n",
      "Epoch: : 121, Accuracy: 0.8357, NNL: 0.4717, ECE:0.0658\n",
      "Epoch: : 122, Accuracy: 0.8357, NNL: 0.4717, ECE:0.0658\n",
      "Epoch: : 123, Accuracy: 0.8357, NNL: 0.4717, ECE:0.0658\n",
      "Epoch: : 124, Accuracy: 0.8357, NNL: 0.4717, ECE:0.0658\n",
      "Epoch: : 125, Accuracy: 0.8357, NNL: 0.4717, ECE:0.0658\n",
      "Epoch: : 126, Accuracy: 0.8357, NNL: 0.4717, ECE:0.0658\n",
      "Epoch: : 127, Accuracy: 0.8357, NNL: 0.4717, ECE:0.0658\n",
      "Epoch: : 128, Accuracy: 0.8357, NNL: 0.4717, ECE:0.0658\n",
      "Epoch: : 129, Accuracy: 0.8357, NNL: 0.4717, ECE:0.0658\n",
      "Epoch: : 130, Accuracy: 0.8357, NNL: 0.4716, ECE:0.0658\n",
      "Epoch: : 131, Accuracy: 0.8357, NNL: 0.4716, ECE:0.0658\n",
      "Epoch: : 132, Accuracy: 0.8357, NNL: 0.4716, ECE:0.0658\n",
      "Epoch: : 133, Accuracy: 0.8357, NNL: 0.4716, ECE:0.0658\n",
      "Epoch: : 134, Accuracy: 0.8357, NNL: 0.4716, ECE:0.0658\n",
      "Epoch: : 135, Accuracy: 0.8357, NNL: 0.4715, ECE:0.0658\n",
      "Epoch: : 136, Accuracy: 0.8357, NNL: 0.4715, ECE:0.0658\n",
      "Epoch: : 137, Accuracy: 0.8357, NNL: 0.4715, ECE:0.0658\n",
      "Epoch: : 138, Accuracy: 0.8357, NNL: 0.4715, ECE:0.0658\n",
      "Epoch: : 139, Accuracy: 0.8357, NNL: 0.4715, ECE:0.0658\n",
      "Epoch: : 140, Accuracy: 0.8357, NNL: 0.4715, ECE:0.0658\n",
      "Epoch: : 141, Accuracy: 0.8357, NNL: 0.4715, ECE:0.0658\n",
      "Epoch: : 142, Accuracy: 0.8357, NNL: 0.4714, ECE:0.0658\n",
      "Epoch: : 143, Accuracy: 0.8357, NNL: 0.4714, ECE:0.0658\n",
      "Epoch: : 144, Accuracy: 0.8357, NNL: 0.4714, ECE:0.0658\n",
      "Epoch: : 145, Accuracy: 0.8357, NNL: 0.4714, ECE:0.0658\n",
      "Epoch: : 146, Accuracy: 0.8357, NNL: 0.4714, ECE:0.0658\n",
      "Epoch: : 147, Accuracy: 0.8357, NNL: 0.4714, ECE:0.0658\n",
      "Epoch: : 148, Accuracy: 0.8357, NNL: 0.4714, ECE:0.0658\n",
      "Epoch: : 149, Accuracy: 0.8357, NNL: 0.4714, ECE:0.0658\n",
      "Epoch: : 150, Accuracy: 0.8357, NNL: 0.4714, ECE:0.0658\n",
      "Epoch: : 151, Accuracy: 0.8357, NNL: 0.4714, ECE:0.0658\n",
      "Epoch: : 152, Accuracy: 0.8357, NNL: 0.4713, ECE:0.0658\n",
      "Epoch: : 153, Accuracy: 0.8357, NNL: 0.4713, ECE:0.0658\n",
      "Epoch: : 154, Accuracy: 0.8357, NNL: 0.4713, ECE:0.0658\n",
      "Epoch: : 155, Accuracy: 0.8357, NNL: 0.4713, ECE:0.0658\n",
      "Epoch: : 156, Accuracy: 0.8357, NNL: 0.4713, ECE:0.0658\n",
      "Epoch: : 157, Accuracy: 0.8357, NNL: 0.4713, ECE:0.0658\n",
      "Epoch: : 158, Accuracy: 0.8357, NNL: 0.4713, ECE:0.0658\n",
      "Epoch: : 159, Accuracy: 0.8357, NNL: 0.4713, ECE:0.0658\n",
      "Epoch: : 160, Accuracy: 0.8357, NNL: 0.4713, ECE:0.0658\n",
      "Epoch: : 161, Accuracy: 0.8357, NNL: 0.4713, ECE:0.0658\n",
      "Epoch: : 162, Accuracy: 0.8357, NNL: 0.4713, ECE:0.0658\n",
      "Epoch: : 163, Accuracy: 0.8357, NNL: 0.4713, ECE:0.0658\n",
      "Epoch: : 164, Accuracy: 0.8357, NNL: 0.4713, ECE:0.0658\n",
      "Epoch: : 165, Accuracy: 0.8357, NNL: 0.4714, ECE:0.0658\n",
      "Epoch: : 166, Accuracy: 0.8357, NNL: 0.4714, ECE:0.0658\n",
      "Epoch: : 167, Accuracy: 0.8357, NNL: 0.4714, ECE:0.0658\n",
      "Epoch: : 168, Accuracy: 0.8357, NNL: 0.4714, ECE:0.0658\n",
      "Epoch: : 169, Accuracy: 0.8357, NNL: 0.4714, ECE:0.0658\n",
      "Epoch: : 170, Accuracy: 0.8357, NNL: 0.4714, ECE:0.0658\n",
      "Epoch: : 171, Accuracy: 0.8357, NNL: 0.4714, ECE:0.0658\n",
      "Epoch: : 172, Accuracy: 0.8357, NNL: 0.4714, ECE:0.0658\n",
      "Epoch: : 173, Accuracy: 0.8357, NNL: 0.4714, ECE:0.0658\n",
      "Epoch: : 174, Accuracy: 0.8357, NNL: 0.4714, ECE:0.0658\n",
      "Epoch: : 175, Accuracy: 0.8357, NNL: 0.4714, ECE:0.0658\n",
      "Epoch: : 176, Accuracy: 0.8357, NNL: 0.4714, ECE:0.0658\n",
      "Epoch: : 177, Accuracy: 0.8357, NNL: 0.4714, ECE:0.0658\n",
      "Epoch: : 178, Accuracy: 0.8357, NNL: 0.4714, ECE:0.0658\n",
      "Epoch: : 179, Accuracy: 0.8357, NNL: 0.4714, ECE:0.0658\n",
      "Epoch: : 180, Accuracy: 0.8357, NNL: 0.4714, ECE:0.0658\n",
      "run 6 TS tempreture:  0.9654847383499146\n",
      "Epoch: : 001, Accuracy: 0.9000, NNL: 0.5270, ECE:0.0746\n",
      "Epoch: : 002, Accuracy: 0.9000, NNL: 0.5245, ECE:0.0746\n",
      "Epoch: : 003, Accuracy: 0.9000, NNL: 0.5220, ECE:0.0746\n",
      "Epoch: : 004, Accuracy: 0.9000, NNL: 0.5195, ECE:0.0746\n",
      "Epoch: : 005, Accuracy: 0.9000, NNL: 0.5170, ECE:0.0746\n",
      "Epoch: : 006, Accuracy: 0.9000, NNL: 0.5145, ECE:0.0746\n",
      "Epoch: : 007, Accuracy: 0.9000, NNL: 0.5120, ECE:0.0746\n",
      "Epoch: : 008, Accuracy: 0.9000, NNL: 0.5095, ECE:0.0746\n",
      "Epoch: : 009, Accuracy: 0.9000, NNL: 0.5070, ECE:0.0746\n",
      "Epoch: : 010, Accuracy: 0.9000, NNL: 0.5045, ECE:0.0746\n",
      "Epoch: : 011, Accuracy: 0.9000, NNL: 0.5021, ECE:0.0746\n",
      "Epoch: : 012, Accuracy: 0.9000, NNL: 0.4996, ECE:0.0746\n",
      "Epoch: : 013, Accuracy: 0.9000, NNL: 0.4971, ECE:0.0746\n",
      "Epoch: : 014, Accuracy: 0.9000, NNL: 0.4947, ECE:0.0746\n",
      "Epoch: : 015, Accuracy: 0.9000, NNL: 0.4923, ECE:0.0746\n",
      "Epoch: : 016, Accuracy: 0.9000, NNL: 0.4899, ECE:0.0746\n",
      "Epoch: : 017, Accuracy: 0.9000, NNL: 0.4875, ECE:0.0746\n",
      "Epoch: : 018, Accuracy: 0.9000, NNL: 0.4851, ECE:0.0746\n",
      "Epoch: : 019, Accuracy: 0.9000, NNL: 0.4827, ECE:0.0746\n",
      "Epoch: : 020, Accuracy: 0.9000, NNL: 0.4804, ECE:0.0746\n",
      "Epoch: : 021, Accuracy: 0.9000, NNL: 0.4780, ECE:0.0746\n",
      "Epoch: : 022, Accuracy: 0.9000, NNL: 0.4757, ECE:0.0746\n",
      "Epoch: : 023, Accuracy: 0.9000, NNL: 0.4734, ECE:0.0746\n",
      "Epoch: : 024, Accuracy: 0.9000, NNL: 0.4712, ECE:0.0746\n",
      "Epoch: : 025, Accuracy: 0.9000, NNL: 0.4689, ECE:0.0746\n",
      "Epoch: : 026, Accuracy: 0.9000, NNL: 0.4667, ECE:0.0746\n",
      "Epoch: : 027, Accuracy: 0.9000, NNL: 0.4646, ECE:0.0746\n",
      "Epoch: : 028, Accuracy: 0.9000, NNL: 0.4624, ECE:0.0746\n",
      "Epoch: : 029, Accuracy: 0.9000, NNL: 0.4603, ECE:0.0746\n",
      "Epoch: : 030, Accuracy: 0.9000, NNL: 0.4582, ECE:0.0746\n",
      "Epoch: : 031, Accuracy: 0.9000, NNL: 0.4562, ECE:0.0746\n",
      "Epoch: : 032, Accuracy: 0.9000, NNL: 0.4541, ECE:0.0746\n",
      "Epoch: : 033, Accuracy: 0.9000, NNL: 0.4522, ECE:0.0746\n",
      "Epoch: : 034, Accuracy: 0.9000, NNL: 0.4502, ECE:0.0746\n",
      "Epoch: : 035, Accuracy: 0.9000, NNL: 0.4484, ECE:0.0746\n",
      "Epoch: : 036, Accuracy: 0.9000, NNL: 0.4465, ECE:0.0746\n",
      "Epoch: : 037, Accuracy: 0.9000, NNL: 0.4447, ECE:0.0746\n",
      "Epoch: : 038, Accuracy: 0.9000, NNL: 0.4430, ECE:0.0746\n",
      "Epoch: : 039, Accuracy: 0.9000, NNL: 0.4413, ECE:0.0746\n",
      "Epoch: : 040, Accuracy: 0.9000, NNL: 0.4396, ECE:0.0746\n",
      "Epoch: : 041, Accuracy: 0.9000, NNL: 0.4380, ECE:0.0746\n",
      "Epoch: : 042, Accuracy: 0.9000, NNL: 0.4365, ECE:0.0746\n",
      "Epoch: : 043, Accuracy: 0.9000, NNL: 0.4350, ECE:0.0746\n",
      "Epoch: : 044, Accuracy: 0.9000, NNL: 0.4336, ECE:0.0746\n",
      "Epoch: : 045, Accuracy: 0.9000, NNL: 0.4322, ECE:0.0746\n",
      "Epoch: : 046, Accuracy: 0.9000, NNL: 0.4309, ECE:0.0746\n",
      "Epoch: : 047, Accuracy: 0.9000, NNL: 0.4297, ECE:0.0746\n",
      "Epoch: : 048, Accuracy: 0.9000, NNL: 0.4285, ECE:0.0746\n",
      "Epoch: : 049, Accuracy: 0.9000, NNL: 0.4274, ECE:0.0746\n",
      "Epoch: : 050, Accuracy: 0.9000, NNL: 0.4264, ECE:0.0746\n",
      "Epoch: : 051, Accuracy: 0.9000, NNL: 0.4254, ECE:0.0746\n",
      "Epoch: : 052, Accuracy: 0.9000, NNL: 0.4244, ECE:0.0746\n",
      "Epoch: : 053, Accuracy: 0.9000, NNL: 0.4236, ECE:0.0746\n",
      "Epoch: : 054, Accuracy: 0.9000, NNL: 0.4228, ECE:0.0746\n",
      "Epoch: : 055, Accuracy: 0.9000, NNL: 0.4220, ECE:0.0746\n",
      "Epoch: : 056, Accuracy: 0.9000, NNL: 0.4214, ECE:0.0746\n",
      "Epoch: : 057, Accuracy: 0.9000, NNL: 0.4208, ECE:0.0746\n",
      "Epoch: : 058, Accuracy: 0.9000, NNL: 0.4202, ECE:0.0746\n",
      "Epoch: : 059, Accuracy: 0.9000, NNL: 0.4197, ECE:0.0746\n",
      "Epoch: : 060, Accuracy: 0.9000, NNL: 0.4193, ECE:0.0746\n",
      "Epoch: : 061, Accuracy: 0.9000, NNL: 0.4189, ECE:0.0746\n",
      "Epoch: : 062, Accuracy: 0.9000, NNL: 0.4185, ECE:0.0746\n",
      "Epoch: : 063, Accuracy: 0.9000, NNL: 0.4183, ECE:0.0746\n",
      "Epoch: : 064, Accuracy: 0.9000, NNL: 0.4180, ECE:0.0746\n",
      "Epoch: : 065, Accuracy: 0.9000, NNL: 0.4178, ECE:0.0746\n",
      "Epoch: : 066, Accuracy: 0.9000, NNL: 0.4177, ECE:0.0746\n",
      "Epoch: : 067, Accuracy: 0.9000, NNL: 0.4176, ECE:0.0746\n",
      "Epoch: : 068, Accuracy: 0.9000, NNL: 0.4175, ECE:0.0746\n",
      "Epoch: : 069, Accuracy: 0.9000, NNL: 0.4175, ECE:0.0746\n",
      "Epoch: : 070, Accuracy: 0.9000, NNL: 0.4175, ECE:0.0746\n",
      "Epoch: : 071, Accuracy: 0.9000, NNL: 0.4175, ECE:0.0746\n",
      "Epoch: : 072, Accuracy: 0.9000, NNL: 0.4175, ECE:0.0746\n",
      "Epoch: : 073, Accuracy: 0.9000, NNL: 0.4176, ECE:0.0746\n",
      "Epoch: : 074, Accuracy: 0.9000, NNL: 0.4177, ECE:0.0746\n",
      "Epoch: : 075, Accuracy: 0.9000, NNL: 0.4178, ECE:0.0746\n",
      "Epoch: : 076, Accuracy: 0.9000, NNL: 0.4180, ECE:0.0746\n",
      "Epoch: : 077, Accuracy: 0.9000, NNL: 0.4181, ECE:0.0746\n",
      "Epoch: : 078, Accuracy: 0.9000, NNL: 0.4183, ECE:0.0746\n",
      "Epoch: : 079, Accuracy: 0.9000, NNL: 0.4184, ECE:0.0746\n",
      "Epoch: : 080, Accuracy: 0.9000, NNL: 0.4186, ECE:0.0746\n",
      "Epoch: : 081, Accuracy: 0.9000, NNL: 0.4188, ECE:0.0746\n",
      "Epoch: : 082, Accuracy: 0.9000, NNL: 0.4190, ECE:0.0746\n",
      "Epoch: : 083, Accuracy: 0.9000, NNL: 0.4191, ECE:0.0746\n",
      "Epoch: : 084, Accuracy: 0.9000, NNL: 0.4193, ECE:0.0746\n",
      "Epoch: : 085, Accuracy: 0.9000, NNL: 0.4195, ECE:0.0746\n",
      "Epoch: : 086, Accuracy: 0.9000, NNL: 0.4197, ECE:0.0746\n",
      "Epoch: : 087, Accuracy: 0.9000, NNL: 0.4198, ECE:0.0746\n",
      "Epoch: : 088, Accuracy: 0.9000, NNL: 0.4200, ECE:0.0746\n",
      "Epoch: : 089, Accuracy: 0.9000, NNL: 0.4202, ECE:0.0746\n",
      "Epoch: : 090, Accuracy: 0.9000, NNL: 0.4203, ECE:0.0746\n",
      "Epoch: : 091, Accuracy: 0.9000, NNL: 0.4205, ECE:0.0746\n",
      "Epoch: : 092, Accuracy: 0.9000, NNL: 0.4206, ECE:0.0746\n",
      "Epoch: : 093, Accuracy: 0.9000, NNL: 0.4207, ECE:0.0746\n",
      "Epoch: : 094, Accuracy: 0.9000, NNL: 0.4208, ECE:0.0746\n",
      "Epoch: : 095, Accuracy: 0.9000, NNL: 0.4210, ECE:0.0746\n",
      "Epoch: : 096, Accuracy: 0.9000, NNL: 0.4211, ECE:0.0746\n",
      "Epoch: : 097, Accuracy: 0.9000, NNL: 0.4211, ECE:0.0746\n",
      "Epoch: : 098, Accuracy: 0.9000, NNL: 0.4212, ECE:0.0746\n",
      "Epoch: : 099, Accuracy: 0.9000, NNL: 0.4213, ECE:0.0746\n",
      "Epoch: : 100, Accuracy: 0.9000, NNL: 0.4214, ECE:0.0746\n",
      "Epoch: : 101, Accuracy: 0.9000, NNL: 0.4214, ECE:0.0746\n",
      "Epoch: : 102, Accuracy: 0.9000, NNL: 0.4214, ECE:0.0746\n",
      "Epoch: : 103, Accuracy: 0.9000, NNL: 0.4215, ECE:0.0746\n",
      "Epoch: : 104, Accuracy: 0.9000, NNL: 0.4215, ECE:0.0746\n",
      "Epoch: : 105, Accuracy: 0.9000, NNL: 0.4215, ECE:0.0746\n",
      "Epoch: : 106, Accuracy: 0.9000, NNL: 0.4215, ECE:0.0746\n",
      "Epoch: : 107, Accuracy: 0.9000, NNL: 0.4216, ECE:0.0746\n",
      "Epoch: : 108, Accuracy: 0.9000, NNL: 0.4216, ECE:0.0746\n",
      "Epoch: : 109, Accuracy: 0.9000, NNL: 0.4215, ECE:0.0746\n",
      "Epoch: : 110, Accuracy: 0.9000, NNL: 0.4215, ECE:0.0746\n",
      "Epoch: : 111, Accuracy: 0.9000, NNL: 0.4215, ECE:0.0746\n",
      "Epoch: : 112, Accuracy: 0.9000, NNL: 0.4215, ECE:0.0746\n",
      "Epoch: : 113, Accuracy: 0.9000, NNL: 0.4215, ECE:0.0746\n",
      "Epoch: : 114, Accuracy: 0.9000, NNL: 0.4215, ECE:0.0746\n",
      "Epoch: : 115, Accuracy: 0.9000, NNL: 0.4214, ECE:0.0746\n",
      "Epoch: : 116, Accuracy: 0.9000, NNL: 0.4214, ECE:0.0746\n",
      "Epoch: : 117, Accuracy: 0.9000, NNL: 0.4214, ECE:0.0746\n",
      "Epoch: : 118, Accuracy: 0.9000, NNL: 0.4214, ECE:0.0746\n",
      "Epoch: : 119, Accuracy: 0.9000, NNL: 0.4213, ECE:0.0746\n",
      "Epoch: : 120, Accuracy: 0.9000, NNL: 0.4213, ECE:0.0746\n",
      "Epoch: : 121, Accuracy: 0.9000, NNL: 0.4213, ECE:0.0746\n",
      "Epoch: : 122, Accuracy: 0.9000, NNL: 0.4212, ECE:0.0746\n",
      "Epoch: : 123, Accuracy: 0.9000, NNL: 0.4212, ECE:0.0746\n",
      "Epoch: : 124, Accuracy: 0.9000, NNL: 0.4212, ECE:0.0746\n",
      "Epoch: : 125, Accuracy: 0.9000, NNL: 0.4212, ECE:0.0746\n",
      "Epoch: : 126, Accuracy: 0.9000, NNL: 0.4211, ECE:0.0746\n",
      "Epoch: : 127, Accuracy: 0.9000, NNL: 0.4211, ECE:0.0746\n",
      "Epoch: : 128, Accuracy: 0.9000, NNL: 0.4211, ECE:0.0746\n",
      "Epoch: : 129, Accuracy: 0.9000, NNL: 0.4211, ECE:0.0746\n",
      "Epoch: : 130, Accuracy: 0.9000, NNL: 0.4210, ECE:0.0746\n",
      "Epoch: : 131, Accuracy: 0.9000, NNL: 0.4210, ECE:0.0746\n",
      "Epoch: : 132, Accuracy: 0.9000, NNL: 0.4210, ECE:0.0746\n",
      "Epoch: : 133, Accuracy: 0.9000, NNL: 0.4210, ECE:0.0746\n",
      "Epoch: : 134, Accuracy: 0.9000, NNL: 0.4210, ECE:0.0746\n",
      "Epoch: : 135, Accuracy: 0.9000, NNL: 0.4210, ECE:0.0746\n",
      "Epoch: : 136, Accuracy: 0.9000, NNL: 0.4210, ECE:0.0746\n",
      "Epoch: : 137, Accuracy: 0.9000, NNL: 0.4210, ECE:0.0746\n",
      "Epoch: : 138, Accuracy: 0.9000, NNL: 0.4209, ECE:0.0746\n",
      "Epoch: : 139, Accuracy: 0.9000, NNL: 0.4209, ECE:0.0746\n",
      "Epoch: : 140, Accuracy: 0.9000, NNL: 0.4209, ECE:0.0746\n",
      "Epoch: : 141, Accuracy: 0.9000, NNL: 0.4209, ECE:0.0746\n",
      "Epoch: : 142, Accuracy: 0.9000, NNL: 0.4209, ECE:0.0746\n",
      "Epoch: : 143, Accuracy: 0.9000, NNL: 0.4209, ECE:0.0746\n",
      "Epoch: : 144, Accuracy: 0.9000, NNL: 0.4209, ECE:0.0746\n",
      "Epoch: : 145, Accuracy: 0.9000, NNL: 0.4209, ECE:0.0746\n",
      "Epoch: : 146, Accuracy: 0.9000, NNL: 0.4209, ECE:0.0746\n",
      "Epoch: : 147, Accuracy: 0.9000, NNL: 0.4210, ECE:0.0746\n",
      "Epoch: : 148, Accuracy: 0.9000, NNL: 0.4210, ECE:0.0746\n",
      "Epoch: : 149, Accuracy: 0.9000, NNL: 0.4210, ECE:0.0746\n",
      "Epoch: : 150, Accuracy: 0.9000, NNL: 0.4210, ECE:0.0746\n",
      "Epoch: : 151, Accuracy: 0.9000, NNL: 0.4210, ECE:0.0746\n",
      "Epoch: : 152, Accuracy: 0.9000, NNL: 0.4210, ECE:0.0746\n",
      "Epoch: : 153, Accuracy: 0.9000, NNL: 0.4210, ECE:0.0746\n",
      "Epoch: : 154, Accuracy: 0.9000, NNL: 0.4210, ECE:0.0746\n",
      "Epoch: : 155, Accuracy: 0.9000, NNL: 0.4210, ECE:0.0746\n",
      "Epoch: : 156, Accuracy: 0.9000, NNL: 0.4210, ECE:0.0746\n",
      "Epoch: : 157, Accuracy: 0.9000, NNL: 0.4210, ECE:0.0746\n",
      "Epoch: : 158, Accuracy: 0.9000, NNL: 0.4210, ECE:0.0746\n",
      "Epoch: : 159, Accuracy: 0.9000, NNL: 0.4210, ECE:0.0746\n",
      "Epoch: : 160, Accuracy: 0.9000, NNL: 0.4210, ECE:0.0746\n",
      "Epoch: : 161, Accuracy: 0.9000, NNL: 0.4210, ECE:0.0746\n",
      "Epoch: : 162, Accuracy: 0.9000, NNL: 0.4210, ECE:0.0746\n",
      "Epoch: : 163, Accuracy: 0.9000, NNL: 0.4210, ECE:0.0746\n",
      "Epoch: : 164, Accuracy: 0.9000, NNL: 0.4210, ECE:0.0746\n",
      "Epoch: : 165, Accuracy: 0.9000, NNL: 0.4210, ECE:0.0746\n",
      "Epoch: : 166, Accuracy: 0.9000, NNL: 0.4210, ECE:0.0746\n",
      "Epoch: : 167, Accuracy: 0.9000, NNL: 0.4210, ECE:0.0746\n",
      "Epoch: : 168, Accuracy: 0.9000, NNL: 0.4210, ECE:0.0746\n",
      "Epoch: : 169, Accuracy: 0.9000, NNL: 0.4210, ECE:0.0746\n",
      "run 7 TS tempreture:  1.041698932647705\n",
      "Epoch: : 001, Accuracy: 0.9214, NNL: 0.4482, ECE:0.0626\n",
      "Epoch: : 002, Accuracy: 0.9214, NNL: 0.4458, ECE:0.0626\n",
      "Epoch: : 003, Accuracy: 0.9214, NNL: 0.4435, ECE:0.0626\n",
      "Epoch: : 004, Accuracy: 0.9214, NNL: 0.4411, ECE:0.0626\n",
      "Epoch: : 005, Accuracy: 0.9214, NNL: 0.4387, ECE:0.0626\n",
      "Epoch: : 006, Accuracy: 0.9214, NNL: 0.4364, ECE:0.0626\n",
      "Epoch: : 007, Accuracy: 0.9214, NNL: 0.4340, ECE:0.0626\n",
      "Epoch: : 008, Accuracy: 0.9214, NNL: 0.4316, ECE:0.0626\n",
      "Epoch: : 009, Accuracy: 0.9214, NNL: 0.4293, ECE:0.0626\n",
      "Epoch: : 010, Accuracy: 0.9214, NNL: 0.4269, ECE:0.0626\n",
      "Epoch: : 011, Accuracy: 0.9214, NNL: 0.4246, ECE:0.0626\n",
      "Epoch: : 012, Accuracy: 0.9214, NNL: 0.4223, ECE:0.0626\n",
      "Epoch: : 013, Accuracy: 0.9214, NNL: 0.4199, ECE:0.0626\n",
      "Epoch: : 014, Accuracy: 0.9214, NNL: 0.4176, ECE:0.0626\n",
      "Epoch: : 015, Accuracy: 0.9214, NNL: 0.4153, ECE:0.0626\n",
      "Epoch: : 016, Accuracy: 0.9214, NNL: 0.4130, ECE:0.0626\n",
      "Epoch: : 017, Accuracy: 0.9214, NNL: 0.4107, ECE:0.0626\n",
      "Epoch: : 018, Accuracy: 0.9214, NNL: 0.4084, ECE:0.0626\n",
      "Epoch: : 019, Accuracy: 0.9214, NNL: 0.4062, ECE:0.0626\n",
      "Epoch: : 020, Accuracy: 0.9214, NNL: 0.4039, ECE:0.0626\n",
      "Epoch: : 021, Accuracy: 0.9214, NNL: 0.4017, ECE:0.0626\n",
      "Epoch: : 022, Accuracy: 0.9214, NNL: 0.3995, ECE:0.0626\n",
      "Epoch: : 023, Accuracy: 0.9214, NNL: 0.3973, ECE:0.0626\n",
      "Epoch: : 024, Accuracy: 0.9214, NNL: 0.3951, ECE:0.0626\n",
      "Epoch: : 025, Accuracy: 0.9214, NNL: 0.3929, ECE:0.0626\n",
      "Epoch: : 026, Accuracy: 0.9214, NNL: 0.3908, ECE:0.0626\n",
      "Epoch: : 027, Accuracy: 0.9214, NNL: 0.3887, ECE:0.0626\n",
      "Epoch: : 028, Accuracy: 0.9214, NNL: 0.3866, ECE:0.0626\n",
      "Epoch: : 029, Accuracy: 0.9214, NNL: 0.3845, ECE:0.0626\n",
      "Epoch: : 030, Accuracy: 0.9214, NNL: 0.3825, ECE:0.0626\n",
      "Epoch: : 031, Accuracy: 0.9214, NNL: 0.3805, ECE:0.0626\n",
      "Epoch: : 032, Accuracy: 0.9214, NNL: 0.3785, ECE:0.0626\n",
      "Epoch: : 033, Accuracy: 0.9214, NNL: 0.3765, ECE:0.0626\n",
      "Epoch: : 034, Accuracy: 0.9214, NNL: 0.3746, ECE:0.0626\n",
      "Epoch: : 035, Accuracy: 0.9214, NNL: 0.3727, ECE:0.0626\n",
      "Epoch: : 036, Accuracy: 0.9214, NNL: 0.3709, ECE:0.0626\n",
      "Epoch: : 037, Accuracy: 0.9214, NNL: 0.3691, ECE:0.0626\n",
      "Epoch: : 038, Accuracy: 0.9214, NNL: 0.3673, ECE:0.0626\n",
      "Epoch: : 039, Accuracy: 0.9214, NNL: 0.3656, ECE:0.0626\n",
      "Epoch: : 040, Accuracy: 0.9214, NNL: 0.3639, ECE:0.0626\n",
      "Epoch: : 041, Accuracy: 0.9214, NNL: 0.3622, ECE:0.0626\n",
      "Epoch: : 042, Accuracy: 0.9214, NNL: 0.3606, ECE:0.0626\n",
      "Epoch: : 043, Accuracy: 0.9214, NNL: 0.3590, ECE:0.0626\n",
      "Epoch: : 044, Accuracy: 0.9214, NNL: 0.3575, ECE:0.0626\n",
      "Epoch: : 045, Accuracy: 0.9214, NNL: 0.3561, ECE:0.0626\n",
      "Epoch: : 046, Accuracy: 0.9214, NNL: 0.3546, ECE:0.0626\n",
      "Epoch: : 047, Accuracy: 0.9214, NNL: 0.3533, ECE:0.0626\n",
      "Epoch: : 048, Accuracy: 0.9214, NNL: 0.3520, ECE:0.0626\n",
      "Epoch: : 049, Accuracy: 0.9214, NNL: 0.3507, ECE:0.0626\n",
      "Epoch: : 050, Accuracy: 0.9214, NNL: 0.3495, ECE:0.0626\n",
      "Epoch: : 051, Accuracy: 0.9214, NNL: 0.3483, ECE:0.0626\n",
      "Epoch: : 052, Accuracy: 0.9214, NNL: 0.3472, ECE:0.0626\n",
      "Epoch: : 053, Accuracy: 0.9214, NNL: 0.3462, ECE:0.0626\n",
      "Epoch: : 054, Accuracy: 0.9214, NNL: 0.3452, ECE:0.0626\n",
      "Epoch: : 055, Accuracy: 0.9214, NNL: 0.3443, ECE:0.0626\n",
      "Epoch: : 056, Accuracy: 0.9214, NNL: 0.3434, ECE:0.0626\n",
      "Epoch: : 057, Accuracy: 0.9214, NNL: 0.3425, ECE:0.0626\n",
      "Epoch: : 058, Accuracy: 0.9214, NNL: 0.3418, ECE:0.0626\n",
      "Epoch: : 059, Accuracy: 0.9214, NNL: 0.3411, ECE:0.0626\n",
      "Epoch: : 060, Accuracy: 0.9214, NNL: 0.3404, ECE:0.0626\n",
      "Epoch: : 061, Accuracy: 0.9214, NNL: 0.3398, ECE:0.0626\n",
      "Epoch: : 062, Accuracy: 0.9214, NNL: 0.3392, ECE:0.0626\n",
      "Epoch: : 063, Accuracy: 0.9214, NNL: 0.3387, ECE:0.0626\n",
      "Epoch: : 064, Accuracy: 0.9214, NNL: 0.3383, ECE:0.0626\n",
      "Epoch: : 065, Accuracy: 0.9214, NNL: 0.3379, ECE:0.0626\n",
      "Epoch: : 066, Accuracy: 0.9214, NNL: 0.3375, ECE:0.0626\n",
      "Epoch: : 067, Accuracy: 0.9214, NNL: 0.3372, ECE:0.0626\n",
      "Epoch: : 068, Accuracy: 0.9214, NNL: 0.3369, ECE:0.0626\n",
      "Epoch: : 069, Accuracy: 0.9214, NNL: 0.3367, ECE:0.0626\n",
      "Epoch: : 070, Accuracy: 0.9214, NNL: 0.3365, ECE:0.0626\n",
      "Epoch: : 071, Accuracy: 0.9214, NNL: 0.3363, ECE:0.0626\n",
      "Epoch: : 072, Accuracy: 0.9214, NNL: 0.3362, ECE:0.0626\n",
      "Epoch: : 073, Accuracy: 0.9214, NNL: 0.3361, ECE:0.0626\n",
      "Epoch: : 074, Accuracy: 0.9214, NNL: 0.3361, ECE:0.0626\n",
      "Epoch: : 075, Accuracy: 0.9214, NNL: 0.3360, ECE:0.0626\n",
      "Epoch: : 076, Accuracy: 0.9214, NNL: 0.3360, ECE:0.0626\n",
      "Epoch: : 077, Accuracy: 0.9214, NNL: 0.3360, ECE:0.0626\n",
      "Epoch: : 078, Accuracy: 0.9214, NNL: 0.3361, ECE:0.0626\n",
      "Epoch: : 079, Accuracy: 0.9214, NNL: 0.3361, ECE:0.0626\n",
      "Epoch: : 080, Accuracy: 0.9214, NNL: 0.3362, ECE:0.0626\n",
      "Epoch: : 081, Accuracy: 0.9214, NNL: 0.3363, ECE:0.0626\n",
      "Epoch: : 082, Accuracy: 0.9214, NNL: 0.3364, ECE:0.0626\n",
      "Epoch: : 083, Accuracy: 0.9214, NNL: 0.3365, ECE:0.0626\n",
      "Epoch: : 084, Accuracy: 0.9214, NNL: 0.3367, ECE:0.0626\n",
      "Epoch: : 085, Accuracy: 0.9214, NNL: 0.3368, ECE:0.0626\n",
      "Epoch: : 086, Accuracy: 0.9214, NNL: 0.3369, ECE:0.0626\n",
      "Epoch: : 087, Accuracy: 0.9214, NNL: 0.3371, ECE:0.0626\n",
      "Epoch: : 088, Accuracy: 0.9214, NNL: 0.3372, ECE:0.0626\n",
      "Epoch: : 089, Accuracy: 0.9214, NNL: 0.3374, ECE:0.0626\n",
      "Epoch: : 090, Accuracy: 0.9214, NNL: 0.3375, ECE:0.0626\n",
      "Epoch: : 091, Accuracy: 0.9214, NNL: 0.3377, ECE:0.0626\n",
      "Epoch: : 092, Accuracy: 0.9214, NNL: 0.3378, ECE:0.0626\n",
      "Epoch: : 093, Accuracy: 0.9214, NNL: 0.3379, ECE:0.0626\n",
      "Epoch: : 094, Accuracy: 0.9214, NNL: 0.3381, ECE:0.0626\n",
      "Epoch: : 095, Accuracy: 0.9214, NNL: 0.3382, ECE:0.0626\n",
      "Epoch: : 096, Accuracy: 0.9214, NNL: 0.3383, ECE:0.0626\n",
      "Epoch: : 097, Accuracy: 0.9214, NNL: 0.3385, ECE:0.0626\n",
      "Epoch: : 098, Accuracy: 0.9214, NNL: 0.3386, ECE:0.0626\n",
      "Epoch: : 099, Accuracy: 0.9214, NNL: 0.3387, ECE:0.0626\n",
      "Epoch: : 100, Accuracy: 0.9214, NNL: 0.3388, ECE:0.0626\n",
      "Epoch: : 101, Accuracy: 0.9214, NNL: 0.3389, ECE:0.0626\n",
      "Epoch: : 102, Accuracy: 0.9214, NNL: 0.3390, ECE:0.0626\n",
      "Epoch: : 103, Accuracy: 0.9214, NNL: 0.3390, ECE:0.0626\n",
      "Epoch: : 104, Accuracy: 0.9214, NNL: 0.3391, ECE:0.0626\n",
      "Epoch: : 105, Accuracy: 0.9214, NNL: 0.3392, ECE:0.0626\n",
      "Epoch: : 106, Accuracy: 0.9214, NNL: 0.3392, ECE:0.0626\n",
      "Epoch: : 107, Accuracy: 0.9214, NNL: 0.3393, ECE:0.0626\n",
      "Epoch: : 108, Accuracy: 0.9214, NNL: 0.3393, ECE:0.0626\n",
      "Epoch: : 109, Accuracy: 0.9214, NNL: 0.3394, ECE:0.0626\n",
      "Epoch: : 110, Accuracy: 0.9214, NNL: 0.3394, ECE:0.0626\n",
      "Epoch: : 111, Accuracy: 0.9214, NNL: 0.3394, ECE:0.0626\n",
      "Epoch: : 112, Accuracy: 0.9214, NNL: 0.3394, ECE:0.0626\n",
      "Epoch: : 113, Accuracy: 0.9214, NNL: 0.3394, ECE:0.0626\n",
      "Epoch: : 114, Accuracy: 0.9214, NNL: 0.3395, ECE:0.0626\n",
      "Epoch: : 115, Accuracy: 0.9214, NNL: 0.3395, ECE:0.0626\n",
      "Epoch: : 116, Accuracy: 0.9214, NNL: 0.3395, ECE:0.0626\n",
      "Epoch: : 117, Accuracy: 0.9214, NNL: 0.3395, ECE:0.0626\n",
      "Epoch: : 118, Accuracy: 0.9214, NNL: 0.3394, ECE:0.0626\n",
      "Epoch: : 119, Accuracy: 0.9214, NNL: 0.3394, ECE:0.0626\n",
      "Epoch: : 120, Accuracy: 0.9214, NNL: 0.3394, ECE:0.0626\n",
      "Epoch: : 121, Accuracy: 0.9214, NNL: 0.3394, ECE:0.0626\n",
      "Epoch: : 122, Accuracy: 0.9214, NNL: 0.3394, ECE:0.0626\n",
      "Epoch: : 123, Accuracy: 0.9214, NNL: 0.3394, ECE:0.0626\n",
      "Epoch: : 124, Accuracy: 0.9214, NNL: 0.3394, ECE:0.0626\n",
      "Epoch: : 125, Accuracy: 0.9214, NNL: 0.3393, ECE:0.0626\n",
      "Epoch: : 126, Accuracy: 0.9214, NNL: 0.3393, ECE:0.0626\n",
      "Epoch: : 127, Accuracy: 0.9214, NNL: 0.3393, ECE:0.0626\n",
      "Epoch: : 128, Accuracy: 0.9214, NNL: 0.3393, ECE:0.0626\n",
      "Epoch: : 129, Accuracy: 0.9214, NNL: 0.3392, ECE:0.0626\n",
      "Epoch: : 130, Accuracy: 0.9214, NNL: 0.3392, ECE:0.0626\n",
      "Epoch: : 131, Accuracy: 0.9214, NNL: 0.3392, ECE:0.0626\n",
      "Epoch: : 132, Accuracy: 0.9214, NNL: 0.3392, ECE:0.0626\n",
      "Epoch: : 133, Accuracy: 0.9214, NNL: 0.3392, ECE:0.0626\n",
      "Epoch: : 134, Accuracy: 0.9214, NNL: 0.3391, ECE:0.0626\n",
      "Epoch: : 135, Accuracy: 0.9214, NNL: 0.3391, ECE:0.0626\n",
      "Epoch: : 136, Accuracy: 0.9214, NNL: 0.3391, ECE:0.0626\n",
      "Epoch: : 137, Accuracy: 0.9214, NNL: 0.3391, ECE:0.0626\n",
      "Epoch: : 138, Accuracy: 0.9214, NNL: 0.3391, ECE:0.0626\n",
      "Epoch: : 139, Accuracy: 0.9214, NNL: 0.3391, ECE:0.0626\n",
      "Epoch: : 140, Accuracy: 0.9214, NNL: 0.3390, ECE:0.0626\n",
      "Epoch: : 141, Accuracy: 0.9214, NNL: 0.3390, ECE:0.0626\n",
      "Epoch: : 142, Accuracy: 0.9214, NNL: 0.3390, ECE:0.0626\n",
      "Epoch: : 143, Accuracy: 0.9214, NNL: 0.3390, ECE:0.0626\n",
      "Epoch: : 144, Accuracy: 0.9214, NNL: 0.3390, ECE:0.0626\n",
      "Epoch: : 145, Accuracy: 0.9214, NNL: 0.3390, ECE:0.0626\n",
      "Epoch: : 146, Accuracy: 0.9214, NNL: 0.3390, ECE:0.0626\n",
      "Epoch: : 147, Accuracy: 0.9214, NNL: 0.3390, ECE:0.0626\n",
      "Epoch: : 148, Accuracy: 0.9214, NNL: 0.3390, ECE:0.0626\n",
      "Epoch: : 149, Accuracy: 0.9214, NNL: 0.3390, ECE:0.0626\n",
      "Epoch: : 150, Accuracy: 0.9214, NNL: 0.3390, ECE:0.0626\n",
      "Epoch: : 151, Accuracy: 0.9214, NNL: 0.3390, ECE:0.0626\n",
      "Epoch: : 152, Accuracy: 0.9214, NNL: 0.3390, ECE:0.0626\n",
      "Epoch: : 153, Accuracy: 0.9214, NNL: 0.3390, ECE:0.0626\n",
      "Epoch: : 154, Accuracy: 0.9214, NNL: 0.3390, ECE:0.0626\n",
      "Epoch: : 155, Accuracy: 0.9214, NNL: 0.3390, ECE:0.0626\n",
      "Epoch: : 156, Accuracy: 0.9214, NNL: 0.3390, ECE:0.0626\n",
      "Epoch: : 157, Accuracy: 0.9214, NNL: 0.3390, ECE:0.0626\n",
      "Epoch: : 158, Accuracy: 0.9214, NNL: 0.3390, ECE:0.0626\n",
      "Epoch: : 159, Accuracy: 0.9214, NNL: 0.3390, ECE:0.0626\n",
      "Epoch: : 160, Accuracy: 0.9214, NNL: 0.3390, ECE:0.0626\n",
      "Epoch: : 161, Accuracy: 0.9214, NNL: 0.3390, ECE:0.0626\n",
      "Epoch: : 162, Accuracy: 0.9214, NNL: 0.3390, ECE:0.0626\n",
      "Epoch: : 163, Accuracy: 0.9214, NNL: 0.3390, ECE:0.0626\n",
      "Epoch: : 164, Accuracy: 0.9214, NNL: 0.3390, ECE:0.0626\n",
      "Epoch: : 165, Accuracy: 0.9214, NNL: 0.3390, ECE:0.0626\n",
      "Epoch: : 166, Accuracy: 0.9214, NNL: 0.3390, ECE:0.0626\n",
      "Epoch: : 167, Accuracy: 0.9214, NNL: 0.3390, ECE:0.0626\n",
      "Epoch: : 168, Accuracy: 0.9214, NNL: 0.3390, ECE:0.0626\n",
      "Epoch: : 169, Accuracy: 0.9214, NNL: 0.3390, ECE:0.0626\n",
      "Epoch: : 170, Accuracy: 0.9214, NNL: 0.3390, ECE:0.0626\n",
      "Epoch: : 171, Accuracy: 0.9214, NNL: 0.3391, ECE:0.0626\n",
      "Epoch: : 172, Accuracy: 0.9214, NNL: 0.3391, ECE:0.0626\n",
      "Epoch: : 173, Accuracy: 0.9214, NNL: 0.3391, ECE:0.0626\n",
      "Epoch: : 174, Accuracy: 0.9214, NNL: 0.3391, ECE:0.0626\n",
      "Epoch: : 175, Accuracy: 0.9214, NNL: 0.3391, ECE:0.0626\n",
      "run 8 TS tempreture:  1.005325436592102\n",
      "Epoch: : 001, Accuracy: 0.8929, NNL: 0.5088, ECE:0.0849\n",
      "Epoch: : 002, Accuracy: 0.8929, NNL: 0.5062, ECE:0.0849\n",
      "Epoch: : 003, Accuracy: 0.8929, NNL: 0.5037, ECE:0.0849\n",
      "Epoch: : 004, Accuracy: 0.8929, NNL: 0.5012, ECE:0.0849\n",
      "Epoch: : 005, Accuracy: 0.8929, NNL: 0.4986, ECE:0.0849\n",
      "Epoch: : 006, Accuracy: 0.8929, NNL: 0.4961, ECE:0.0849\n",
      "Epoch: : 007, Accuracy: 0.8929, NNL: 0.4936, ECE:0.0849\n",
      "Epoch: : 008, Accuracy: 0.8929, NNL: 0.4911, ECE:0.0849\n",
      "Epoch: : 009, Accuracy: 0.8929, NNL: 0.4885, ECE:0.0849\n",
      "Epoch: : 010, Accuracy: 0.8929, NNL: 0.4860, ECE:0.0849\n",
      "Epoch: : 011, Accuracy: 0.8929, NNL: 0.4835, ECE:0.0849\n",
      "Epoch: : 012, Accuracy: 0.8929, NNL: 0.4810, ECE:0.0849\n",
      "Epoch: : 013, Accuracy: 0.8929, NNL: 0.4785, ECE:0.0849\n",
      "Epoch: : 014, Accuracy: 0.8929, NNL: 0.4760, ECE:0.0849\n",
      "Epoch: : 015, Accuracy: 0.8929, NNL: 0.4736, ECE:0.0849\n",
      "Epoch: : 016, Accuracy: 0.8929, NNL: 0.4711, ECE:0.0849\n",
      "Epoch: : 017, Accuracy: 0.8929, NNL: 0.4686, ECE:0.0849\n",
      "Epoch: : 018, Accuracy: 0.8929, NNL: 0.4662, ECE:0.0849\n",
      "Epoch: : 019, Accuracy: 0.8929, NNL: 0.4637, ECE:0.0849\n",
      "Epoch: : 020, Accuracy: 0.8929, NNL: 0.4613, ECE:0.0849\n",
      "Epoch: : 021, Accuracy: 0.8929, NNL: 0.4589, ECE:0.0849\n",
      "Epoch: : 022, Accuracy: 0.8929, NNL: 0.4565, ECE:0.0849\n",
      "Epoch: : 023, Accuracy: 0.8929, NNL: 0.4542, ECE:0.0849\n",
      "Epoch: : 024, Accuracy: 0.8929, NNL: 0.4518, ECE:0.0849\n",
      "Epoch: : 025, Accuracy: 0.8929, NNL: 0.4495, ECE:0.0849\n",
      "Epoch: : 026, Accuracy: 0.8929, NNL: 0.4472, ECE:0.0849\n",
      "Epoch: : 027, Accuracy: 0.8929, NNL: 0.4449, ECE:0.0849\n",
      "Epoch: : 028, Accuracy: 0.8929, NNL: 0.4426, ECE:0.0849\n",
      "Epoch: : 029, Accuracy: 0.8929, NNL: 0.4404, ECE:0.0849\n",
      "Epoch: : 030, Accuracy: 0.8929, NNL: 0.4382, ECE:0.0849\n",
      "Epoch: : 031, Accuracy: 0.8929, NNL: 0.4360, ECE:0.0849\n",
      "Epoch: : 032, Accuracy: 0.8929, NNL: 0.4338, ECE:0.0849\n",
      "Epoch: : 033, Accuracy: 0.8929, NNL: 0.4317, ECE:0.0849\n",
      "Epoch: : 034, Accuracy: 0.8929, NNL: 0.4296, ECE:0.0849\n",
      "Epoch: : 035, Accuracy: 0.8929, NNL: 0.4276, ECE:0.0849\n",
      "Epoch: : 036, Accuracy: 0.8929, NNL: 0.4256, ECE:0.0849\n",
      "Epoch: : 037, Accuracy: 0.8929, NNL: 0.4236, ECE:0.0849\n",
      "Epoch: : 038, Accuracy: 0.8929, NNL: 0.4217, ECE:0.0849\n",
      "Epoch: : 039, Accuracy: 0.8929, NNL: 0.4198, ECE:0.0849\n",
      "Epoch: : 040, Accuracy: 0.8929, NNL: 0.4179, ECE:0.0849\n",
      "Epoch: : 041, Accuracy: 0.8929, NNL: 0.4161, ECE:0.0849\n",
      "Epoch: : 042, Accuracy: 0.8929, NNL: 0.4144, ECE:0.0849\n",
      "Epoch: : 043, Accuracy: 0.8929, NNL: 0.4127, ECE:0.0849\n",
      "Epoch: : 044, Accuracy: 0.8929, NNL: 0.4110, ECE:0.0849\n",
      "Epoch: : 045, Accuracy: 0.8929, NNL: 0.4094, ECE:0.0849\n",
      "Epoch: : 046, Accuracy: 0.8929, NNL: 0.4079, ECE:0.0849\n",
      "Epoch: : 047, Accuracy: 0.8929, NNL: 0.4064, ECE:0.0849\n",
      "Epoch: : 048, Accuracy: 0.8929, NNL: 0.4049, ECE:0.0849\n",
      "Epoch: : 049, Accuracy: 0.8929, NNL: 0.4036, ECE:0.0849\n",
      "Epoch: : 050, Accuracy: 0.8929, NNL: 0.4022, ECE:0.0849\n",
      "Epoch: : 051, Accuracy: 0.8929, NNL: 0.4010, ECE:0.0849\n",
      "Epoch: : 052, Accuracy: 0.8929, NNL: 0.3998, ECE:0.0849\n",
      "Epoch: : 053, Accuracy: 0.8929, NNL: 0.3986, ECE:0.0849\n",
      "Epoch: : 054, Accuracy: 0.8929, NNL: 0.3975, ECE:0.0849\n",
      "Epoch: : 055, Accuracy: 0.8929, NNL: 0.3965, ECE:0.0849\n",
      "Epoch: : 056, Accuracy: 0.8929, NNL: 0.3956, ECE:0.0849\n",
      "Epoch: : 057, Accuracy: 0.8929, NNL: 0.3947, ECE:0.0849\n",
      "Epoch: : 058, Accuracy: 0.8929, NNL: 0.3938, ECE:0.0849\n",
      "Epoch: : 059, Accuracy: 0.8929, NNL: 0.3930, ECE:0.0849\n",
      "Epoch: : 060, Accuracy: 0.8929, NNL: 0.3923, ECE:0.0849\n",
      "Epoch: : 061, Accuracy: 0.8929, NNL: 0.3917, ECE:0.0849\n",
      "Epoch: : 062, Accuracy: 0.8929, NNL: 0.3910, ECE:0.0849\n",
      "Epoch: : 063, Accuracy: 0.8929, NNL: 0.3905, ECE:0.0849\n",
      "Epoch: : 064, Accuracy: 0.8929, NNL: 0.3900, ECE:0.0849\n",
      "Epoch: : 065, Accuracy: 0.8929, NNL: 0.3896, ECE:0.0849\n",
      "Epoch: : 066, Accuracy: 0.8929, NNL: 0.3892, ECE:0.0849\n",
      "Epoch: : 067, Accuracy: 0.8929, NNL: 0.3888, ECE:0.0849\n",
      "Epoch: : 068, Accuracy: 0.8929, NNL: 0.3885, ECE:0.0849\n",
      "Epoch: : 069, Accuracy: 0.8929, NNL: 0.3883, ECE:0.0849\n",
      "Epoch: : 070, Accuracy: 0.8929, NNL: 0.3881, ECE:0.0849\n",
      "Epoch: : 071, Accuracy: 0.8929, NNL: 0.3879, ECE:0.0849\n",
      "Epoch: : 072, Accuracy: 0.8929, NNL: 0.3878, ECE:0.0849\n",
      "Epoch: : 073, Accuracy: 0.8929, NNL: 0.3877, ECE:0.0849\n",
      "Epoch: : 074, Accuracy: 0.8929, NNL: 0.3877, ECE:0.0849\n",
      "Epoch: : 075, Accuracy: 0.8929, NNL: 0.3876, ECE:0.0849\n",
      "Epoch: : 076, Accuracy: 0.8929, NNL: 0.3876, ECE:0.0849\n",
      "Epoch: : 077, Accuracy: 0.8929, NNL: 0.3877, ECE:0.0849\n",
      "Epoch: : 078, Accuracy: 0.8929, NNL: 0.3877, ECE:0.0849\n",
      "Epoch: : 079, Accuracy: 0.8929, NNL: 0.3878, ECE:0.0849\n",
      "Epoch: : 080, Accuracy: 0.8929, NNL: 0.3879, ECE:0.0849\n",
      "Epoch: : 081, Accuracy: 0.8929, NNL: 0.3880, ECE:0.0849\n",
      "Epoch: : 082, Accuracy: 0.8929, NNL: 0.3881, ECE:0.0849\n",
      "Epoch: : 083, Accuracy: 0.8929, NNL: 0.3883, ECE:0.0849\n",
      "Epoch: : 084, Accuracy: 0.8929, NNL: 0.3884, ECE:0.0849\n",
      "Epoch: : 085, Accuracy: 0.8929, NNL: 0.3886, ECE:0.0849\n",
      "Epoch: : 086, Accuracy: 0.8929, NNL: 0.3887, ECE:0.0849\n",
      "Epoch: : 087, Accuracy: 0.8929, NNL: 0.3889, ECE:0.0849\n",
      "Epoch: : 088, Accuracy: 0.8929, NNL: 0.3890, ECE:0.0849\n",
      "Epoch: : 089, Accuracy: 0.8929, NNL: 0.3892, ECE:0.0849\n",
      "Epoch: : 090, Accuracy: 0.8929, NNL: 0.3894, ECE:0.0849\n",
      "Epoch: : 091, Accuracy: 0.8929, NNL: 0.3895, ECE:0.0849\n",
      "Epoch: : 092, Accuracy: 0.8929, NNL: 0.3897, ECE:0.0849\n",
      "Epoch: : 093, Accuracy: 0.8929, NNL: 0.3898, ECE:0.0849\n",
      "Epoch: : 094, Accuracy: 0.8929, NNL: 0.3900, ECE:0.0849\n",
      "Epoch: : 095, Accuracy: 0.8929, NNL: 0.3901, ECE:0.0849\n",
      "Epoch: : 096, Accuracy: 0.8929, NNL: 0.3903, ECE:0.0849\n",
      "Epoch: : 097, Accuracy: 0.8929, NNL: 0.3904, ECE:0.0849\n",
      "Epoch: : 098, Accuracy: 0.8929, NNL: 0.3905, ECE:0.0849\n",
      "Epoch: : 099, Accuracy: 0.8929, NNL: 0.3906, ECE:0.0849\n",
      "Epoch: : 100, Accuracy: 0.8929, NNL: 0.3908, ECE:0.0849\n",
      "Epoch: : 101, Accuracy: 0.8929, NNL: 0.3909, ECE:0.0849\n",
      "Epoch: : 102, Accuracy: 0.8929, NNL: 0.3909, ECE:0.0849\n",
      "Epoch: : 103, Accuracy: 0.8929, NNL: 0.3910, ECE:0.0849\n",
      "Epoch: : 104, Accuracy: 0.8929, NNL: 0.3911, ECE:0.0849\n",
      "Epoch: : 105, Accuracy: 0.8929, NNL: 0.3912, ECE:0.0849\n",
      "Epoch: : 106, Accuracy: 0.8929, NNL: 0.3912, ECE:0.0849\n",
      "Epoch: : 107, Accuracy: 0.8929, NNL: 0.3913, ECE:0.0849\n",
      "Epoch: : 108, Accuracy: 0.8929, NNL: 0.3913, ECE:0.0849\n",
      "Epoch: : 109, Accuracy: 0.8929, NNL: 0.3914, ECE:0.0849\n",
      "Epoch: : 110, Accuracy: 0.8929, NNL: 0.3914, ECE:0.0849\n",
      "Epoch: : 111, Accuracy: 0.8929, NNL: 0.3914, ECE:0.0849\n",
      "Epoch: : 112, Accuracy: 0.8929, NNL: 0.3914, ECE:0.0849\n",
      "Epoch: : 113, Accuracy: 0.8929, NNL: 0.3914, ECE:0.0849\n",
      "Epoch: : 114, Accuracy: 0.8929, NNL: 0.3915, ECE:0.0849\n",
      "Epoch: : 115, Accuracy: 0.8929, NNL: 0.3915, ECE:0.0849\n",
      "Epoch: : 116, Accuracy: 0.8929, NNL: 0.3914, ECE:0.0849\n",
      "Epoch: : 117, Accuracy: 0.8929, NNL: 0.3914, ECE:0.0849\n",
      "Epoch: : 118, Accuracy: 0.8929, NNL: 0.3914, ECE:0.0849\n",
      "Epoch: : 119, Accuracy: 0.8929, NNL: 0.3914, ECE:0.0849\n",
      "Epoch: : 120, Accuracy: 0.8929, NNL: 0.3914, ECE:0.0849\n",
      "Epoch: : 121, Accuracy: 0.8929, NNL: 0.3914, ECE:0.0849\n",
      "Epoch: : 122, Accuracy: 0.8929, NNL: 0.3914, ECE:0.0849\n",
      "Epoch: : 123, Accuracy: 0.8929, NNL: 0.3913, ECE:0.0849\n",
      "Epoch: : 124, Accuracy: 0.8929, NNL: 0.3913, ECE:0.0849\n",
      "Epoch: : 125, Accuracy: 0.8929, NNL: 0.3913, ECE:0.0849\n",
      "Epoch: : 126, Accuracy: 0.8929, NNL: 0.3913, ECE:0.0849\n",
      "Epoch: : 127, Accuracy: 0.8929, NNL: 0.3912, ECE:0.0849\n",
      "Epoch: : 128, Accuracy: 0.8929, NNL: 0.3912, ECE:0.0849\n",
      "Epoch: : 129, Accuracy: 0.8929, NNL: 0.3912, ECE:0.0849\n",
      "Epoch: : 130, Accuracy: 0.8929, NNL: 0.3912, ECE:0.0849\n",
      "Epoch: : 131, Accuracy: 0.8929, NNL: 0.3911, ECE:0.0849\n",
      "Epoch: : 132, Accuracy: 0.8929, NNL: 0.3911, ECE:0.0849\n",
      "Epoch: : 133, Accuracy: 0.8929, NNL: 0.3911, ECE:0.0849\n",
      "Epoch: : 134, Accuracy: 0.8929, NNL: 0.3911, ECE:0.0849\n",
      "Epoch: : 135, Accuracy: 0.8929, NNL: 0.3911, ECE:0.0849\n",
      "Epoch: : 136, Accuracy: 0.8929, NNL: 0.3910, ECE:0.0849\n",
      "Epoch: : 137, Accuracy: 0.8929, NNL: 0.3910, ECE:0.0849\n",
      "Epoch: : 138, Accuracy: 0.8929, NNL: 0.3910, ECE:0.0849\n",
      "Epoch: : 139, Accuracy: 0.8929, NNL: 0.3910, ECE:0.0849\n",
      "Epoch: : 140, Accuracy: 0.8929, NNL: 0.3910, ECE:0.0849\n",
      "Epoch: : 141, Accuracy: 0.8929, NNL: 0.3910, ECE:0.0849\n",
      "Epoch: : 142, Accuracy: 0.8929, NNL: 0.3910, ECE:0.0849\n",
      "Epoch: : 143, Accuracy: 0.8929, NNL: 0.3909, ECE:0.0849\n",
      "Epoch: : 144, Accuracy: 0.8929, NNL: 0.3909, ECE:0.0849\n",
      "Epoch: : 145, Accuracy: 0.8929, NNL: 0.3909, ECE:0.0849\n",
      "Epoch: : 146, Accuracy: 0.8929, NNL: 0.3909, ECE:0.0849\n",
      "Epoch: : 147, Accuracy: 0.8929, NNL: 0.3909, ECE:0.0849\n",
      "Epoch: : 148, Accuracy: 0.8929, NNL: 0.3909, ECE:0.0849\n",
      "Epoch: : 149, Accuracy: 0.8929, NNL: 0.3909, ECE:0.0849\n",
      "Epoch: : 150, Accuracy: 0.8929, NNL: 0.3909, ECE:0.0849\n",
      "Epoch: : 151, Accuracy: 0.8929, NNL: 0.3909, ECE:0.0849\n",
      "Epoch: : 152, Accuracy: 0.8929, NNL: 0.3909, ECE:0.0849\n",
      "Epoch: : 153, Accuracy: 0.8929, NNL: 0.3909, ECE:0.0849\n",
      "Epoch: : 154, Accuracy: 0.8929, NNL: 0.3909, ECE:0.0849\n",
      "Epoch: : 155, Accuracy: 0.8929, NNL: 0.3909, ECE:0.0849\n",
      "Epoch: : 156, Accuracy: 0.8929, NNL: 0.3909, ECE:0.0849\n",
      "Epoch: : 157, Accuracy: 0.8929, NNL: 0.3909, ECE:0.0849\n",
      "Epoch: : 158, Accuracy: 0.8929, NNL: 0.3909, ECE:0.0849\n",
      "Epoch: : 159, Accuracy: 0.8929, NNL: 0.3909, ECE:0.0849\n",
      "Epoch: : 160, Accuracy: 0.8929, NNL: 0.3909, ECE:0.0849\n",
      "Epoch: : 161, Accuracy: 0.8929, NNL: 0.3910, ECE:0.0849\n",
      "Epoch: : 162, Accuracy: 0.8929, NNL: 0.3910, ECE:0.0849\n",
      "Epoch: : 163, Accuracy: 0.8929, NNL: 0.3910, ECE:0.0849\n",
      "Epoch: : 164, Accuracy: 0.8929, NNL: 0.3910, ECE:0.0849\n",
      "Epoch: : 165, Accuracy: 0.8929, NNL: 0.3910, ECE:0.0849\n",
      "Epoch: : 166, Accuracy: 0.8929, NNL: 0.3910, ECE:0.0849\n",
      "Epoch: : 167, Accuracy: 0.8929, NNL: 0.3910, ECE:0.0849\n",
      "Epoch: : 168, Accuracy: 0.8929, NNL: 0.3910, ECE:0.0849\n",
      "Epoch: : 169, Accuracy: 0.8929, NNL: 0.3910, ECE:0.0849\n",
      "Epoch: : 170, Accuracy: 0.8929, NNL: 0.3910, ECE:0.0849\n",
      "Epoch: : 171, Accuracy: 0.8929, NNL: 0.3910, ECE:0.0849\n",
      "Epoch: : 172, Accuracy: 0.8929, NNL: 0.3910, ECE:0.0849\n",
      "Epoch: : 173, Accuracy: 0.8929, NNL: 0.3910, ECE:0.0849\n",
      "Epoch: : 174, Accuracy: 0.8929, NNL: 0.3910, ECE:0.0849\n",
      "run 9 TS tempreture:  1.0034840106964111\n",
      "Epoch: : 001, Accuracy: 0.8429, NNL: 0.5662, ECE:0.0627\n",
      "Epoch: : 002, Accuracy: 0.8429, NNL: 0.5641, ECE:0.0627\n",
      "Epoch: : 003, Accuracy: 0.8429, NNL: 0.5620, ECE:0.0627\n",
      "Epoch: : 004, Accuracy: 0.8429, NNL: 0.5598, ECE:0.0627\n",
      "Epoch: : 005, Accuracy: 0.8429, NNL: 0.5577, ECE:0.0627\n",
      "Epoch: : 006, Accuracy: 0.8429, NNL: 0.5556, ECE:0.0627\n",
      "Epoch: : 007, Accuracy: 0.8429, NNL: 0.5535, ECE:0.0627\n",
      "Epoch: : 008, Accuracy: 0.8429, NNL: 0.5514, ECE:0.0627\n",
      "Epoch: : 009, Accuracy: 0.8429, NNL: 0.5494, ECE:0.0627\n",
      "Epoch: : 010, Accuracy: 0.8429, NNL: 0.5473, ECE:0.0627\n",
      "Epoch: : 011, Accuracy: 0.8429, NNL: 0.5452, ECE:0.0627\n",
      "Epoch: : 012, Accuracy: 0.8429, NNL: 0.5432, ECE:0.0627\n",
      "Epoch: : 013, Accuracy: 0.8429, NNL: 0.5411, ECE:0.0627\n",
      "Epoch: : 014, Accuracy: 0.8429, NNL: 0.5391, ECE:0.0627\n",
      "Epoch: : 015, Accuracy: 0.8429, NNL: 0.5371, ECE:0.0627\n",
      "Epoch: : 016, Accuracy: 0.8429, NNL: 0.5351, ECE:0.0627\n",
      "Epoch: : 017, Accuracy: 0.8429, NNL: 0.5332, ECE:0.0627\n",
      "Epoch: : 018, Accuracy: 0.8429, NNL: 0.5312, ECE:0.0627\n",
      "Epoch: : 019, Accuracy: 0.8429, NNL: 0.5293, ECE:0.0627\n",
      "Epoch: : 020, Accuracy: 0.8429, NNL: 0.5274, ECE:0.0627\n",
      "Epoch: : 021, Accuracy: 0.8429, NNL: 0.5255, ECE:0.0627\n",
      "Epoch: : 022, Accuracy: 0.8429, NNL: 0.5237, ECE:0.0627\n",
      "Epoch: : 023, Accuracy: 0.8429, NNL: 0.5219, ECE:0.0627\n",
      "Epoch: : 024, Accuracy: 0.8429, NNL: 0.5201, ECE:0.0627\n",
      "Epoch: : 025, Accuracy: 0.8429, NNL: 0.5183, ECE:0.0627\n",
      "Epoch: : 026, Accuracy: 0.8429, NNL: 0.5166, ECE:0.0627\n",
      "Epoch: : 027, Accuracy: 0.8429, NNL: 0.5149, ECE:0.0627\n",
      "Epoch: : 028, Accuracy: 0.8429, NNL: 0.5133, ECE:0.0627\n",
      "Epoch: : 029, Accuracy: 0.8429, NNL: 0.5117, ECE:0.0627\n",
      "Epoch: : 030, Accuracy: 0.8429, NNL: 0.5101, ECE:0.0627\n",
      "Epoch: : 031, Accuracy: 0.8429, NNL: 0.5086, ECE:0.0627\n",
      "Epoch: : 032, Accuracy: 0.8429, NNL: 0.5071, ECE:0.0627\n",
      "Epoch: : 033, Accuracy: 0.8429, NNL: 0.5057, ECE:0.0627\n",
      "Epoch: : 034, Accuracy: 0.8429, NNL: 0.5043, ECE:0.0627\n",
      "Epoch: : 035, Accuracy: 0.8429, NNL: 0.5030, ECE:0.0627\n",
      "Epoch: : 036, Accuracy: 0.8429, NNL: 0.5017, ECE:0.0627\n",
      "Epoch: : 037, Accuracy: 0.8429, NNL: 0.5005, ECE:0.0627\n",
      "Epoch: : 038, Accuracy: 0.8429, NNL: 0.4993, ECE:0.0627\n",
      "Epoch: : 039, Accuracy: 0.8429, NNL: 0.4982, ECE:0.0627\n",
      "Epoch: : 040, Accuracy: 0.8429, NNL: 0.4972, ECE:0.0627\n",
      "Epoch: : 041, Accuracy: 0.8429, NNL: 0.4962, ECE:0.0627\n",
      "Epoch: : 042, Accuracy: 0.8429, NNL: 0.4952, ECE:0.0627\n",
      "Epoch: : 043, Accuracy: 0.8429, NNL: 0.4944, ECE:0.0627\n",
      "Epoch: : 044, Accuracy: 0.8429, NNL: 0.4936, ECE:0.0627\n",
      "Epoch: : 045, Accuracy: 0.8429, NNL: 0.4928, ECE:0.0627\n",
      "Epoch: : 046, Accuracy: 0.8429, NNL: 0.4921, ECE:0.0627\n",
      "Epoch: : 047, Accuracy: 0.8429, NNL: 0.4915, ECE:0.0627\n",
      "Epoch: : 048, Accuracy: 0.8429, NNL: 0.4909, ECE:0.0627\n",
      "Epoch: : 049, Accuracy: 0.8429, NNL: 0.4904, ECE:0.0627\n",
      "Epoch: : 050, Accuracy: 0.8429, NNL: 0.4900, ECE:0.0627\n",
      "Epoch: : 051, Accuracy: 0.8429, NNL: 0.4896, ECE:0.0627\n",
      "Epoch: : 052, Accuracy: 0.8429, NNL: 0.4892, ECE:0.0627\n",
      "Epoch: : 053, Accuracy: 0.8429, NNL: 0.4889, ECE:0.0627\n",
      "Epoch: : 054, Accuracy: 0.8429, NNL: 0.4887, ECE:0.0627\n",
      "Epoch: : 055, Accuracy: 0.8429, NNL: 0.4885, ECE:0.0627\n",
      "Epoch: : 056, Accuracy: 0.8429, NNL: 0.4884, ECE:0.0627\n",
      "Epoch: : 057, Accuracy: 0.8429, NNL: 0.4883, ECE:0.0627\n",
      "Epoch: : 058, Accuracy: 0.8429, NNL: 0.4882, ECE:0.0627\n",
      "Epoch: : 059, Accuracy: 0.8429, NNL: 0.4882, ECE:0.0627\n",
      "Epoch: : 060, Accuracy: 0.8429, NNL: 0.4882, ECE:0.0627\n",
      "Epoch: : 061, Accuracy: 0.8429, NNL: 0.4882, ECE:0.0627\n",
      "Epoch: : 062, Accuracy: 0.8429, NNL: 0.4883, ECE:0.0627\n",
      "Epoch: : 063, Accuracy: 0.8429, NNL: 0.4883, ECE:0.0627\n",
      "Epoch: : 064, Accuracy: 0.8429, NNL: 0.4884, ECE:0.0627\n",
      "Epoch: : 065, Accuracy: 0.8429, NNL: 0.4886, ECE:0.0627\n",
      "Epoch: : 066, Accuracy: 0.8429, NNL: 0.4887, ECE:0.0627\n",
      "Epoch: : 067, Accuracy: 0.8429, NNL: 0.4889, ECE:0.0627\n",
      "Epoch: : 068, Accuracy: 0.8429, NNL: 0.4891, ECE:0.0627\n",
      "Epoch: : 069, Accuracy: 0.8429, NNL: 0.4892, ECE:0.0627\n",
      "Epoch: : 070, Accuracy: 0.8429, NNL: 0.4894, ECE:0.0627\n",
      "Epoch: : 071, Accuracy: 0.8429, NNL: 0.4896, ECE:0.0627\n",
      "Epoch: : 072, Accuracy: 0.8429, NNL: 0.4898, ECE:0.0627\n",
      "Epoch: : 073, Accuracy: 0.8429, NNL: 0.4900, ECE:0.0627\n",
      "Epoch: : 074, Accuracy: 0.8429, NNL: 0.4902, ECE:0.0627\n",
      "Epoch: : 075, Accuracy: 0.8429, NNL: 0.4904, ECE:0.0627\n",
      "Epoch: : 076, Accuracy: 0.8429, NNL: 0.4905, ECE:0.0627\n",
      "Epoch: : 077, Accuracy: 0.8429, NNL: 0.4907, ECE:0.0627\n",
      "Epoch: : 078, Accuracy: 0.8429, NNL: 0.4909, ECE:0.0627\n",
      "Epoch: : 079, Accuracy: 0.8429, NNL: 0.4911, ECE:0.0627\n",
      "Epoch: : 080, Accuracy: 0.8429, NNL: 0.4912, ECE:0.0627\n",
      "Epoch: : 081, Accuracy: 0.8429, NNL: 0.4914, ECE:0.0627\n",
      "Epoch: : 082, Accuracy: 0.8429, NNL: 0.4915, ECE:0.0627\n",
      "Epoch: : 083, Accuracy: 0.8429, NNL: 0.4916, ECE:0.0627\n",
      "Epoch: : 084, Accuracy: 0.8429, NNL: 0.4917, ECE:0.0627\n",
      "Epoch: : 085, Accuracy: 0.8429, NNL: 0.4918, ECE:0.0627\n",
      "Epoch: : 086, Accuracy: 0.8429, NNL: 0.4919, ECE:0.0627\n",
      "Epoch: : 087, Accuracy: 0.8429, NNL: 0.4920, ECE:0.0627\n",
      "Epoch: : 088, Accuracy: 0.8429, NNL: 0.4921, ECE:0.0627\n",
      "Epoch: : 089, Accuracy: 0.8429, NNL: 0.4921, ECE:0.0627\n",
      "Epoch: : 090, Accuracy: 0.8429, NNL: 0.4922, ECE:0.0627\n",
      "Epoch: : 091, Accuracy: 0.8429, NNL: 0.4922, ECE:0.0627\n",
      "Epoch: : 092, Accuracy: 0.8429, NNL: 0.4922, ECE:0.0627\n",
      "Epoch: : 093, Accuracy: 0.8429, NNL: 0.4922, ECE:0.0627\n",
      "Epoch: : 094, Accuracy: 0.8429, NNL: 0.4923, ECE:0.0627\n",
      "Epoch: : 095, Accuracy: 0.8429, NNL: 0.4923, ECE:0.0627\n",
      "Epoch: : 096, Accuracy: 0.8429, NNL: 0.4923, ECE:0.0627\n",
      "Epoch: : 097, Accuracy: 0.8429, NNL: 0.4923, ECE:0.0627\n",
      "Epoch: : 098, Accuracy: 0.8429, NNL: 0.4922, ECE:0.0627\n",
      "Epoch: : 099, Accuracy: 0.8429, NNL: 0.4922, ECE:0.0627\n",
      "Epoch: : 100, Accuracy: 0.8429, NNL: 0.4922, ECE:0.0627\n",
      "Epoch: : 101, Accuracy: 0.8429, NNL: 0.4922, ECE:0.0627\n",
      "Epoch: : 102, Accuracy: 0.8429, NNL: 0.4921, ECE:0.0627\n",
      "Epoch: : 103, Accuracy: 0.8429, NNL: 0.4921, ECE:0.0627\n",
      "Epoch: : 104, Accuracy: 0.8429, NNL: 0.4921, ECE:0.0627\n",
      "Epoch: : 105, Accuracy: 0.8429, NNL: 0.4920, ECE:0.0627\n",
      "Epoch: : 106, Accuracy: 0.8429, NNL: 0.4920, ECE:0.0627\n",
      "Epoch: : 107, Accuracy: 0.8429, NNL: 0.4920, ECE:0.0627\n",
      "Epoch: : 108, Accuracy: 0.8429, NNL: 0.4919, ECE:0.0627\n",
      "Epoch: : 109, Accuracy: 0.8429, NNL: 0.4919, ECE:0.0627\n",
      "Epoch: : 110, Accuracy: 0.8429, NNL: 0.4919, ECE:0.0627\n",
      "Epoch: : 111, Accuracy: 0.8429, NNL: 0.4918, ECE:0.0627\n",
      "Epoch: : 112, Accuracy: 0.8429, NNL: 0.4918, ECE:0.0627\n",
      "Epoch: : 113, Accuracy: 0.8429, NNL: 0.4918, ECE:0.0627\n",
      "Epoch: : 114, Accuracy: 0.8429, NNL: 0.4917, ECE:0.0627\n",
      "Epoch: : 115, Accuracy: 0.8429, NNL: 0.4917, ECE:0.0627\n",
      "Epoch: : 116, Accuracy: 0.8429, NNL: 0.4917, ECE:0.0627\n",
      "Epoch: : 117, Accuracy: 0.8429, NNL: 0.4917, ECE:0.0627\n",
      "Epoch: : 118, Accuracy: 0.8429, NNL: 0.4916, ECE:0.0627\n",
      "Epoch: : 119, Accuracy: 0.8429, NNL: 0.4916, ECE:0.0627\n",
      "Epoch: : 120, Accuracy: 0.8429, NNL: 0.4916, ECE:0.0627\n",
      "Epoch: : 121, Accuracy: 0.8429, NNL: 0.4916, ECE:0.0627\n",
      "Epoch: : 122, Accuracy: 0.8429, NNL: 0.4916, ECE:0.0627\n",
      "Epoch: : 123, Accuracy: 0.8429, NNL: 0.4916, ECE:0.0627\n",
      "Epoch: : 124, Accuracy: 0.8429, NNL: 0.4916, ECE:0.0627\n",
      "Epoch: : 125, Accuracy: 0.8429, NNL: 0.4916, ECE:0.0627\n",
      "Epoch: : 126, Accuracy: 0.8429, NNL: 0.4916, ECE:0.0627\n",
      "Epoch: : 127, Accuracy: 0.8429, NNL: 0.4916, ECE:0.0627\n",
      "Epoch: : 128, Accuracy: 0.8429, NNL: 0.4916, ECE:0.0627\n",
      "Epoch: : 129, Accuracy: 0.8429, NNL: 0.4916, ECE:0.0627\n",
      "Epoch: : 130, Accuracy: 0.8429, NNL: 0.4916, ECE:0.0627\n",
      "Epoch: : 131, Accuracy: 0.8429, NNL: 0.4916, ECE:0.0627\n",
      "Epoch: : 132, Accuracy: 0.8429, NNL: 0.4916, ECE:0.0627\n",
      "Epoch: : 133, Accuracy: 0.8429, NNL: 0.4916, ECE:0.0627\n",
      "Epoch: : 134, Accuracy: 0.8429, NNL: 0.4916, ECE:0.0627\n",
      "Epoch: : 135, Accuracy: 0.8429, NNL: 0.4916, ECE:0.0627\n",
      "Epoch: : 136, Accuracy: 0.8429, NNL: 0.4916, ECE:0.0627\n",
      "Epoch: : 137, Accuracy: 0.8429, NNL: 0.4916, ECE:0.0627\n",
      "Epoch: : 138, Accuracy: 0.8429, NNL: 0.4916, ECE:0.0627\n",
      "Epoch: : 139, Accuracy: 0.8429, NNL: 0.4916, ECE:0.0627\n",
      "Epoch: : 140, Accuracy: 0.8429, NNL: 0.4916, ECE:0.0627\n",
      "Epoch: : 141, Accuracy: 0.8429, NNL: 0.4916, ECE:0.0627\n",
      "Epoch: : 142, Accuracy: 0.8429, NNL: 0.4916, ECE:0.0627\n",
      "Epoch: : 143, Accuracy: 0.8429, NNL: 0.4917, ECE:0.0627\n",
      "Epoch: : 144, Accuracy: 0.8429, NNL: 0.4917, ECE:0.0627\n",
      "Epoch: : 145, Accuracy: 0.8429, NNL: 0.4917, ECE:0.0627\n",
      "Epoch: : 146, Accuracy: 0.8429, NNL: 0.4917, ECE:0.0627\n",
      "Epoch: : 147, Accuracy: 0.8429, NNL: 0.4917, ECE:0.0627\n",
      "Epoch: : 148, Accuracy: 0.8429, NNL: 0.4917, ECE:0.0627\n",
      "Epoch: : 149, Accuracy: 0.8429, NNL: 0.4917, ECE:0.0627\n",
      "Epoch: : 150, Accuracy: 0.8429, NNL: 0.4917, ECE:0.0627\n",
      "Epoch: : 151, Accuracy: 0.8429, NNL: 0.4917, ECE:0.0627\n",
      "Epoch: : 152, Accuracy: 0.8429, NNL: 0.4917, ECE:0.0627\n",
      "Epoch: : 153, Accuracy: 0.8429, NNL: 0.4917, ECE:0.0627\n",
      "Epoch: : 154, Accuracy: 0.8429, NNL: 0.4917, ECE:0.0627\n",
      "Epoch: : 155, Accuracy: 0.8429, NNL: 0.4917, ECE:0.0627\n",
      "Epoch: : 156, Accuracy: 0.8429, NNL: 0.4917, ECE:0.0627\n",
      "Epoch: : 157, Accuracy: 0.8429, NNL: 0.4917, ECE:0.0627\n",
      "Epoch: : 158, Accuracy: 0.8429, NNL: 0.4917, ECE:0.0627\n",
      "run 10 TS tempreture:  1.108913540840149\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for run in range(10):\n",
    "    model1 = create_model(dataset, args).to(device)\n",
    "    model1.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    ts = TS(model1)\n",
    "    ts.fit(data, data.val_mask, data.train_mask, wdecay=0, edge_weight=None, verbose=True)\n",
    "    print(f'run {run+1} TS tempreture: ', ts.temperature.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "59b0c713-6d7c-47e2-a001-989c06d9a0fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 0 ECE1 :  0.07813651859760284\n",
      "run 0 DCGC ECE :  0.11692075431346893\n",
      "run 0 TS ECE :  0.047383226454257965\n",
      "run 0 TS+DCGC ECE :  0.05302067846059799\n",
      "----------------------------------------------\n",
      "run 1 ECE1 :  0.06994426250457764\n",
      "run 1 DCGC ECE :  0.1017957553267479\n",
      "run 1 TS ECE :  0.047383226454257965\n",
      "run 1 TS+DCGC ECE :  0.06746464222669601\n",
      "----------------------------------------------\n",
      "run 2 ECE1 :  0.06502808630466461\n",
      "run 2 DCGC ECE :  0.09834396839141846\n",
      "run 2 TS ECE :  0.047383226454257965\n",
      "run 2 TS+DCGC ECE :  0.061257507652044296\n",
      "----------------------------------------------\n",
      "run 3 ECE1 :  0.06497398018836975\n",
      "run 3 DCGC ECE :  0.09130603820085526\n",
      "run 3 TS ECE :  0.04738324135541916\n",
      "run 3 TS+DCGC ECE :  0.056458279490470886\n",
      "----------------------------------------------\n",
      "run 4 ECE1 :  0.06122978776693344\n",
      "run 4 DCGC ECE :  0.08220722526311874\n",
      "run 4 TS ECE :  0.047383226454257965\n",
      "run 4 TS+DCGC ECE :  0.06695885956287384\n",
      "----------------------------------------------\n",
      "run 5 ECE1 :  0.0686628445982933\n",
      "run 5 DCGC ECE :  0.09720927476882935\n",
      "run 5 TS ECE :  0.04738324135541916\n",
      "run 5 TS+DCGC ECE :  0.05765315517783165\n",
      "----------------------------------------------\n",
      "run 6 ECE1 :  0.06431218236684799\n",
      "run 6 DCGC ECE :  0.08971624821424484\n",
      "run 6 TS ECE :  0.04738323763012886\n",
      "run 6 TS+DCGC ECE :  0.062109947204589844\n",
      "----------------------------------------------\n",
      "run 7 ECE1 :  0.07570798695087433\n",
      "run 7 DCGC ECE :  0.10116211324930191\n",
      "run 7 TS ECE :  0.04738323390483856\n",
      "run 7 TS+DCGC ECE :  0.052055150270462036\n",
      "----------------------------------------------\n",
      "run 8 ECE1 :  0.07670363783836365\n",
      "run 8 DCGC ECE :  0.10877972841262817\n",
      "run 8 TS ECE :  0.047383226454257965\n",
      "run 8 TS+DCGC ECE :  0.06225043535232544\n",
      "----------------------------------------------\n",
      "run 9 ECE1 :  0.059173401445150375\n",
      "run 9 DCGC ECE :  0.09270382672548294\n",
      "run 9 TS ECE :  0.04738324508070946\n",
      "run 9 TS+DCGC ECE :  0.06139931455254555\n",
      "----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "    # print(f'run {run+1} ES tempreture: ', ts.temperature.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f1eac149-3795-4ad9-857d-16c3c80028b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edge_Weight(\n",
       "  (model): GAT(\n",
       "    (layer_list): ModuleDict(\n",
       "      (conv1): GATConv(1433, 8, heads=8)\n",
       "      (conv2): GATConv(64, 7, heads=1)\n",
       "    )\n",
       "  )\n",
       "  (extractor): MLP(14, 28, 1)\n",
       ")"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ew\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "af3b517f-9caa-4e2f-b76d-78f305350d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "def get_dataset(data):\n",
    "    # 10 5 85\n",
    "    torch.manual_seed(np.random.randint(0, 10000))\n",
    "    train_num = int(data.num_nodes * 0.1)\n",
    "    val_num = int(data.num_nodes * 0.05)\n",
    "    test_num = data.num_nodes - train_num - val_num\n",
    "    idx = range(data.num_nodes)\n",
    "    train_idx, test_idx = random_split(dataset=idx, lengths=[train_num, val_num + test_num])\n",
    "    val_idx, test_idx = random_split(dataset=test_idx, lengths=[val_num, test_num])\n",
    "\n",
    "    return list(train_idx), list(val_idx), list(test_idx)\n",
    "\n",
    "dataset_cora = Planetoid('./data/', args.dataset, transform=T.NormalizeFeatures())\n",
    "data_cora = dataset_cora[0].to(device)\n",
    "idx_train, idx_val, idx_test = get_dataset(data_cora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "748dd15f-86e9-4c94-8855-a59b9318483f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270\n",
      "135\n",
      "2303\n"
     ]
    }
   ],
   "source": [
    "print(len(idx_train))\n",
    "print(len(idx_val))\n",
    "print(len(idx_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "d010e193-75cb-47e4-8a92-c421d58d706b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29, 30, 48, 78, 45, 26, 14]\n",
      "[21, 17, 13, 40, 24, 13, 7]\n",
      "[301, 170, 357, 700, 357, 259, 159]\n"
     ]
    }
   ],
   "source": [
    "train_label = [(data_cora.y[idx_train]==k).sum().item() for k in range(dataset_cora.num_classes)]\n",
    "val_label = [(data_cora.y[idx_val]==k).sum().item() for k in range(dataset_cora.num_classes)]\n",
    "test_label = [(data_cora.y[idx_test]==k).sum().item() for k in range(dataset_cora.num_classes)]\n",
    "print(train_label)\n",
    "print(val_label)\n",
    "print(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "680e024c-26da-4b00-b6ae-48677c811622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_cora.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2115b2-ed7e-4f85-8521-0e5ac0fae2da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
